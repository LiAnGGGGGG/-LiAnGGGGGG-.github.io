<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LiAnG&#39;s Blog</title>
  
  <subtitle>Stay hungry, stay foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liangggggg.github.io/"/>
  <updated>2020-06-03T11:13:19.990Z</updated>
  <id>https://liangggggg.github.io/</id>
  
  <author>
    <name>LiAnG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>学习日志：2020智慧海洋建设top5方案学习</title>
    <link href="https://liangggggg.github.io/2020/05/30/My-New-Post/"/>
    <id>https://liangggggg.github.io/2020/05/30/My-New-Post/</id>
    <published>2020-05-30T01:15:34.000Z</published>
    <updated>2020-06-03T11:13:19.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2020-5-30-赛题解读"><a href="#2020-5-30-赛题解读" class="headerlink" title="2020.5.30 赛题解读"></a>2020.5.30 赛题解读</h2><h3 id="赛题背景：渔船作业分类"><a href="#赛题背景：渔船作业分类" class="headerlink" title="赛题背景：渔船作业分类"></a>赛题背景：渔船作业分类</h3><p>本赛题基于位置数据对海上目标进行智能识别和作业行为分析，要求选手通过分析渔船北斗设备位置数据，得出该船的生产作业行为，具体判断出是拖网作业、围网作业还是流刺网作业。初赛将提供11000条(其中7000条训练数据、2000条testA、2000条testB)渔船轨迹北斗数据。</p><a id="more"></a><p>复赛考虑以往渔船在海上作业时主要依赖AIS数据，北斗相比AIS数据，数据上报频率和数据质量均低于AIS数据，因此复赛拟加入AIS轨迹数据辅助北斗数据更好的做渔船类型识别，其中AIS数据与北斗数据的匹配需选手自行实现，具体细节复赛开赛时更新。同时，希望选手通过数据可视化与分析，挖掘更多海洋通信导航设备的应用价值。</p><h3 id="竞赛数据"><a href="#竞赛数据" class="headerlink" title="竞赛数据:"></a>竞赛数据:</h3><p>提供11000条渔船北斗数据，数据包含脱敏后的渔船ID、经纬度坐标、上报时间、速度、航向信息，由于真实场景下海上环境复杂，经常出现信号丢失，设备故障等原因导致的上报坐标错误、上报数据丢失、甚至有些设备疯狂上报等。</p><p>数据示例：</p><table><thead><tr><th align="left">渔船ID</th><th align="center">x</th><th align="center">y</th><th align="center">速度</th><th align="center">方向</th><th align="center">time</th><th align="right">type</th></tr></thead><tbody><tr><td align="left">1102</td><td align="center">6283649.656204367</td><td align="center">5284013.963699763</td><td align="center">3</td><td align="center">12.1</td><td align="center">0921 09:00</td><td align="right">围网</td></tr></tbody></table><p>渔船ID：渔船的唯一识别，结果文件以此ID为标示</p><p>x: 渔船在平面坐标系的x轴坐标</p><p>y: 渔船在平面坐标系的y轴坐标</p><p>速度：渔船当前时刻航速，单位节</p><p>方向：渔船当前时刻航首向，单位度</p><p>time：数据上报时刻，单位月日 时：分</p><p>type：渔船label，作业类型</p><p>原始数据经过脱敏处理，渔船信息被隐去，坐标等信息精度和位置被转换偏移。<br>选手可通过学习围网、刺网、拖网等专业知识辅助大赛数据处理。<br>AIS数据</p><table><thead><tr><th align="left">ais_id</th><th align="center">lon</th><th align="center">lat</th><th align="center">速度</th><th align="center">航向</th><th align="right">time</th></tr></thead><tbody><tr><td align="left">110</td><td align="center">119.6705</td><td align="center">26.5938</td><td align="center">3</td><td align="center">12.1</td><td align="right">0921 09:00</td></tr></tbody></table><p>ais_id：AIS设备的唯一识别ID</p><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>选手提交结果与实际渔船作业类型结果进行对比，以3种类别的各自F1值取平均做为评价指标，结果越大越好，具体计算公式如下：</p><p>$$Score ={F1_{围网}+F1_{刺网}+F1_{拖网}  \over 3}$$</p><p>$$F1 ={2\ast P\ast R\over P+R}$$</p><p>其中P为某类别的准确率，R为某类别的召回率，评测程序f1函数为sklearn.metrics.f1_score，average=’macro’。</p><h2 id="模型大致思路"><a href="#模型大致思路" class="headerlink" title="模型大致思路"></a>模型大致思路</h2><ul><li><p>将所有数据数据切入：速度等于0和非0，白天和黑夜，四个数据集对每艘船的速度，方向，xy进行统计。</p></li><li><p>采用TFIDF对速度和XY进行抽取特征并降维</p></li><li><p>采用自然语言思路对速度，xy进行嵌入</p></li><li><p>训练模型前采用Lightgbm进行初步的特征筛选</p></li><li><p>最后用Lightgbm进行模型训练</p></li></ul><h2 id="具体分析"><a href="#具体分析" class="headerlink" title="具体分析"></a>具体分析</h2><p><strong>1. 按照同一个渔船id速度为0和非0两部分进行分析</strong></p><p><img src="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861" alt></p><p>思路：</p><p>1、针对同一艘渔船，将其数据分为 速度为0和非0两个部分。分别统计该船在速度为0 和 非0情况下做可视化分析，观察经纬度xy、方向direction这些原始特征的变化情况（均值、方差、极值、峰度、偏度等统计特征）</p><p>2、根据1构建的特征，原始特征被构造出一系列统计特征，一种含义的特征会被分成速度为0和非0情况。根据这个特点，对这些特征进行一个比值处理。</p><p><strong>2. 渔船在白天和黑夜会按照同一个渔船id白天和黑夜两部分进行分析</strong></p><p>早6点整至晚8点整设置为白天(图标识Day)</p><p>晚8点整至早6点整设置为黑夜(图标识Night)</p><p><img src="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861" alt></p><p>思路：<br>1、    数据按照时间划分成白天和黑夜两部分，分别统计该船在不同时间做可视化分析，观察经纬度xy、方向direction这些原始特征的变化情况（均值、方差、极值、峰度、偏度等统计特征）</p><p>2、    根据1构造的两组时间特征，提取关键的速度speed、经纬度xy进行白天与黑夜特征的对比。</p><p><strong>3. 借鉴自然语言处理（NLP）角度去处理船的轨迹特征</strong></p><p>速度speed、经纬度xy按照作业时间排序，可以反映出每艘船的行为规律。而每种作业方式都有其内在的一些规律, 借鉴自然语言处理(NLP)的相关算法进行特征提取。利用nlp的算法对速度、经纬这些序列的学习，尝试挖掘出每艘船的行为特点。</p><p>思路一：TF-IDF + NMF(如图，从左到有分别是ngram=1, ngram=2,ngram=3，经过t-SNE降维的可视化结果)</p><p><img src="https://note.youdao.com/yws/api/personal/file/BAC79B4C1B3440D6BFF6A1D51AC8A0FF?method=download&shareKey=16b799d9748d761fab551bec75914030" alt></p><p>1、    使用不同的ngram去处理每个渔船的速度、经纬度数据，提取出每艘船的TF-IDF特征（ngram=1, 2, 3）。</p><p>2、    并利用非负矩阵分解(NMF)算法，对处理后的速度、经纬度进行降维生成一个主题分布向量。（此题目分成了8类）。</p><p>3、    对每个渔船的主题分布向量进行T-SNE降维，进行可视化。</p><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>该代码主要包括三个文件</p><ul><li>feature_selector.py 特征选择文件</li><li>nmf_list.py 处理轨迹文件</li><li>model.py 模型文件</li></ul><p><strong>1. feature_selector.py代码分析：</strong></p><pre><code># numpy and pandas for data manipulationimport pandas as pdimport numpy as np# model used for feature importancesimport lightgbm as lgb# utility for early stopping with a validation setfrom sklearn.model_selection import train_test_split# visualizationsimport matplotlib.pyplot as pltimport seaborn as sns# memory managementimport gc# utilitiesfrom itertools import chainclass FeatureSelector():&quot;&quot;&quot;    这个类用于为机器学习或数据预处理执行特征选择实现五种不同的方法来识别要删除的特性1、查找丢失百分比大于指定阈值的列2、查找具有唯一值的列3、找出相关系数大于指定相关系数的相关变量4、从梯度增强机(gbm)中查找特性重要性为0.0的特性5、从gbm中查找不影响指定的累积特性重要性的低重要性特性参数--------data:dataframe    一个数据集，行中有观察值，列中有特性labels : array or series, default = None    数组标签用于训练机器学习模型，以发现特征重要性。它们可以是二进制标签    (如果任务是“分类”)或连续目标(如果任务是“回归”)。    如果没有提供标签，那么基于特征重要性的方法是不可用的。属性--------ops : dict    运行的操作字典和要删除的特性missing_stats : dataframe    所有特征的缺失值的比例record_missing : dataframe    缺失值在阈值以上的特征的缺失值的比例unique_stats: dataframe    所有特性的唯一值的个数record_single_unique: dataframe    记录具有唯一值的特性corr_matrix : dataframe    数据中所有特征之间的所有相关性record_collinear : dataframe    记录相关系数高于阈值的相关变量对feature_importances: dataframe    从梯度增强机的所有特征的重要性record_zero_importance: dataframe    根据gbm记录数据中的零重要性特征record_low_importance: dataframe    根据gbm记录不需要达到累积重要性阈值的最低重要性特征Notes--------    -所有5个操作都可以用identify_all方法运行。    -如果使用特性重要度，则对创建新列的分类变量使用one-hot编码&quot;&quot;&quot;    def __init__(self, data, labels=None):                    # 数据集和标签        self.data = data        self.labels = labels        if labels is None:            print(&apos;No labels provided. Feature importance based methods are not available.&apos;)            # 记录关于要删除的特性的信息            self.record_missing = None        self.record_single_unique = None        self.record_collinear = None        self.record_zero_importance = None        self.record_low_importance = None        self.missing_stats = None        self.unique_stats = None        self.corr_matrix = None        self.feature_importances = None        # 用于保存删除操作的字典        self.ops = {}        self.one_hot_correlated = False    def identify_missing(self, missing_threshold):        # 找到丢失值大于&apos; missing_threshold &apos;的部分特征        self.missing_threshold = missing_threshold        # 计算每一列特征的缺失率        missing_series = self.data.isnull().sum() / self.data.shape[0]        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {&apos;index&apos;: &apos;feature&apos;, 0: &apos;missing_fraction&apos;})        # 将特征的缺失率排序        self.missing_stats = self.missing_stats.sort_values(&apos;missing_fraction&apos;, ascending = False)        #找到缺失百分比大于阈值的列        record_missing = pd.DataFrame(missing_series[missing_series &gt; missing_threshold]).reset_index().rename(columns =                                                                                                                    {&apos;index&apos;: &apos;feature&apos;,                                                                                                                     0: &apos;missing_fraction&apos;})        to_drop = list(record_missing[&apos;feature&apos;])        self.record_missing = record_missing        self.ops[&apos;missing&apos;] = to_drop        print(&apos;%d features with greater than %0.2f missing values.\n&apos; % (len(self.ops[&apos;missing&apos;]), self.missing_threshold))    def identify_single_unique(self):    # 查找只有一个唯一值的特征        # 计算每个列中的惟一计数        unique_counts = self.data.nunique()        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {&apos;index&apos;: &apos;feature&apos;, 0: &apos;nunique&apos;})        self.unique_stats = self.unique_stats.sort_values(&apos;nunique&apos;, ascending = True)        # 查找只有惟一计数的列        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {&apos;index&apos;: &apos;feature&apos;,                                                                                                                0: &apos;nunique&apos;})        to_drop = list(record_single_unique[&apos;feature&apos;])        self.record_single_unique = record_single_unique        self.ops[&apos;single_unique&apos;] = to_drop        print(&apos;%d features with a single unique value.\n&apos; % len(self.ops[&apos;single_unique&apos;]))    def identify_collinear(self, correlation_threshold, one_hot=False):        &quot;&quot;&quot;        找寻相关系数大于“correlation_threshold”的特征并删除        参数        --------        correlation_threshold : float between 0 and 1        one_hot : boolean, default = False        &quot;&quot;&quot;        self.correlation_threshold = correlation_threshold        self.one_hot_correlated = one_hot        # 计算每一列之间的相关性        if one_hot:            # one_hot编码            features = pd.get_dummies(self.data)            self.one_hot_features = [column for column in features.columns if column not in self.base_features]            # 向原始数据添加一个热编码数据            self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)            corr_matrix = pd.get_dummies(features).corr()        else:            corr_matrix = self.data.corr()        self.corr_matrix = corr_matrix        # 提取关联矩阵的上三角        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))        # 选择相关性高于阈值的特性         # 需要使用绝对值        to_drop = [column for column in upper.columns if any(upper[column].abs() &gt; correlation_threshold)]        # 保存线性相关特征        record_collinear = pd.DataFrame(columns = [&apos;drop_feature&apos;, &apos;corr_feature&apos;, &apos;corr_value&apos;])        # 遍历列以删除相关特性对        for column in to_drop:            # 找出相关特征            corr_features = list(upper.index[upper[column].abs() &gt; correlation_threshold])            # 找出相关系数            corr_values = list(upper[column][upper[column].abs() &gt; correlation_threshold])            drop_features = [column for _ in range(len(corr_features))]            # 记录信息(现在需要一个临时df)            temp_df = pd.DataFrame.from_dict({&apos;drop_feature&apos;: drop_features,                                                 &apos;corr_feature&apos;: corr_features,                                                 &apos;corr_value&apos;: corr_values})            #添加到dataframe            record_collinear = record_collinear.append(temp_df, ignore_index = True)        self.record_collinear = record_collinear        self.ops[&apos;collinear&apos;] = to_drop        print(&apos;%d features with a correlation magnitude greater than %0.2f.\n&apos; % (len(self.ops[&apos;collinear&apos;]), self.correlation_threshold))    def identify_zero_importance(self, task, eval_metric=None,                                  n_iterations=10, early_stopping = True):        &quot;&quot;&quot;        根据梯度增强机识别零重要性的特征。        gbm可以使用验证集进行早期停止训练，以防止过拟合。        在“n_iteration”上对特征重要性求平均值以减少方差。        参数         --------        eval_metric : string            评价指标用于梯度提升机的早期停止，如果&apos; early_stopped &apos;为真，则必须提供        task : string            机器学习任务，是“classification”还是“regression”        n_iterations : int, default = 10            gbm的训练迭代次数        early_stopping : boolean, default = True            是否在训练时使用提前停止与验证集        Notes        --------            - 在训练前热编码特征            - gbm没有针对任何特定任务进行优化，可能需要进行一些超参数调优            - 特性重要性，包括零重要性特性，可以在运行过程中改变        &quot;&quot;&quot;        if early_stopping and eval_metric is None:            raise ValueError(&quot;&quot;&quot;eval metric must be provided with early stopping. Examples include &quot;auc&quot; for classification or                             &quot;l2&quot; for regression.&quot;&quot;&quot;)        if self.labels is None:             raise ValueError(&quot;No training labels provided.&quot;)        # 热编码特征        features = pd.get_dummies(self.data)        self.one_hot_features = [column for column in features.columns if column not in self.base_features]        # 将热编码数据添加到原始数据中        self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)        # 提取特征名字        feature_names = list(features.columns)        # 转换成np array        features = np.array(features)        labels = np.array(self.labels).reshape((-1, ))        # 创建特征重要性数组        feature_importance_values = np.zeros(len(feature_names))        print(&apos;Training Gradient Boosting Model\n&apos;)        # 在每折中迭代        for _ in range(n_iterations):            if task == &apos;classification&apos;:                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)            elif task == &apos;regression&apos;:                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)            else:                raise ValueError(&apos;Task must be either &quot;classification&quot; or &quot;regression&quot;&apos;)            # 如果使用早期停止训练需要一个验证集            if early_stopping:                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15, stratify=labels)                # 使用早停机制训练模型                model.fit(train_features, train_labels, eval_metric = eval_metric,                              eval_set = [(valid_features, valid_labels)],                              early_stopping_rounds = 100, verbose = -1)                # 清空内存                gc.enable()                del train_features, train_labels, valid_features, valid_labels                gc.collect()            else:                model.fit(features, labels)            # 记录特征重要性            feature_importance_values += model.feature_importances_ / n_iterations        feature_importances = pd.DataFrame({&apos;feature&apos;: feature_names, &apos;importance&apos;: feature_importance_values})        # 根据特征重要性排序        feature_importances = feature_importances.sort_values(&apos;importance&apos;, ascending = False).reset_index(drop = True)        # 归一化特征重要性        feature_importances[&apos;normalized_importance&apos;] = feature_importances[&apos;importance&apos;] / feature_importances[&apos;importance&apos;].sum()        feature_importances[&apos;cumulative_importance&apos;] = np.cumsum(feature_importances[&apos;normalized_importance&apos;])        # 提取特征重要性为0的特征        record_zero_importance = feature_importances[feature_importances[&apos;importance&apos;] == 0.0]        to_drop = list(record_zero_importance[&apos;feature&apos;])        self.feature_importances = feature_importances        self.record_zero_importance = record_zero_importance        self.ops[&apos;zero_importance&apos;] = to_drop        print(&apos;\n%d features with zero importance after one-hot encoding.\n&apos; % len(self.ops[&apos;zero_importance&apos;]))    def identify_low_importance(self, cumulative_importance):        &quot;&quot;&quot;        找到特征重要性低于“cumulative_importance”的特征        参数        --------        cumulative_importance : float between 0 and 1            重要性分数        &quot;&quot;&quot;        self.cumulative_importance = cumulative_importance        # 特征重要性需要在运行之前计算        if self.feature_importances is None:            raise NotImplementedError(&quot;&quot;&quot;Feature importances have not yet been determined.                                      Call the `identify_zero_importance` method first.&quot;&quot;&quot;)        # 将特征重要性排序        self.feature_importances = self.feature_importances.sort_values(&apos;cumulative_importance&apos;)        # 识别出特征重要性低于设定阈值的特征        record_low_importance = self.feature_importances[self.feature_importances[&apos;cumulative_importance&apos;] &gt; cumulative_importance]        to_drop = list(record_low_importance[&apos;feature&apos;])        self.record_low_importance = record_low_importance        self.ops[&apos;low_importance&apos;] = to_drop        print(&apos;%d features required for cumulative importance of %0.2f after one hot encoding.&apos; % (len(self.feature_importances) -                                                                            len(self.record_low_importance), self.cumulative_importance))        print(&apos;%d features do not contribute to cumulative importance of %0.2f.\n&apos; % (len(self.ops[&apos;low_importance&apos;]),                                                                                               self.cumulative_importance))    def identify_all(self, selection_params):        &quot;&quot;&quot;        使用所有五种方法来删除不需要的特征        参数        --------        selection_params : dict            在五种特征选择方法中使用的参数。            参数必须包含键[&apos;missing_threshold&apos;， &apos;correlation_threshold&apos;， &apos;eval_metric&apos;， &apos;task&apos;， &apos; collecative_importance &apos;]        # 检查所必要的参数        for param in [&apos;missing_threshold&apos;, &apos;correlation_threshold&apos;, &apos;eval_metric&apos;, &apos;task&apos;, &apos;cumulative_importance&apos;]:            if param not in selection_params.keys():                raise ValueError(&apos;%s is a required parameter for this method.&apos; % param)        # 实现五种方法        self.identify_missing(selection_params[&apos;missing_threshold&apos;])        self.identify_single_unique()        self.identify_collinear(selection_params[&apos;correlation_threshold&apos;])        self.identify_zero_importance(task = selection_params[&apos;task&apos;], eval_metric = selection_params[&apos;eval_metric&apos;])        self.identify_low_importance(selection_params[&apos;cumulative_importance&apos;])        # 查找要删除的特性的数量        self.all_identified = set(list(chain(*list(self.ops.values()))))        self.n_identified = len(self.all_identified)        print(&apos;%d total features out of %d identified for removal after one-hot encoding.\n&apos; % (self.n_identified,                                                                                                   self.data_all.shape[1]))     def check_removal(self, keep_one_hot=True):         &quot;&quot;&quot;         在删除前检查已识别的特征。返回一个列表的独特的功能识别。         &quot;&quot;&quot;        self.all_identified = set(list(chain(*list(self.ops.values()))))        print(&apos;Total of %d features identified for removal&apos; % len(self.all_identified))        if not keep_one_hot:            if self.one_hot_features is None:                print(&apos;Data has not been one-hot encoded&apos;)            else:                one_hot_to_remove = [x for x in self.one_hot_features if x not in self.all_identified]                print(&apos;%d additional one-hot features can be removed&apos; % len(one_hot_to_remove))        return list(self.all_identified)    def remove(self, methods, keep_one_hot = True):        &quot;&quot;&quot;        根据指定的方法从数据中删除特征。        参数        --------            methods : &apos;all&apos; or list of methods                可以是[&apos;missing&apos;， &apos;single_unique&apos;， &apos;collinear&apos;， &apos;zero_importance&apos;， &apos;low_importance&apos;]            keep_one_hot : boolean, default = True                是否热编码        返回        --------            data : dataframe                删除了特征的数据        Notes         --------            -如果使用特性重要度，则一个热编码列将被添加到数据中(然后可能被删除)            -在转换数据之前，检查将被删除的功能!        features_to_drop = []        if methods == &apos;all&apos;:            # 热编码数据            data = self.data_all            print(&apos;{} methods have been run\n&apos;.format(list(self.ops.keys())))            # 找到需要删除的特征            features_to_drop = set(list(chain(*list(self.ops.values()))))        else:            # Need to use one-hot encoded data as well            if &apos;zero_importance&apos; in methods or &apos;low_importance&apos; in methods or self.one_hot_correlated:                data = self.data_all            else:                data = self.data            # 遍历指定方法            for method in methods:                # 确定方法已经在运行                if method not in self.ops.keys():                    raise NotImplementedError(&apos;%s method has not been run&apos; % method)                # 添加要删除的方法                else:                    features_to_drop.append(self.ops[method])            # 找到要删除的特征            features_to_drop = set(list(chain(*features_to_drop)))        features_to_drop = list(features_to_drop)        if not keep_one_hot:            if self.one_hot_features is None:                print(&apos;Data has not been one-hot encoded&apos;)            else:                features_to_drop = list(set(features_to_drop) | set(self.one_hot_features))        # 在原数据中删除特征        data = data.drop(columns = features_to_drop)        self.removed_features = features_to_drop        if not keep_one_hot:            print(&apos;Removed %d features including one-hot features.&apos; % len(features_to_drop))         else:            print(&apos;Removed %d features.&apos; % len(features_to_drop))        return data    # 各种绘图函数    def plot_missing(self):        &quot;&quot;&quot;Histogram of missing fraction in each feature&quot;&quot;&quot;        if self.record_missing is None:            raise NotImplementedError(&quot;Missing values have not been calculated. Run `identify_missing`&quot;)        self.reset_plot()        # Histogram of missing values        plt.style.use(&apos;seaborn-white&apos;)        plt.figure(figsize = (7, 5))        plt.hist(self.missing_stats[&apos;missing_fraction&apos;], bins = np.linspace(0, 1, 11), edgecolor = &apos;k&apos;, color = &apos;red&apos;, linewidth = 1.5)        plt.xticks(np.linspace(0, 1, 11));        plt.xlabel(&apos;Missing Fraction&apos;, size = 14); plt.ylabel(&apos;Count of Features&apos;, size = 14);         plt.title(&quot;Fraction of Missing Values Histogram&quot;, size = 16);    def plot_unique(self):        &quot;&quot;&quot;Histogram of number of unique values in each feature&quot;&quot;&quot;        if self.record_single_unique is None:            raise NotImplementedError(&apos;Unique values have not been calculated. Run `identify_single_unique`&apos;)        self.reset_plot()        # Histogram of number of unique values        self.unique_stats.plot.hist(edgecolor = &apos;k&apos;, figsize = (7, 5))        plt.ylabel(&apos;Frequency&apos;, size = 14); plt.xlabel(&apos;Unique Values&apos;, size = 14);         plt.title(&apos;Number of Unique Values Histogram&apos;, size = 16);    def plot_collinear(self, plot_all = False):        &quot;&quot;&quot;        Heatmap of the correlation values. If plot_all = True plots all the correlations otherwise        plots only those features that have a correlation above the threshold        Notes        --------            - Not all of the plotted correlations are above the threshold because this plots            all the variables that have been idenfitied as having even one correlation above the threshold            - The features on the x-axis are those that will be removed. The features on the y-axis            are the correlated features with those on the x-axis        Code adapted from https://seaborn.pydata.org/examples/many_pairwise_correlations.html        &quot;&quot;&quot;        if self.record_collinear is None:            raise NotImplementedError(&apos;Collinear features have not been idenfitied. Run `identify_collinear`.&apos;)        if plot_all:            corr_matrix_plot = self.corr_matrix            title = &apos;All Correlations&apos;        else:            # Identify the correlations that were above the threshold            # columns (x-axis) are features to drop and rows (y_axis) are correlated pairs            corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear[&apos;corr_feature&apos;])),                                                     list(set(self.record_collinear[&apos;drop_feature&apos;]))]            title = &quot;Correlations Above Threshold&quot;        f, ax = plt.subplots(figsize=(10, 8))        # Diverging colormap        cmap = sns.diverging_palette(220, 10, as_cmap=True)        # Draw the heatmap with a color bar        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,                    linewidths=.25, cbar_kws={&quot;shrink&quot;: 0.6})        # Set the ylabels         ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));        # Set the xlabels         ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));        plt.title(title, size = 14)    def plot_feature_importances(self, plot_n = 15, threshold = None):        &quot;&quot;&quot;        Plots `plot_n` most important features and the cumulative importance of features.        If `threshold` is provided, prints the number of features needed to reach `threshold` cumulative importance.        Parameters        --------        plot_n : int, default = 15            Number of most important features to plot. Defaults to 15 or the maximum number of features whichever is smaller        threshold : float, between 0 and 1 default = None            Threshold for printing information about cumulative importances        &quot;&quot;&quot;        if self.record_zero_importance is None:            raise NotImplementedError(&apos;Feature importances have not been determined. Run `idenfity_zero_importance`&apos;)        # Need to adjust number of features if greater than the features in the data        if plot_n &gt; self.feature_importances.shape[0]:            plot_n = self.feature_importances.shape[0] - 1        self.reset_plot()        # Make a horizontal bar chart of feature importances        plt.figure(figsize = (10, 6))        ax = plt.subplot()        # Need to reverse the index to plot most important on top        # There might be a more efficient method to accomplish this        ax.barh(list(reversed(list(self.feature_importances.index[:plot_n]))),                 self.feature_importances[&apos;normalized_importance&apos;][:plot_n],                 align = &apos;center&apos;, edgecolor = &apos;k&apos;)        # Set the yticks and labels        ax.set_yticks(list(reversed(list(self.feature_importances.index[:plot_n]))))        ax.set_yticklabels(self.feature_importances[&apos;feature&apos;][:plot_n], size = 12)        # Plot labeling        plt.xlabel(&apos;Normalized Importance&apos;, size = 16); plt.title(&apos;Feature Importances&apos;, size = 18)        plt.show()        # Cumulative importance plot        plt.figure(figsize = (6, 4))        plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances[&apos;cumulative_importance&apos;], &apos;r-&apos;)        plt.xlabel(&apos;Number of Features&apos;, size = 14); plt.ylabel(&apos;Cumulative Importance&apos;, size = 14);         plt.title(&apos;Cumulative Feature Importance&apos;, size = 16);        if threshold:            # Index of minimum number of features needed for cumulative importance threshold            # np.where returns the index so need to add 1 to have correct number            importance_index = np.min(np.where(self.feature_importances[&apos;cumulative_importance&apos;] &gt; threshold))            plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles=&apos;--&apos;, colors = &apos;blue&apos;)            plt.show();            print(&apos;%d features required for %0.2f of cumulative importance&apos; % (importance_index + 1, threshold))    def reset_plot(self):        plt.rcParams = plt.rcParamsDefault</code></pre><p><strong>2. nmf_list.py 代码分析：</strong></p><pre><code>import pickleimport numpy as npimport pandas as pdfrom collections import Counterfrom sklearn.decomposition import NMFfrom sklearn.preprocessing import LabelEncoderfrom sklearn.feature_extraction.text import TfidfVectorizerimport tqdmfrom gensim.models import FastText, Word2Vecimport multiprocessingclass nmf_list(object):    def __init__(self,data,by_name,to_list,nmf_n,top_n):        self.data = data        self.by_name = by_name        self.to_list = to_list        self.nmf_n = nmf_n        self.top_n = top_n    def run(self,tf_n):        df_all = self.data.groupby(self.by_name)[self.to_list].apply(lambda x :&apos;|&apos;.join(x)).reset_index()        self.data =df_all.copy()        print(&apos;bulid word_fre&apos;)    # 词频的构建    def word_fre(x):        word_dict = []        x = x.split(&apos;|&apos;)        docs = []        for doc in x:            doc = doc.split()            docs.append(doc)            word_dict.extend(doc)        word_dict = Counter(word_dict)        new_word_dict = {}        for key,value in word_dict.items():            new_word_dict[key] = [value,0]        del word_dict          del x        for doc in docs:            doc = Counter(doc)            for word in doc.keys():                new_word_dict[word][1] += 1        return new_word_dict     self.data[&apos;word_fre&apos;] = self.data[self.to_list].apply(word_fre)    print(&apos;bulid top_&apos; + str(self.top_n))    # 设定100个高频词    def top_100(word_dict):        return sorted(word_dict.items(),key = lambda x:(x[1][1],x[1][0]),reverse = True)[:self.top_n]    self.data[&apos;top_&apos;+str(self.top_n)] = self.data[&apos;word_fre&apos;].apply(top_100)    def top_100_word(word_list):        words = []        for i in word_list:            i = list(i)            words.append(i[0])        return words     self.data[&apos;top_&apos;+str(self.top_n)+&apos;_word&apos;] = self.data[&apos;top_&apos; + str(self.top_n)].apply(top_100_word)    # print(&apos;top_&apos;+str(self.top_n)+&apos;_word的shape&apos;)    print(self.data.shape)    word_list = []    for i in self.data[&apos;top_&apos;+str(self.top_n)+&apos;_word&apos;].values:        word_list.extend(i)    word_list = Counter(word_list)    word_list = sorted(word_list.items(),key = lambda x:x[1],reverse = True)    user_fre = []    for i in word_list:        i = list(i)        user_fre.append(i[1]/self.data[self.by_name].nunique())    stop_words = []    for i,j in zip(word_list,user_fre):        if j&gt;0.5:            i = list(i)            stop_words.append(i[0])    print(&apos;start title_feature&apos;)    # 讲融合后的taglist当作一句话进行文本处理    self.data[&apos;title_feature&apos;] = self.data[self.to_list].apply(lambda x: x.split(&apos;|&apos;))    self.data[&apos;title_feature&apos;] = self.data[&apos;title_feature&apos;].apply(lambda line: [w for w in line if w not in stop_words])    self.data[&apos;title_feature&apos;] = self.data[&apos;title_feature&apos;].apply(lambda x: &apos; &apos;.join(x))    print(&apos;start NMF&apos;)    # 使用tfidf对元素进行处理    tfidf_vectorizer = TfidfVectorizer(ngram_range=(tf_n,tf_n))    tfidf = tfidf_vectorizer.fit_transform(self.data[&apos;title_feature&apos;].values)    #使用nmf算法，提取文本的主题分布    text_nmf = NMF(n_components=self.nmf_n).fit_transform(tfidf)    # 整理并输出文件    name = [str(tf_n) + self.to_list + &apos;_&apos; +str(x) for x in range(1,self.nmf_n+1)]    tag_list = pd.DataFrame(text_nmf)    print(tag_list.shape)    tag_list.columns = name    tag_list[self.by_name] = self.data[self.by_name]    column_name = [self.by_name] + name    tag_list = tag_list[column_name]    return tag_list</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;2020-5-30-赛题解读&quot;&gt;&lt;a href=&quot;#2020-5-30-赛题解读&quot; class=&quot;headerlink&quot; title=&quot;2020.5.30 赛题解读&quot;&gt;&lt;/a&gt;2020.5.30 赛题解读&lt;/h2&gt;&lt;h3 id=&quot;赛题背景：渔船作业分类&quot;&gt;&lt;a href=&quot;#赛题背景：渔船作业分类&quot; class=&quot;headerlink&quot; title=&quot;赛题背景：渔船作业分类&quot;&gt;&lt;/a&gt;赛题背景：渔船作业分类&lt;/h3&gt;&lt;p&gt;本赛题基于位置数据对海上目标进行智能识别和作业行为分析，要求选手通过分析渔船北斗设备位置数据，得出该船的生产作业行为，具体判断出是拖网作业、围网作业还是流刺网作业。初赛将提供11000条(其中7000条训练数据、2000条testA、2000条testB)渔船轨迹北斗数据。&lt;/p&gt;
    
    </summary>
    
    
      <category term="竞赛" scheme="https://liangggggg.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="算法竞赛" scheme="https://liangggggg.github.io/tags/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/"/>
    
  </entry>
  
  <entry>
    <title>学习日志：2020华为云大数据挑战赛（正式赛）</title>
    <link href="https://liangggggg.github.io/2020/05/29/hello-world/"/>
    <id>https://liangggggg.github.io/2020/05/29/hello-world/</id>
    <published>2020-05-29T13:15:34.000Z</published>
    <updated>2020-06-03T11:12:54.992Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2020-5-29-正式赛题解读"><a href="#2020-5-29-正式赛题解读" class="headerlink" title="2020.5.29 正式赛题解读"></a>2020.5.29 正式赛题解读</h2><h3 id="赛题背景：船运到达时间预测"><a href="#赛题背景：船运到达时间预测" class="headerlink" title="赛题背景：船运到达时间预测"></a>赛题背景：船运到达时间预测</h3><p>通过船运的历史数据构建模型，对目的港到达时间进行预测，预测时间简称为ETA（estimated time of arrival），目的港到达时间预测为ARRIVAL_ETA。</p><a id="more"></a><p>本次大赛提供<strong>历史运单GPS数据、历史运单事件数据、港口坐标数据</strong>，预测货物运单的到达时间，对应“历史运单事件”数据中EVENT_CODE字段值为ARRIVAL AT PORT时EVENT_CONVOLUTION_DATE的时间值。</p><h3 id="比赛数据"><a href="#比赛数据" class="headerlink" title="比赛数据"></a>比赛数据</h3><p><strong>1. GPS数据（建议重点参考）</strong></p><table><thead><tr><th align="left">列名</th><th align="center">类型</th><th align="right">说明</th></tr></thead><tbody><tr><td align="left">loadingOrder</td><td align="center">VARCHAR2</td><td align="right">脱敏后的主运单，货物的运单编号，类似快递单号</td></tr><tr><td align="left">carrierName</td><td align="center">VARCHAR2</td><td align="right">脱敏后的承运商名称，类似快递公司名称</td></tr><tr><td align="left">timestamp</td><td align="center">DATE</td><td align="right">时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-05T16:33:17.000Z</td></tr><tr><td align="left">longitude</td><td align="center">NUMBER</td><td align="right">货物在运输过程中，当前船舶所处的经度坐标，如114.234567</td></tr><tr><td align="left">latitude</td><td align="center">NUMBER</td><td align="right">货物在运输过程中，当前船舶所处的纬度坐标，如21.234567</td></tr><tr><td align="left">vesselMMSI</td><td align="center">VARCHAR2</td><td align="right">脱敏后的船舶海上移动业务识别码MMSI， 唯一标识，对应到每一艘船</td></tr><tr><td align="left">speed</td><td align="center">NUMBER</td><td align="right">单位km/h，货物在运输过程中，当前船舶的瞬时速度，部分数据未提供的可自行计算。</td></tr><tr><td align="left">direction</td><td align="center">NUMBER</td><td align="right">当前船舶的行驶方向，正北是0度，31480代表西北方向314.80度，900代表正北偏东9度。</td></tr><tr><td align="left">vesselNextport</td><td align="center">VARCHAR2</td><td align="right">船舶将要到达的下一港口，港口名称可能不规范，如CNQIN、CN QIN、CN QINGDAO都代表下一站为中国青岛港口。</td></tr><tr><td align="left">vesselNextportETA</td><td align="center">DATE</td><td align="right">船运公司给出的到“下一个港口”预计到达时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-12T16:33:17.000Z</td></tr><tr><td align="left">vesselStatus</td><td align="center">VARCHAR2</td><td align="right">当前船舶航行状态，主要包括：moored、under way using engine、not under command、at anchor、under way sailing、constrained by her draught</td></tr><tr><td align="left">TRANSPORT_TRACE</td><td align="center">VARCHAR2</td><td align="right">船的路由，由“-”连接组成，例如CNSHK-MYPKG-MYTPP。由承运商预先录入，实际小概率存在不按此路由行驶（如遇塞港时），但最终会到达目的港口。</td></tr></tbody></table><p><strong>数据说明：</strong></p><p>每个运单表示一次运输的运输单号，不会重复使用，一次运输过程中的多条GPS数据拥有相同的运输单号。船号为运单货物所在的船编号，会重复出现在不同次运输的GPS数据中。需要注意的是GPS数据中可能会有异常的GPS，可能且不限于如下问题：</p><p>（1） GPS坐标在陆地，或者有些港口是内陆的港口。</p><p>（2） GPS漂移：两点距离过大，超过船的行驶能力。</p><p>（3） GPS在部分地区的比较稀疏（比如南半球、敏感海域）。</p><p>（4） 最后的GPS点可能和港口的距离较远（比如塞港时，或者临近目的港时已无GPS数据）。</p><p>（5） speed字段之后数据可能会有少量缺失（如GPS设备短暂异常）。</p><p><strong>注意 :</strong> 字段vesselNextport、vesselNextportETA、TRANSPORT_TRACE为手工输入，误填可能性较大。</p><p><strong>2. 历史运单时间数据（手工录入，辅助参考）</strong></p><p>历史运单事件数据描述每个运单在船运的过程中，与港口相关的关键信息，如离开起运港、到达目的港等。</p><table><thead><tr><th align="left">列名</th><th align="center">类型</th><th align="right">说明</th></tr></thead><tbody><tr><td align="left">loadingOrder</td><td align="center">VARCHAR2</td><td align="right">运单号，与历史运单GPS数据中的loadingOrder字段一致</td></tr><tr><td align="left">EVENT_CODE</td><td align="center">VARCHAR2</td><td align="right">事件编码，主要事件包括：TRANSIT PORT ATD实际离开中转港、SHIPMENT ONBOARD DATE实际离开起运港、TRANSIT PORT ATA实际到达中转港、ARRIVAL AT PORT实际到达目的港、注：部分船可能没有中转港</td></tr><tr><td align="left">EVENT_LOCATION_ID</td><td align="center">VARCHAR2</td><td align="right">港口名称，对应“港口坐标据”表中的字段TRANS_NODE_NAME</td></tr><tr><td align="left">EVENT_CONVOLUTION_DATE</td><td align="center">DATE</td><td align="right">事件发生的时间，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格）。例如Event_code为“SHIPMENT ONBOARD DATE”时，此字段表示船从起运港出发的时间。EVENT_CODE为“ARRIVAL AT PORT”时，此字段表示船到达目的港的时间。</td></tr></tbody></table><p><strong>3. 港口坐标数据</strong><br>港口坐标数据描述每个运单在船运的过程中涉及的港口位置信息。</p><table><thead><tr><th align="left">列名</th><th align="center">类型</th><th align="right">说明</th></tr></thead><tbody><tr><td align="left">TRANS_NODE_NAME</td><td align="center">VARCHAR2</td><td align="right">港口名称，如：WAREHOUSE_TURKEYMOSCOW_RUSSIAN FEDERATION，CHIWAN(44)，SHEKOU，深圳蛇口港等</td></tr><tr><td align="left">LONGITUDE</td><td align="center">VARCHAR2</td><td align="right">港口的经度坐标</td></tr><tr><td align="left">LATITUDE</td><td align="center">VARCHAR2</td><td align="right">港口的纬度坐标</td></tr><tr><td align="left">COUNTRY</td><td align="center">VARCHAR2</td><td align="right">国家</td></tr><tr><td align="left">STATE</td><td align="center">VARCHAR2</td><td align="right">省、州</td></tr><tr><td align="left">CITY</td><td align="center">VARCHAR2</td><td align="right">城市</td></tr><tr><td align="left">REGION</td><td align="center">VARCHAR2</td><td align="right">县、区</td></tr><tr><td align="left">ADDRESS</td><td align="center">VARCHAR2</td><td align="right">详细地址。</td></tr><tr><td align="left">PORT_CODE</td><td align="center">VARCHAR2</td><td align="right">港口编码，即港口的字母简码，如CNSHK代表中国蛇口港</td></tr></tbody></table><p><strong>数据说明：</strong><br>（1） 重点为NAME和经纬度数据</p><p>（2） 一个港口可能会有多个NAME表示，且不按五位编码表示</p><p>（3） 经纬度如果出现负数则为错误信息，可以删除或自行补充</p><p><strong>4. 测试运单数据</strong></p><p>测试运单数据为运单运输过程中的不同位置点所构成，供选手测试对应的ETA时间。测试运单数据如下表描述。</p><table><thead><tr><th align="left">列名</th><th align="center">类型</th><th align="right">说明</th></tr></thead><tbody><tr><td align="left">loadingOrder</td><td align="center">VARCHAR2</td><td align="right">脱敏后的主运单，货物的运单编号，类似快递单号</td></tr><tr><td align="left">timestamp</td><td align="center">DATE</td><td align="right">时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-05T16:33:17.000Z</td></tr><tr><td align="left">longitude</td><td align="center">NUMBER</td><td align="right">货物在运输过程中，当前船舶所处的经度坐标，如114.234567</td></tr><tr><td align="left">latitude</td><td align="center">NUMBER</td><td align="right">货物在运输过程中，当前船舶所处的纬度坐标，如21.234567</td></tr><tr><td align="left">speed</td><td align="center">NUMBER</td><td align="right">单位km/h，货物在运输过程中，当前船舶的瞬时速度，部分数据未提供的可自行计算。</td></tr><tr><td align="left">direction</td><td align="center">NUMBER</td><td align="right">当前船舶的行驶方向，正北是0度，31480代表西北方向314.80度，900代表正北偏东9度。</td></tr><tr><td align="left">carrierName</td><td align="center">VARCHAR2</td><td align="right">脱敏后的承运商名称，类似快递公司名称</td></tr><tr><td align="left">vesselMMSI</td><td align="center">VARCHAR2</td><td align="right">脱敏后的船舶海上移动业务识别码MMSI， 唯一标识，对应到每一艘船</td></tr><tr><td align="left">onboardDate</td><td align="center">DATE</td><td align="right">离开起运港时间，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格），如2019/09/05 16:33:17</td></tr><tr><td align="left">TRANSPORT_TRACE</td><td align="center">VARCHAR2</td><td align="right">船的路由，由“-”连接组成，例如CNSHK-MYPKG-MYTPP。由承运商预先录入，实际小概率存在不按此路由行驶（如遇塞港时），但最终会到达目的港口。</td></tr></tbody></table><p><strong>数据说明：</strong><br>（1） 根据航程提供起始一段形成GPS数据，用于预测到达目的港时间（按不同航线，提供10%~50%左右的GPS数据）</p><p><strong>5. 提交数据</strong></p><p>所有参与竞赛的选手登录到大赛平台，提交结果数据，具体提交格式要求：</p><table><thead><tr><th align="left">列名</th><th align="center">类型</th><th align="right">说明</th></tr></thead><tbody><tr><td align="left">loadingOrder</td><td align="center">VARCHAR2</td><td align="right">脱敏后的主运单，货物的运单编号，类似快递单号</td></tr><tr><td align="left">timestamp</td><td align="center">DATE</td><td align="right">时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-05T16:33:17.000Z</td></tr><tr><td align="left">longitude</td><td align="center">NUMBER</td><td align="right">货物在运输过程中，当前船舶所处的经度坐标，如114.234567</td></tr><tr><td align="left">latitude</td><td align="center">NUMBER</td><td align="right">货物在运输过程中，当前船舶所处的纬度坐标，如21.234567</td></tr><tr><td align="left">carrierName</td><td align="center">VARCHAR2</td><td align="right">脱敏后的承运商名称，类似快递公司名称</td></tr><tr><td align="left">vesselMMSI</td><td align="center">VARCHAR2</td><td align="right">脱敏后的船舶海上移动业务识别码MMSI， 唯一标识，对应到每一艘船</td></tr><tr><td align="left">onboardDate</td><td align="center">DATE</td><td align="right">离开起运港时间，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格），如2019/09/05 16:33:17</td></tr><tr><td align="left">ETA</td><td align="center">DATE</td><td align="right">到达目的港口的ETA，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格），如2019/09/18 22:28:46</td></tr><tr><td align="left">creatDate</td><td align="center">DATE</td><td align="right">当前表创建时间，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格），如2020/05/05 16:33:17</td></tr></tbody></table><p><strong>数据说明：</strong><br>（1） 前7列基于测试数据，ETA列为预测时间</p><p>（2） creatDate为文件生成时间，判分不关注。</p><h2 id="2020-6-2-baseline分析"><a href="#2020-6-2-baseline分析" class="headerlink" title="2020.6.2 baseline分析"></a>2020.6.2 baseline分析</h2><p>分析正式赛提供的baseline </p><p><strong>MSE : 176495.2554</strong></p><ul><li>导入所需要的包</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"># from tqdm import tqdm</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from sklearn.metrics import mean_squared_error,explained_variance_score</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">import lightgbm as lgb</span><br><span class="line"></span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&#39;ignore&#39;)</span><br></pre></td></tr></table></figure><ul><li>加载数据</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># baseline只用到gps定位数据，即train_gps_path</span><br><span class="line">train_gps_path &#x3D; &#39;dataset&#x2F;train0523.csv&#39;</span><br><span class="line">test_data_path &#x3D; &#39;test&#x2F;A_testData0531.csv&#39;</span><br><span class="line">order_data_path &#x3D; &#39;dataset&#x2F;loadingOrderEvent.csv&#39;</span><br><span class="line">port_data_path &#x3D; &#39;dataset&#x2F;port.csv&#39;</span><br><span class="line"></span><br><span class="line"># 取前1000000行</span><br><span class="line">debug &#x3D; True</span><br><span class="line">NDATA &#x3D; 1000000</span><br><span class="line"></span><br><span class="line">if debug:</span><br><span class="line">    train_data &#x3D; pd.read_csv(train_gps_path,nrows&#x3D;NDATA,header&#x3D;None)</span><br><span class="line">else:</span><br><span class="line">    train_data &#x3D; pd.read_csv(train_gps_path,header&#x3D;None)</span><br><span class="line"></span><br><span class="line">train_data.columns &#x3D; [&#39;loadingOrder&#39;,&#39;carrierName&#39;,&#39;timestamp&#39;,&#39;longitude&#39;,</span><br><span class="line">                  &#39;latitude&#39;,&#39;vesselMMSI&#39;,&#39;speed&#39;,&#39;direction&#39;,&#39;vesselNextport&#39;,</span><br><span class="line">                  &#39;vesselNextportETA&#39;,&#39;vesselStatus&#39;,&#39;vesselDatasource&#39;,&#39;TRANSPORT_TRACE&#39;]</span><br><span class="line">test_data &#x3D; pd.read_csv(test_data_path)</span><br><span class="line"></span><br><span class="line">def get_data(data, mode&#x3D;&#39;train&#39;):</span><br><span class="line"></span><br><span class="line">assert mode&#x3D;&#x3D;&#39;train&#39; or mode&#x3D;&#x3D;&#39;test&#39;</span><br><span class="line"></span><br><span class="line">if mode&#x3D;&#x3D;&#39;train&#39;:</span><br><span class="line">    data[&#39;vesselNextportETA&#39;] &#x3D; pd.to_datetime(data[&#39;vesselNextportETA&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">elif mode&#x3D;&#x3D;&#39;test&#39;:</span><br><span class="line">    data[&#39;temp_timestamp&#39;] &#x3D; data[&#39;timestamp&#39;]</span><br><span class="line">    data[&#39;onboardDate&#39;] &#x3D; pd.to_datetime(data[&#39;onboardDate&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">data[&#39;timestamp&#39;] &#x3D; pd.to_datetime(data[&#39;timestamp&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">data[&#39;longitude&#39;] &#x3D; data[&#39;longitude&#39;].astype(float)</span><br><span class="line">data[&#39;loadingOrder&#39;] &#x3D; data[&#39;loadingOrder&#39;].astype(str)</span><br><span class="line">data[&#39;latitude&#39;] &#x3D; data[&#39;latitude&#39;].astype(float)</span><br><span class="line">data[&#39;speed&#39;] &#x3D; data[&#39;speed&#39;].astype(float)</span><br><span class="line">data[&#39;direction&#39;] &#x3D; data[&#39;direction&#39;].astype(float)</span><br><span class="line"></span><br><span class="line">return data</span><br><span class="line"></span><br><span class="line">train_data &#x3D; get_data(train_data, mode&#x3D;&#39;train&#39;)</span><br><span class="line">test_data &#x3D; get_data(test_data, mode&#x3D;&#39;test&#39;)</span><br></pre></td></tr></table></figure><p>由于训练集过于庞大，因此baseline只选取了前100万条数据，因此，我们应该可以通过清洗数据来选取合适的训练数据来降低MSE。</p><ul><li>获取特征，目前baseline的特征只选择经纬度、速度\方向。我们需要进一步可视化数据集，构造更加合理的特征。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># 代码参考：https:&#x2F;&#x2F;github.com&#x2F;juzstu&#x2F;TianChi_HaiYang</span><br><span class="line">def get_feature(df, mode&#x3D;&#39;train&#39;):</span><br><span class="line">    </span><br><span class="line">    assert mode&#x3D;&#x3D;&#39;train&#39; or mode&#x3D;&#x3D;&#39;test&#39;</span><br><span class="line">    </span><br><span class="line">    df.sort_values([&#39;loadingOrder&#39;, &#39;timestamp&#39;], inplace&#x3D;True)</span><br><span class="line">    # 特征只选择经纬度、速度\方向</span><br><span class="line">    df[&#39;lat_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;latitude&#39;].diff(1)</span><br><span class="line">    df[&#39;lon_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;longitude&#39;].diff(1)</span><br><span class="line">    df[&#39;speed_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;speed&#39;].diff(1)</span><br><span class="line">    df[&#39;diff_minutes&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].diff(1).dt.total_seconds() &#x2F;&#x2F; 60</span><br><span class="line">    df[&#39;anchor&#39;] &#x3D; df.apply(lambda x: 1 if x[&#39;lat_diff&#39;] &lt;&#x3D; 0.03 and x[&#39;lon_diff&#39;] &lt;&#x3D; 0.03</span><br><span class="line">                            and x[&#39;speed_diff&#39;] &lt;&#x3D; 0.3 and x[&#39;diff_minutes&#39;] &lt;&#x3D; 10 else 0, axis&#x3D;1)</span><br><span class="line">    </span><br><span class="line">    if mode&#x3D;&#x3D;&#39;train&#39;:</span><br><span class="line">        group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(mmax&#x3D;&#39;max&#39;, count&#x3D;&#39;count&#39;, mmin&#x3D;&#39;min&#39;).reset_index()</span><br><span class="line">        # 读取数据的最大值-最小值，即确认时间间隔为label</span><br><span class="line">        group_df[&#39;label&#39;] &#x3D; (group_df[&#39;mmax&#39;] - group_df[&#39;mmin&#39;]).dt.total_seconds()</span><br><span class="line">    elif mode&#x3D;&#x3D;&#39;test&#39;:</span><br><span class="line">        group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(count&#x3D;&#39;count&#39;).reset_index()</span><br><span class="line">        </span><br><span class="line">    anchor_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;anchor&#39;].agg(&#39;sum&#39;).reset_index()</span><br><span class="line">    anchor_df.columns &#x3D; [&#39;loadingOrder&#39;, &#39;anchor_cnt&#39;]</span><br><span class="line">    group_df &#x3D; group_df.merge(anchor_df, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line">    group_df[&#39;anchor_ratio&#39;] &#x3D; group_df[&#39;anchor_cnt&#39;] &#x2F; group_df[&#39;count&#39;]</span><br><span class="line"></span><br><span class="line">    agg_function &#x3D; [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;median&#39;]</span><br><span class="line">    agg_col &#x3D; [&#39;latitude&#39;, &#39;longitude&#39;, &#39;speed&#39;, &#39;direction&#39;]</span><br><span class="line"></span><br><span class="line">    group &#x3D; df.groupby(&#39;loadingOrder&#39;)[agg_col].agg(agg_function).reset_index()</span><br><span class="line">    group.columns &#x3D; [&#39;loadingOrder&#39;] + [&#39;&#123;&#125;_&#123;&#125;&#39;.format(i, j) for i in agg_col for j in agg_function]</span><br><span class="line">    group_df &#x3D; group_df.merge(group, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line"></span><br><span class="line">    return group_df</span><br><span class="line">    </span><br><span class="line">train &#x3D; get_feature(train_data, mode&#x3D;&#39;train&#39;)</span><br><span class="line">test &#x3D; get_feature(test_data, mode&#x3D;&#39;test&#39;)</span><br><span class="line">features &#x3D; [c for c in train.columns if c not in [&#39;loadingOrder&#39;, &#39;label&#39;, &#39;mmin&#39;, &#39;mmax&#39;, &#39;count&#39;]]</span><br></pre></td></tr></table></figure><p>这部分代码可能由于pandas版本问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(mmax&#x3D;&#39;max&#39;, count&#x3D;&#39;count&#39;, mmin&#x3D;&#39;min&#39;).reset_index()</span><br></pre></td></tr></table></figure><p>需要修改为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(&#39;mmax&#39;:&#39;max&#39;, &#39;count&#39;:&#39;count&#39;, &#39;mmin&#39;:&#39;min&#39;).reset_index()</span><br></pre></td></tr></table></figure><ul><li>使用十折交叉验证构造模型</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">def mse_score_eval(preds, valid):</span><br><span class="line">    labels &#x3D; valid.get_label()</span><br><span class="line">    scores &#x3D; mean_squared_error(y_true&#x3D;labels, y_pred&#x3D;preds)</span><br><span class="line">    return &#39;mse_score&#39;, scores, True</span><br><span class="line"></span><br><span class="line">def build_model(train, test, pred, label, seed&#x3D;1080, is_shuffle&#x3D;True):</span><br><span class="line">    train_pred &#x3D; np.zeros((train.shape[0], ))</span><br><span class="line">    test_pred &#x3D; np.zeros((test.shape[0], ))</span><br><span class="line">    n_splits &#x3D; 10</span><br><span class="line">    # Kfold</span><br><span class="line">    fold &#x3D; KFold(n_splits&#x3D;n_splits, shuffle&#x3D;is_shuffle, random_state&#x3D;seed)</span><br><span class="line">    kf_way &#x3D; fold.split(train[pred])</span><br><span class="line">    # params</span><br><span class="line">    params &#x3D; &#123;</span><br><span class="line">        &#39;learning_rate&#39;: 0.01,</span><br><span class="line">        &#39;boosting_type&#39;: &#39;gbdt&#39;,</span><br><span class="line">        &#39;objective&#39;: &#39;regression&#39;,</span><br><span class="line">        &#39;num_leaves&#39;: 36,</span><br><span class="line">        &#39;feature_fraction&#39;: 0.6,</span><br><span class="line">        &#39;bagging_fraction&#39;: 0.7,</span><br><span class="line">        &#39;bagging_freq&#39;: 6,</span><br><span class="line">        &#39;seed&#39;: 8,</span><br><span class="line">        &#39;bagging_seed&#39;: 1,</span><br><span class="line">        &#39;feature_fraction_seed&#39;: 7,</span><br><span class="line">        &#39;min_data_in_leaf&#39;: 20,</span><br><span class="line">        &#39;nthread&#39;: 8,</span><br><span class="line">        &#39;verbose&#39;: 1,</span><br><span class="line">    &#125;</span><br><span class="line">    # train</span><br><span class="line">    for n_fold, (train_idx, valid_idx) in enumerate(kf_way, start&#x3D;1):</span><br><span class="line">        train_x, train_y &#x3D; train[pred].iloc[train_idx], train[label].iloc[train_idx]</span><br><span class="line">        valid_x, valid_y &#x3D; train[pred].iloc[valid_idx], train[label].iloc[valid_idx]</span><br><span class="line">        # 数据加载</span><br><span class="line">        n_train &#x3D; lgb.Dataset(train_x, label&#x3D;train_y)</span><br><span class="line">        n_valid &#x3D; lgb.Dataset(valid_x, label&#x3D;valid_y)</span><br><span class="line"></span><br><span class="line">        clf &#x3D; lgb.train(</span><br><span class="line">            params&#x3D;params,</span><br><span class="line">            train_set&#x3D;n_train,</span><br><span class="line">            num_boost_round&#x3D;3000,</span><br><span class="line">            valid_sets&#x3D;[n_valid],</span><br><span class="line">            early_stopping_rounds&#x3D;100,</span><br><span class="line">            verbose_eval&#x3D;100,</span><br><span class="line">            feval&#x3D;mse_score_eval</span><br><span class="line">        )</span><br><span class="line">        train_pred[valid_idx] &#x3D; clf.predict(valid_x, num_iteration&#x3D;clf.best_iteration)</span><br><span class="line">        test_pred +&#x3D; clf.predict(test[pred], num_iteration&#x3D;clf.best_iteration)&#x2F;fold.n_splits</span><br><span class="line">    </span><br><span class="line">    test[&#39;label&#39;] &#x3D; test_pred</span><br><span class="line">    </span><br><span class="line">    return test[[&#39;loadingOrder&#39;, &#39;label&#39;]]</span><br><span class="line"></span><br><span class="line">result &#x3D; build_model(train, test, features, &#39;label&#39;, is_shuffle&#x3D;True)</span><br></pre></td></tr></table></figure><ul><li>生成最终的测试文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">test_data &#x3D; test_data.merge(result, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line">test_data[&#39;ETA&#39;] &#x3D; (test_data[&#39;onboardDate&#39;] + test_data[&#39;label&#39;].apply(lambda x:pd.Timedelta(seconds&#x3D;x))).apply(lambda x:x.strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;))</span><br><span class="line">test_data.drop([&#39;direction&#39;,&#39;TRANSPORT_TRACE&#39;],axis&#x3D;1,inplace&#x3D;True)</span><br><span class="line">test_data[&#39;onboardDate&#39;] &#x3D; test_data[&#39;onboardDate&#39;].apply(lambda x:x.strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;))</span><br><span class="line">test_data[&#39;creatDate&#39;] &#x3D; pd.datetime.now().strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;)</span><br><span class="line">test_data[&#39;timestamp&#39;] &#x3D; test_data[&#39;temp_timestamp&#39;]</span><br><span class="line"># 整理columns顺序</span><br><span class="line">result &#x3D; test_data[[&#39;loadingOrder&#39;, &#39;timestamp&#39;, &#39;longitude&#39;, &#39;latitude&#39;, &#39;carrierName&#39;, &#39;vesselMMSI&#39;, &#39;onboardDate&#39;, &#39;ETA&#39;, &#39;creatDate&#39;]]</span><br><span class="line"></span><br><span class="line">result.to_csv(&#39;result.csv&#39;, index&#x3D;False)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;2020-5-29-正式赛题解读&quot;&gt;&lt;a href=&quot;#2020-5-29-正式赛题解读&quot; class=&quot;headerlink&quot; title=&quot;2020.5.29 正式赛题解读&quot;&gt;&lt;/a&gt;2020.5.29 正式赛题解读&lt;/h2&gt;&lt;h3 id=&quot;赛题背景：船运到达时间预测&quot;&gt;&lt;a href=&quot;#赛题背景：船运到达时间预测&quot; class=&quot;headerlink&quot; title=&quot;赛题背景：船运到达时间预测&quot;&gt;&lt;/a&gt;赛题背景：船运到达时间预测&lt;/h3&gt;&lt;p&gt;通过船运的历史数据构建模型，对目的港到达时间进行预测，预测时间简称为ETA（estimated time of arrival），目的港到达时间预测为ARRIVAL_ETA。&lt;/p&gt;
    
    </summary>
    
    
      <category term="竞赛" scheme="https://liangggggg.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="算法竞赛" scheme="https://liangggggg.github.io/tags/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/"/>
    
  </entry>
  
</feed>
