<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/L.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/L.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"liangggggg.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1 支持向量机（SVM）原理从算法的功能来看，SVM几乎涵盖了机器学习的所有算法需求     功能    有监督学习 线性二分类与多分类（Linear Support Vector Classiﬁcation）    非线性二分类与多分类（Support Vector Classiﬁcation, SVC）    普通连续型变量的回归（Support Vector Regression）    概">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记（二）支持向量机（SVM）">
<meta property="og:url" content="https://liangggggg.github.io/2020/07/16/svm/index.html">
<meta property="og:site_name" content="LiAnG&#39;s Blog">
<meta property="og:description" content="1 支持向量机（SVM）原理从算法的功能来看，SVM几乎涵盖了机器学习的所有算法需求     功能    有监督学习 线性二分类与多分类（Linear Support Vector Classiﬁcation）    非线性二分类与多分类（Support Vector Classiﬁcation, SVC）    普通连续型变量的回归（Support Vector Regression）    概">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/A4A7E90B88454F098F8BDBDCCF02B27D?method=download&shareKey=a5c90f8c069094d544bacfbb471dbd02">
<meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/F29C776BA1854D9C87B9BA1EEFF2F3B9?method=download&shareKey=f6addba1a1e3223deb09059443ef2cf1">
<meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/B26C385AAC024C1791BBFDDF643F3D01?method=download&shareKey=aa3e06d8df0f9b472581aa1282400ae3">
<meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/0D76DD0804FD486EA95C591CDF1335C5?method=download&shareKey=c5e8aae3bd6137b2f4c315008302565b">
<meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/C2FFA1AA25854902ACF46B5A0C5D3410?method=download&shareKey=95be7710c6d12317d6fbda16a8bd436e">
<meta property="article:published_time" content="2020-07-16T13:24:13.000Z">
<meta property="article:modified_time" content="2020-07-18T06:27:12.058Z">
<meta property="article:author" content="LiAnG">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://note.youdao.com/yws/api/personal/file/A4A7E90B88454F098F8BDBDCCF02B27D?method=download&shareKey=a5c90f8c069094d544bacfbb471dbd02">

<link rel="canonical" href="https://liangggggg.github.io/2020/07/16/svm/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习笔记（二）支持向量机（SVM） | LiAnG's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="LiAnG's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">LiAnG's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Stay hungry, stay foolish</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://liangggggg.github.io/2020/07/16/svm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/timg.jpg">
      <meta itemprop="name" content="LiAnG">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiAnG's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习笔记（二）支持向量机（SVM）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-16 21:24:13" itemprop="dateCreated datePublished" datetime="2020-07-16T21:24:13+08:00">2020-07-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-18 14:27:12" itemprop="dateModified" datetime="2020-07-18T14:27:12+08:00">2020-07-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/07/16/svm/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/07/16/svm/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="1-支持向量机（SVM）原理"><a href="#1-支持向量机（SVM）原理" class="headerlink" title="1 支持向量机（SVM）原理"></a>1 支持向量机（SVM）原理</h1><p>从算法的功能来看，SVM几乎涵盖了机器学习的所有算法需求</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">功能</th>
</tr>
</thead>
<tbody><tr>
<td align="left">有监督学习</td>
<td align="left">线性二分类与多分类（Linear Support Vector Classiﬁcation）</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">非线性二分类与多分类（Support Vector Classiﬁcation, SVC）</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">普通连续型变量的回归（Support Vector Regression）</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">概率型连续变量的回归（Bayesian SVM）</td>
</tr>
<tr>
<td align="left">无监督学习</td>
<td align="left">支持向量聚类（Support Vector Clustering，SVC）</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">异常值检测（One-class SVM）</td>
</tr>
<tr>
<td align="left">半监督学习</td>
<td align="left">转导支持向量机（Transductive Support Vector Machines，TSVM）</td>
</tr>
</tbody></table>
<a id="more"></a>
<h2 id="1-1-线性SVM用于分类的原理"><a href="#1-1-线性SVM用于分类的原理" class="headerlink" title="1.1 线性SVM用于分类的原理"></a>1.1 线性SVM用于分类的原理</h2><p>要理解SVM的损失函数，先定义决策边界，假设数据中共计有$N$个训练样本，每个训练样本$i$可以被表示为$(x_i,y_i)(i=1,2,\dots N)$，其中$x_i$是$(x_{1i},x_{2i}\dots x_{ni})^T$这样一个特征向量，每个样本总共包含$n$个特征。</p>
<p>二分类标签$y_i$取值为${-1,1}$</p>
<p>若$n=2$，则有$i=(x_{1i},x_{2i},y_i)^T$，在二维平面上，以$x_2$为横坐标，$x_1$为从坐标，$y$为颜色，来可视化所有的$N$个样本</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/A4A7E90B88454F098F8BDBDCCF02B27D?method=download&shareKey=a5c90f8c069094d544bacfbb471dbd02" alt></p>
<p>所以要在这个数据集上找寻一个决策边界(超平面)，让紫色点的标签为1，红色点的标签为-1</p>
<p>二维平面上，任意一条线可以表示为：<br>$$x_1 = ax_2+b$$</p>
<p>将表达式变换：<br>$$0=ax_2-x_1+b$$<br>$$0=(a,-1)*\binom{x_2}{x_1}+b$$<br>$$0=w^Tx+b$$</p>
<p>其中$(a,-1)$是参数向量$w$，$x$是特征向量，$b$是截距，在SVM中，使用这个表达式表示决策边界。</p>
<p><strong>我们的目标是求解能够让边际最大化的决策边界</strong></p>
<p>如果在决策边界上任意取两个点$x_a,x_b$，并代入决策边界的表达式，则有：<br>$$w^Tx_a+b=0$$<br>$$w^Tx_b+b=0$$<br>两式相减，可以得到：<br>$$w^T*(x_a-x_b)=0$$</p>
<p>两个向量的点积为0，证明参数向量$w$的方向垂直与我们的决策边界</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/F29C776BA1854D9C87B9BA1EEFF2F3B9?method=download&shareKey=f6addba1a1e3223deb09059443ef2cf1" alt></p>
<p>由于紫色的点代表标签$y=1，y=-1$，因此以下式判定：<br>$$<br>y=<br>\begin{cases}<br>1, &amp;if\ w*x_t+b&gt;0\\<br>-1, &amp;if\ w * x_t+b&lt;0<br>\end{cases}<br>$$</p>
<p><strong>注意：决策边界以上的点都为正，以下的点都为负，是人为规定的，不会影响对参数$w$和截距$b$的求解</strong></p>
<p>决策边界的两边要有两个超平面，两个超平面在二维空间中是两条平行线（虚线超平面），而他们之间的距离就是我们的边际$d$。<br>我们将这两条平行线表示为：</p>
<p>$$w * x+b=k, w * x+b=-k$$</p>
<p>两边同时除以$k$,则可以得到：</p>
<p>$$w * x+b=1, w * x+b=-1$$</p>
<p>表达式两边1和-1分别表示了两条平行决策边界的虚线到决策边界的相对距离。让这两条线分别过两类数据中距离决策边界最近的点，这些点被称为“支持向量”。</p>
<p>令紫色类的点为$x_p$，红色类的点为$x_r$，则可以得到：</p>
<p>$$w * x_p+b=1,w * x_r+b=-1$$</p>
<p>两个式相减，则有：<br>$$w * (w_p-w_r)=2$$<br>如下图所示，$(x_p-x_r)$可以表示为两点之间的连线，而边际$d$是平行于$w$的。</p>
<p>又因为以下数学性质：</p>
<table>
<thead>
<tr>
<th align="left">线性代数中模长的运用</th>
</tr>
</thead>
<tbody><tr>
<td align="left">向量$b$除以自身的模长可以得到$b$方向上的单位向量</td>
</tr>
<tr>
<td align="left">向量a乘以向量b方向上的单位向量，可以得到向量a在向量b方向上的投影的长度。</td>
</tr>
</tbody></table>
<p>所以，将上述式子两边同时除以$||w||$, 则可以得到：<br>$$\frac{w*(x_p-x_r)}{||w||}=\frac{2}{||w||}$$<br>$$d = \frac{2}{||w||}$$</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/B26C385AAC024C1791BBFDDF643F3D01?method=download&shareKey=aa3e06d8df0f9b472581aa1282400ae3" alt></p>
<p>所以，求最大边界，对应的就是最大化d,求解以下函数的最小值：<br>$$f(w)=\frac{||w||^2}{2}$$</p>
<p>对于任意样本，可以把决策函数写作：<br>$$w * x_i+b&gt;=1\   if\ y_i=1$$<br>$$w * x_i+b&lt;=-1 \  if\ y_i=-1$$<br>将两个式子整合：<br>$$y_i(w*x_i+b)&gt;=1, i=1,2,\dots N$$</p>
<h2 id="1-2-线性SVM的拉格朗日对偶函数和决策函数"><a href="#1-2-线性SVM的拉格朗日对偶函数和决策函数" class="headerlink" title="1.2 线性SVM的拉格朗日对偶函数和决策函数"></a>1.2 线性SVM的拉格朗日对偶函数和决策函数</h2><p>损失函数分为两个部分：最要最小化的函数，以及参数求解后必须满足的约束条件</p>
<h3 id="1-2-1-将损失函数从最初形态转换为拉格朗日乘数形态"><a href="#1-2-1-将损失函数从最初形态转换为拉格朗日乘数形态" class="headerlink" title="1.2.1 将损失函数从最初形态转换为拉格朗日乘数形态"></a>1.2.1 将损失函数从最初形态转换为拉格朗日乘数形态</h3><p><strong>1.为什么要进行转换？</strong></p>
<p>我们的目标是求解让损失函数最小化的$w$，希望能够找出一种方式，能在满足条件$y_i(w*x_i+b)&gt;=1$，因此，一种业界认可的方式是使用拉格朗日乘数法(standard Lagrange multiplier method)</p>
<p><strong>2.为什么可以进行转换</strong></p>
<p>拉格朗日乘数法正好可以用来解决凸优化问题，使用拉格朗日乘数法将损失函数改写为考虑了约束条件的形式：<br>$$L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w*x_i+b)-1) (\alpha_i&gt;=0)$$</p>
<p>其中，$\alpha_i$叫作拉格朗日乘数。</p>
<p><strong>3.怎样进行转换</strong></p>
<p>拉格朗日函数分为两个部分，第一部分是原始损失函数，第二部分是约束条件。我们希望，$L(w,b,\alpha)$在满足约束条件下，最小化损失函数。所以要先以$\alpha$为参数，求解$L(w,b,\alpha)$的最大值，在以$w,b$为参数，求解$L(w,b,\alpha)$的最小值。</p>
<p>因此，我们的目标可写作：</p>
<p>$$\min_{w,b}\max_{\alpha_i&gt;=0}L(w,b,\alpha) (\alpha_i&gt;=0)$$</p>
<p>首先，第一步执行max，即最大化$L(w,b,\alpha)$，有两种情况：</p>
<ul>
<li>当$y_i(w * x_i+b)&gt;1$，函数的第二部分$\sum_{i=1}^N\alpha_i(y_i(w*x_i+b)-1)$一定为正，式子$\frac{1}{2}||w||^2$就要减去一个正数，此时若需要最大化$L(w,b,\alpha)$,则$\alpha$必须取到0</li>
<li>当$y_i(w * x_i+b)&lt;1$，函数的第二部分$\sum_{i=1}^N\alpha_i(y_i(w*x_i+b)-1)$一定为负，式子$\frac{1}{2}||w||^2$就要减去一个负数，此时若需要最大化$L(w,b,\alpha)$,则$\alpha$必须取到正无穷</li>
</ul>
<p>因此，可以把函数的第二部分当做一个惩罚项，只有当$y_i(w*x_i+b)&gt;1$时函数没有受到惩罚，从而在求解最小值的时候让约束条件满足</p>
<p>现在，$L(w,b,\alpha)$是我们新的损失函数，目标要通过先最大化，再最小化求解$w，b$</p>
<h3 id="1-2-2-将拉格朗日函数转换为拉格朗日对偶函数"><a href="#1-2-2-将拉格朗日函数转换为拉格朗日对偶函数" class="headerlink" title="1.2.2 将拉格朗日函数转换为拉格朗日对偶函数"></a>1.2.2 将拉格朗日函数转换为拉格朗日对偶函数</h3><p><strong>1.为什么要进行转换？</strong><br>求极值，最简单的方法是对参数求导后让一阶导数为0，这里对参数$w,b$分别求偏导并使其为0</p>
<p>$$<br>\begin{equation}\begin{split}<br>L(w,b,\alpha)&amp;=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w * x_i+b)-1)\\<br>&amp;=\frac{1}{2}||w||^2-\sum_{i=1}^N(\alpha_iy_iw * x_i+\alpha_iy_ib-\alpha_i) \\<br>&amp; =\frac{1}{2}||w||^2-\sum_{i=1}^N(\alpha_iy_iw * x_i)-\sum_{i=1}^N\alpha_iy_ib+\sum_{i=1}^N\alpha_i\\<br>&amp;=\frac{1}{2}(w^Tw)^{\frac{1}{2}*2}-\sum_{i=1}^N(\alpha_iy_iw * x_i)-\sum_{i=1}^N\alpha_iy_ib+\sum_{i=1}^N\alpha_i<br>\end{split}\end{equation}<br>$$</p>
<p>$$<br>\begin{equation}\begin{split}<br>\frac{\partial L(w,b,\alpha)}{\partial w}&amp;=\frac{1}{2}*2w-\sum_{i=1}^N\alpha_iy_ix_i \\<br>&amp;=w-\sum_{i=1}^N\alpha_iy_ix_i=0 \to w = \sum_{i=1}^N\alpha_iy_ix_i<br>\end{split}\end{equation}<br>$$</p>
<p>$$<br>\begin{equation}\begin{split}<br>\frac{\partial L(w,b,\alpha)}{\partial b}&amp;=\sum_{i=1}^N\alpha_iy_i=0 \to \sum_{i=1}^N\alpha_iy_i=0 \\<br>\end{split}\end{equation}<br>$$</p>
<p>由于两个求偏导结果中都带有未知的拉格朗日乘数$\alpha_i$，因此需要有一种方法来求解拉格朗日乘数，而拉格朗日对偶函数，是只带有$alpha_i$而不带有$w,b$的形式，可以求解$\alpha_i$然后再代入（2）、（3）式中来求解参数$w,b$</p>
<p><strong>2.为什么能够进行转换</strong></p>
<p>对于任意一个拉格朗日函数$L(x,\alpha)=f(x)+\sum_{i=1}^q\alpha_ih_i(x)$，都存在一个与它对应的对偶函数$g(\alpha)$，只带有拉格朗日乘数$\alpha$作为唯一参数。如果$L(x,\alpha)$存在最优解并可以表示为$\min_xL(x,\alpha)$，并且对偶函数的最优解也存在并可以表示为$\max_\alpha g(\alpha)$，则可以定义对偶差异(dual gap)<br>$$\Delta = \min_{x}L(x,\alpha)-\max_\alpha g(\alpha)$$</p>
<p>如果$\Delta=0$,则称$L(x,\alpha)$与其对偶函数之间存在强对偶关系，此时可以通过求其对偶函数的最优解来代替原始函数。</p>
<p>强对偶关系存在需要满足KKT(Karush-Kuhn-Tucker)条件：<br>$$<br>\begin{equation}\begin{split}<br>\frac{\partial L}{\partial x_i}=0,\forall_i = 1,2,\dots,d \\<br>h_i(x)&lt;=0,\forall_i = 1,2,\dots,q \\<br>\alpha_i&gt;=0,\forall_i = 1,2,\dots,q \\<br>\alpha_ih_i(x)=0,\forall_i = 1,2,\dots,q \\<br>\end{split}\end{equation}<br>$$</p>
<p>如果我们可以让KKT条件全部成立，则可以求解出$L(w,b,\alpha)$的对偶函数来求解$\alpha$</p>
<p>之前求偏导时，得到<br>$$w = \sum_{i=1}^N\alpha_iy_ix_i$$<br>$$\sum_{i=1}^N\alpha_iy_i=0$$</p>
<p>并且满足：<br>$$-(y(w*x_i+b)-1)&lt;=0, \alpha_i&gt;=0$$</p>
<p>接下来，只需要满足：<br>$$\alpha_i(y_i(w*x_i+b)-1)=0$$</p>
<p>当这个条件满足时，能够让$y_i(w*x_i+b)-1=0$是在虚线的超平面上的样本点，即我们的支持向量，其余样本点必须满足$\alpha_i=0$。</p>
<p>现在KKT的五个条件都得到了满足，可以使用对偶函数求解。</p>
<p><strong>3.怎样进行转换</strong></p>
<p>首先将求导为0代入原始：<br>$$<br>\begin{equation}\begin{split}<br>L(w,b,\alpha)&amp;=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w * x_i+b)-1)\\<br>&amp;=\frac{1}{2}||w||^2-\sum_{i=1}^N(\alpha_iy_iw * x_i+\alpha_iy_ib-\alpha_i) \\<br>&amp; =\frac{1}{2}||w||^2-\sum_{i=1}^N(\alpha_iy_iw * x_i)-\sum_{i=1}^N\alpha_iy_ib+\sum_{i=1}^N\alpha_i\\<br>&amp;=\frac{1}{2}(w^Tw)^{\frac{1}{2}*2}-w\sum_{i=1}^N(\alpha_iy_i * x_i)-b\sum_{i=1}^N\alpha_iy_i+\sum_{i=1}^N\alpha_i \\<br>\end{split}\end{equation}</p>
<p>将（2）、（3）式代入则有<br>$$=\frac{1}{2}w^Tw-w^Tw+\sum_{i=1}^N\alpha_i $$</p>
<p>$$=-\frac{1}{2}w^Tw+\sum_{i=1}^N\alpha_i$$</p>
<p>将（2）式代入，则有：<br>$$=-\frac{1}{2}\sum_{i=1}^N\alpha_iy_ix_i^T * \sum_{i=1}^N\alpha_iy_ix_i+\sum_{i=1}^N\alpha_i$$</p>
<p>令这两个$w$源于不同的特征和标签：<br>$$=-\frac{1}{2}\sum_{i=1,j=1}^N\alpha_iy_ix_i^T\alpha_iy_ix_i+\sum_{i=1}^N\alpha_i$$</p>
<p>$$=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j$$</p>
<p>将矩阵相乘转换为内积形式：<br>$$L_d = \sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^N\alpha_i\alpha_jy_iy_jx_i * x_j$$</p>
<p>函数$L_d$就是对偶函数，其对偶差异$L(w,b,\alpha)$和$L_d$，则有：<br>$$\Delta = \min_{w,b}\max_{\alpha_i&gt;=0}L(w,b,\alpha)-\max_{\alpha_i&gt;=0}L_d$$</p>
<p>又因为在推导$L_d$时，对$L(w,b,\alpha)$偏导为0，所以可以把公式写成：<br>$$\Delta = \min_{w,b}\max_{\alpha_i&gt;=0}L(w,b,\alpha)-\max_{\alpha_i&gt;=0}\min_{w,b}L(w,b,\alpha)$$</p>
<p>又因为所有KKT条件满足，所以：<br>$$ \min_{w,b}\max_{\alpha_i&gt;=0}L(w,b,\alpha)=\max_{\alpha_i&gt;=0}\min_{w,b}L(w,b,\alpha)$$</p>
<p>所以，我们只需要求解对偶函数的最大值就可以了，最终，目标函数变化为：<br>$$\max_{\alpha_i&gt;=0}(\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^N\alpha_i\alpha_jy_iy_jx_i * x_j)$$</p>
<h3 id="1-2-3-求解拉格朗日对偶函数极值及其后续过程"><a href="#1-2-3-求解拉格朗日对偶函数极值及其后续过程" class="headerlink" title="1.2.3 求解拉格朗日对偶函数极值及其后续过程"></a>1.2.3 求解拉格朗日对偶函数极值及其后续过程</h3><p>需要使用梯度下降、SMO或者二次规划(QP, quadratic programming)来求解，但是这一过程对数学要求的程度已经远远超出了需要的程度，因此不再深入了解。</p>
<p>只需要知道，一旦求到了$\alpha$值，就可以求得决策边界参数$w,b$</p>
<p>当求得参数后，就可以得到决策边界的表达式：<br>$$f(x_{test})=sign(w*x_{test}+b)$$</p>
<p>其中$x_{test}$是任意测试样本，$sign(h)$是符号函数。</p>
<p>至此，我们对SVM的原理已经有了一个比较深刻的理解。</p>
<h2 id="1-3-非线性SVM与核函数"><a href="#1-3-非线性SVM与核函数" class="headerlink" title="1.3 非线性SVM与核函数"></a>1.3 非线性SVM与核函数</h2><h3 id="1-3-1-SVC在非线性数据上的推广"><a href="#1-3-1-SVC在非线性数据上的推广" class="headerlink" title="1.3.1 SVC在非线性数据上的推广"></a>1.3.1 SVC在非线性数据上的推广</h3><p>为了找出非线性数据的线性决策边界，我们需要将数据从原始空间$x$，投射到新空间$\Phi(x)$中，$\Phi(x)$是一个映射函数，代表某种非线性变换。线性SVM的原理可以很容易推广到非线性情况下，推导过程和逻辑都与线性SVM一样，只是在定义决策边界之前，需要对数据升维，将原始的$x$，转换成$\Phi(x)$</p>
<p>因此，非线性SVM的损失函数初始形态为：<br>$$\min_{w,b}\frac{||w||^2}{2}$$<br>$$subject\  to\  y_i(w * \Phi(x)+b&gt;=1), i=1,2,\dots,N$$<br>拉格朗日和拉格朗日对偶函数也可得：<br>$$L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w * \Phi(x)+b)-1)$$<br>$$L_d = \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j\Phi(x_i)\Phi(x_j)$$</p>
<p>同理可以使用相同的推导方式让拉格朗日函数满足KKT条件，并且对每个参数求导，再使用梯度下降或SMO等方式求解，最终得到决策边界函数：<br>$$f(x_{test})=sign(w*\Phi(x_{test})+b)$$</p>
<h3 id="1-3-2-重要参数kernel"><a href="#1-3-2-重要参数kernel" class="headerlink" title="1.3.2 重要参数kernel"></a>1.3.2 重要参数kernel</h3><p>但是，我们不清楚什么样的数据应该使用什么类型的映射函数确保在变换空间中可以找出决策边界。</p>
<table>
<thead>
<tr>
<th align="left">关键概念：核函数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">解决这个问题的数学方式，叫作“核技巧”(Kernel Trick)，是一种讷讷够使用数据原始空间中的向量计算表示生维后的空间中的点积结果的数学方式$K(u,v)=\Phi(u)*\Phi(v)$,而这个点积函数$K(u,v)$，就被叫作“核函数”(Kernel Funciton)</td>
</tr>
</tbody></table>
<p>选用不同的核函数，就可以解决不同数据分布下寻找超平面的问题。以下是“kernel”在Sklearn中的几种选项：</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/0D76DD0804FD486EA95C591CDF1335C5?method=download&shareKey=c5e8aae3bd6137b2f4c315008302565b" alt></p>
<h3 id="1-3-3-核函数的优势和缺陷"><a href="#1-3-3-核函数的优势和缺陷" class="headerlink" title="1.3.3 核函数的优势和缺陷"></a>1.3.3 核函数的优势和缺陷</h3><ol>
<li>线性核，尤其是多项式核：在高次项计算非常缓慢</li>
<li>rbf和多项式核不擅长处理量纲不统一的数据集</li>
</ol>
<p><strong>因此，SVM执行之前，推荐先进行数据的无量纲化处理</strong></p>
<h3 id="1-3-4-核函数相关的参数：degree-amp-gamma-amp-coef0"><a href="#1-3-4-核函数相关的参数：degree-amp-gamma-amp-coef0" class="headerlink" title="1.3.4 核函数相关的参数：degree &amp; gamma &amp; coef0"></a>1.3.4 核函数相关的参数：degree &amp; gamma &amp; coef0</h3><ul>
<li>gamma是表达式中的$\gamma$</li>
<li>degree是多项式核函数的次数$d$</li>
<li>ceof0是常数项</li>
</ul>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">degree</td>
<td align="left">整数，可不填，默认3</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">多项式核函数的次数（’poly’），如果核函数没有选择”poly”，这个参数会被忽略</td>
</tr>
<tr>
<td align="left">gamma</td>
<td align="left">浮点数，可不填，默认“auto”</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">核函数的系数，仅在参数Kernel的选项为”rbf”,”poly”和”sigmoid”的时候有效</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">输入“auto”，自动使用1/(n_features)作为gamma的取值</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">输入”scale”，则使用1/(n_features * X.std())作为gamma的取值</td>
</tr>
<tr>
<td align="left">ceof0</td>
<td align="left">浮点数，可不填，默认=0.0</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">核函数中的常数项，它只在参数kernel为’poly’和’sigmoid’的时候有效。</td>
</tr>
</tbody></table>
<h2 id="1-4-硬间隔与软间隔：重要参数C"><a href="#1-4-硬间隔与软间隔：重要参数C" class="headerlink" title="1.4 硬间隔与软间隔：重要参数C"></a>1.4 硬间隔与软间隔：重要参数C</h2><h3 id="1-4-1-SVM在软间隔数据上的推广"><a href="#1-4-1-SVM在软间隔数据上的推广" class="headerlink" title="1.4.1 SVM在软间隔数据上的推广"></a>1.4.1 SVM在软间隔数据上的推广</h3><p>我们可以通过调整我们对决策边界的定义，将硬间隔时得出的数学结论推广到软间隔的情况上，让决策边界能够忍  受一小部分训练误差。这个时候，我们的决策边界就不是单纯地寻求最大边际了，因为对于软间隔地数据来说，边  际越大被分错的样本也就会越多，因此我们需要找出一个”最大边际“与”被分错的样本数量“之间的平衡<br>|关键概念：硬间隔与软间隔|<br>|:–|<br>|当两组数据是完全线性可分，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为  是存在”硬间隔“的。当两组数据几乎是完全线性可分的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”软间隔“。|</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/C2FFA1AA25854902ACF46B5A0C5D3410?method=download&shareKey=95be7710c6d12317d6fbda16a8bd436e" alt></p>
<p>由上图可知，超平面无法让数据上的训练误差为0，因此需要放松判别函数的条件，引入松弛系数$\zeta$来帮助优化判别函数：<br>$$w<em>x_i +b&gt;=1-\zeta_i if\ y_i = 1$$<br>$$w</em>x_i +b&lt;=-1+\zeta_i if\ y_i = -1$$</p>
<p>其中$\zeta_i&gt;0$，在求解最大边际的损失函数中加上一个惩罚项，我们的拉格朗日函数，拉格朗日对偶函数也因此被松弛系数改变：<br>$$\min_{w,b,\zeta}\frac{||w||^2}{2}+C\sum_{i=1}^n\zeta_i$$<br>$$subject to y_i(w* \Phi(x_i)+b&gt;=1-\zeta_i)$$<br>$$\zeta_i&gt;=0,i=1,2,\dots,N$$</p>
<p>其中，$C$是用来控制惩罚力度的系数</p>
<p>拉格朗日函数为(其中$\mu$是第二个拉格朗日乘数)：<br>$$L(w,b,\alpha,\zeta)= \frac{1}{2}||w||^2+C\sum_{i=1}^N\zeta_i-\sum_{i=1}^N\alpha_i(y_i(w*\zeta(x_i)+b)-1+\zeta_i)-\sum_{i=1}^N\mu_i\zeta_i$$</p>
<p>需要满足的KKT条件为：<br>$$\frac{\partial L(w,b,\alpha,\zeta)}{\partial w}=\frac{\partial L(w,b,\alpha,\zeta)}{\partial b}=\frac{\partial L(w,b,\alpha,\zeta)}{\partial \zeta}=0$$</p>
<p>$$\zeta_i&gt;=0,\alpha_i&gt;=0,\mu_i&gt;=0$$<br>$$\alpha_i(y_i(w*\Phi(x_i)+b)-1+\zeta_i)=0$$<br>$$\mu_i\zeta_i=0$$</p>
<p>拉格朗日对偶函数为：<br>$$L_D = \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\Phi(x_i)\Phi(x_j)$$<br>$$subject\  to\  C &gt;=\alpha_i&gt;=0$$<br>公式中唯一出现的新变量，松弛系数惩罚力度$C$。由参数$C$来进行控制</p>
<h3 id="1-4-2-重要参数C"><a href="#1-4-2-重要参数C" class="headerlink" title="1.4.2 重要参数C"></a>1.4.2 重要参数C</h3><p>参数C用于权衡”训练样本的正确分类“与”决策函数的边际最大化“两个不可同时完成的目标，希望找出一个平衡点来让模型的效果最佳。</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">C</td>
<td align="left">浮点数，默认1，必须大于等于0，可不填</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">松弛系数的惩罚项系数。如果C值设定比较大，那SVC可能会选择边际较小的，能够更好地分类所有训练点的决策边界，不过模型的训练时间也会更长。如果C的设定值较小，那SVC会尽量最大化边界，决策功能会更简单，但代价是训练的准确度。换句话说，C在SVM中的影响就像正则化参数对逻辑回归的影响。</td>
</tr>
</tbody></table>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/15/tree/" rel="prev" title="机器学习笔记（一）决策树">
      <i class="fa fa-chevron-left"></i> 机器学习笔记（一）决策树
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/22/LinearRegression/" rel="next" title="机器学习笔记（三）线性回归">
      机器学习笔记（三）线性回归 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-支持向量机（SVM）原理"><span class="nav-text">1 支持向量机（SVM）原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-线性SVM用于分类的原理"><span class="nav-text">1.1 线性SVM用于分类的原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-线性SVM的拉格朗日对偶函数和决策函数"><span class="nav-text">1.2 线性SVM的拉格朗日对偶函数和决策函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-将损失函数从最初形态转换为拉格朗日乘数形态"><span class="nav-text">1.2.1 将损失函数从最初形态转换为拉格朗日乘数形态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-将拉格朗日函数转换为拉格朗日对偶函数"><span class="nav-text">1.2.2 将拉格朗日函数转换为拉格朗日对偶函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-3-求解拉格朗日对偶函数极值及其后续过程"><span class="nav-text">1.2.3 求解拉格朗日对偶函数极值及其后续过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-非线性SVM与核函数"><span class="nav-text">1.3 非线性SVM与核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-SVC在非线性数据上的推广"><span class="nav-text">1.3.1 SVC在非线性数据上的推广</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-重要参数kernel"><span class="nav-text">1.3.2 重要参数kernel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-核函数的优势和缺陷"><span class="nav-text">1.3.3 核函数的优势和缺陷</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-核函数相关的参数：degree-amp-gamma-amp-coef0"><span class="nav-text">1.3.4 核函数相关的参数：degree &amp; gamma &amp; coef0</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-硬间隔与软间隔：重要参数C"><span class="nav-text">1.4 硬间隔与软间隔：重要参数C</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-1-SVM在软间隔数据上的推广"><span class="nav-text">1.4.1 SVM在软间隔数据上的推广</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-2-重要参数C"><span class="nav-text">1.4.2 重要参数C</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiAnG"
      src="/images/timg.jpg">
  <p class="site-author-name" itemprop="name">LiAnG</p>
  <div class="site-description" itemprop="description">Stay hungry, stay foolish</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/LiAnGGGGGG" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;LiAnGGGGGG" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1318457129@qq.com" title="E-Mail → mailto:1318457129@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="rss → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i>rss</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script
  async
  src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"
></script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiAnG</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">232k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">3:31</span>
</div>
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("05/30/2020 13:14:21");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  
  <script src="/js/src/wobblewindow.js"></script>
  <script>
    //只在桌面版网页启用特效
    if (window.innerWidth > 768) {
      $(document).ready(function () {
        
        // 根据类名获取，而不是id
        $('.header').wobbleWindow({
          radius: 50,
          movementTop: false,
          movementLeft: false,
          movementRight: false,
          debug: false,
        });
        

        
         // 根据类名获取，而不是id
        $('.sidebar').wobbleWindow({
          radius: 50,
          movementLeft: false,
          movementTop: false,
          movementBottom: false,
          position: 'fixed',
          debug: false,
        });
        

        
         // 根据类名获取，而不是id
        $('.footer').wobbleWindow({
          radius: 50,
          movementBottom: false,
          movementLeft: false,
          movementRight: false,
          
          position: 'absolute',
          debug: false,
        });
        
      });
    }
  </script>


 
<script>
  var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });
</script>






  <script async src="/js/cursor/fireworks.js"></script>



<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'rueYojQYHpaShC9b3zwCTcgt-gzGzoHsz',
      appKey     : 'SuFkJxmzL9eehU2FBrkopQlJ',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>


</body>
</html>
