<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/L.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/L.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"liangggggg.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="2020.5.30 赛题解读赛题背景：渔船作业分类本赛题基于位置数据对海上目标进行智能识别和作业行为分析，要求选手通过分析渔船北斗设备位置数据，得出该船的生产作业行为，具体判断出是拖网作业、围网作业还是流刺网作业。初赛将提供11000条(其中7000条训练数据、2000条testA、2000条testB)渔船轨迹北斗数据。">
<meta property="og:type" content="article">
<meta property="og:title" content="学习日志：2020智慧海洋建设top5方案学习">
<meta property="og:url" content="https://liangggggg.github.io/2020/05/30/My-New-Post/index.html">
<meta property="og:site_name" content="LiAnG&#39;s Blog">
<meta property="og:description" content="2020.5.30 赛题解读赛题背景：渔船作业分类本赛题基于位置数据对海上目标进行智能识别和作业行为分析，要求选手通过分析渔船北斗设备位置数据，得出该船的生产作业行为，具体判断出是拖网作业、围网作业还是流刺网作业。初赛将提供11000条(其中7000条训练数据、2000条testA、2000条testB)渔船轨迹北斗数据。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861">
<meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861">
<meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/BAC79B4C1B3440D6BFF6A1D51AC8A0FF?method=download&shareKey=16b799d9748d761fab551bec75914030">
<meta property="article:published_time" content="2020-05-30T01:15:34.000Z">
<meta property="article:modified_time" content="2020-06-04T02:25:15.553Z">
<meta property="article:author" content="LiAnG">
<meta property="article:tag" content="算法竞赛">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861">

<link rel="canonical" href="https://liangggggg.github.io/2020/05/30/My-New-Post/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>学习日志：2020智慧海洋建设top5方案学习 | LiAnG's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="LiAnG's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">LiAnG's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Stay hungry, stay foolish</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://liangggggg.github.io/2020/05/30/My-New-Post/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/timg.jpg">
      <meta itemprop="name" content="LiAnG">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiAnG's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          学习日志：2020智慧海洋建设top5方案学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-30 09:15:34" itemprop="dateCreated datePublished" datetime="2020-05-30T09:15:34+08:00">2020-05-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-04 10:25:15" itemprop="dateModified" datetime="2020-06-04T10:25:15+08:00">2020-06-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AB%9E%E8%B5%9B/" itemprop="url" rel="index"><span itemprop="name">竞赛</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/05/30/My-New-Post/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/05/30/My-New-Post/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>40k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>36 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="2020-5-30-赛题解读"><a href="#2020-5-30-赛题解读" class="headerlink" title="2020.5.30 赛题解读"></a>2020.5.30 赛题解读</h2><h3 id="赛题背景：渔船作业分类"><a href="#赛题背景：渔船作业分类" class="headerlink" title="赛题背景：渔船作业分类"></a>赛题背景：渔船作业分类</h3><p>本赛题基于位置数据对海上目标进行智能识别和作业行为分析，要求选手通过分析渔船北斗设备位置数据，得出该船的生产作业行为，具体判断出是拖网作业、围网作业还是流刺网作业。初赛将提供11000条(其中7000条训练数据、2000条testA、2000条testB)渔船轨迹北斗数据。</p>
<a id="more"></a>

<p>复赛考虑以往渔船在海上作业时主要依赖AIS数据，北斗相比AIS数据，数据上报频率和数据质量均低于AIS数据，因此复赛拟加入AIS轨迹数据辅助北斗数据更好的做渔船类型识别，其中AIS数据与北斗数据的匹配需选手自行实现，具体细节复赛开赛时更新。同时，希望选手通过数据可视化与分析，挖掘更多海洋通信导航设备的应用价值。</p>
<h3 id="竞赛数据"><a href="#竞赛数据" class="headerlink" title="竞赛数据:"></a>竞赛数据:</h3><p>提供11000条渔船北斗数据，数据包含脱敏后的渔船ID、经纬度坐标、上报时间、速度、航向信息，由于真实场景下海上环境复杂，经常出现信号丢失，设备故障等原因导致的上报坐标错误、上报数据丢失、甚至有些设备疯狂上报等。</p>
<p>数据示例：</p>
<table>
<thead>
<tr>
<th align="left">渔船ID</th>
<th align="center">x</th>
<th align="center">y</th>
<th align="center">速度</th>
<th align="center">方向</th>
<th align="center">time</th>
<th align="right">type</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1102</td>
<td align="center">6283649.656204367</td>
<td align="center">5284013.963699763</td>
<td align="center">3</td>
<td align="center">12.1</td>
<td align="center">0921 09:00</td>
<td align="right">围网</td>
</tr>
</tbody></table>
<p>渔船ID：渔船的唯一识别，结果文件以此ID为标示</p>
<p>x: 渔船在平面坐标系的x轴坐标</p>
<p>y: 渔船在平面坐标系的y轴坐标</p>
<p>速度：渔船当前时刻航速，单位节</p>
<p>方向：渔船当前时刻航首向，单位度</p>
<p>time：数据上报时刻，单位月日 时：分</p>
<p>type：渔船label，作业类型</p>
<p>原始数据经过脱敏处理，渔船信息被隐去，坐标等信息精度和位置被转换偏移。<br>选手可通过学习围网、刺网、拖网等专业知识辅助大赛数据处理。<br>AIS数据</p>
<table>
<thead>
<tr>
<th align="left">ais_id</th>
<th align="center">lon</th>
<th align="center">lat</th>
<th align="center">速度</th>
<th align="center">航向</th>
<th align="right">time</th>
</tr>
</thead>
<tbody><tr>
<td align="left">110</td>
<td align="center">119.6705</td>
<td align="center">26.5938</td>
<td align="center">3</td>
<td align="center">12.1</td>
<td align="right">0921 09:00</td>
</tr>
</tbody></table>
<p>ais_id：AIS设备的唯一识别ID</p>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>选手提交结果与实际渔船作业类型结果进行对比，以3种类别的各自F1值取平均做为评价指标，结果越大越好，具体计算公式如下：</p>
<p>$$Score ={F1_{围网}+F1_{刺网}+F1_{拖网}  \over 3}$$</p>
<p>$$F1 ={2\ast P\ast R\over P+R}$$</p>
<p>其中P为某类别的准确率，R为某类别的召回率，评测程序f1函数为sklearn.metrics.f1_score，average=’macro’。</p>
<h2 id="模型大致思路"><a href="#模型大致思路" class="headerlink" title="模型大致思路"></a>模型大致思路</h2><ul>
<li><p>将所有数据数据切入：速度等于0和非0，白天和黑夜，四个数据集对每艘船的速度，方向，xy进行统计。</p>
</li>
<li><p>采用TFIDF对速度和XY进行抽取特征并降维</p>
</li>
<li><p>采用自然语言思路对速度，xy进行嵌入</p>
</li>
<li><p>训练模型前采用Lightgbm进行初步的特征筛选</p>
</li>
<li><p>最后用Lightgbm进行模型训练</p>
</li>
</ul>
<h2 id="具体分析"><a href="#具体分析" class="headerlink" title="具体分析"></a>具体分析</h2><p><strong>1. 按照同一个渔船id速度为0和非0两部分进行分析</strong></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861" alt></p>
<p>思路：</p>
<p>1、针对同一艘渔船，将其数据分为 速度为0和非0两个部分。分别统计该船在速度为0 和 非0情况下做可视化分析，观察经纬度xy、方向direction这些原始特征的变化情况（均值、方差、极值、峰度、偏度等统计特征）</p>
<p>2、根据1构建的特征，原始特征被构造出一系列统计特征，一种含义的特征会被分成速度为0和非0情况。根据这个特点，对这些特征进行一个比值处理。</p>
<p><strong>2. 渔船在白天和黑夜会按照同一个渔船id白天和黑夜两部分进行分析</strong></p>
<p>早6点整至晚8点整设置为白天(图标识Day)</p>
<p>晚8点整至早6点整设置为黑夜(图标识Night)</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861" alt></p>
<p>思路：<br>1、    数据按照时间划分成白天和黑夜两部分，分别统计该船在不同时间做可视化分析，观察经纬度xy、方向direction这些原始特征的变化情况（均值、方差、极值、峰度、偏度等统计特征）</p>
<p>2、    根据1构造的两组时间特征，提取关键的速度speed、经纬度xy进行白天与黑夜特征的对比。</p>
<p><strong>3. 借鉴自然语言处理（NLP）角度去处理船的轨迹特征</strong></p>
<p>速度speed、经纬度xy按照作业时间排序，可以反映出每艘船的行为规律。而每种作业方式都有其内在的一些规律, 借鉴自然语言处理(NLP)的相关算法进行特征提取。利用nlp的算法对速度、经纬这些序列的学习，尝试挖掘出每艘船的行为特点。</p>
<p>思路一：TF-IDF + NMF(如图，从左到有分别是ngram=1, ngram=2,ngram=3，经过t-SNE降维的可视化结果)</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/BAC79B4C1B3440D6BFF6A1D51AC8A0FF?method=download&shareKey=16b799d9748d761fab551bec75914030" alt></p>
<p>1、    使用不同的ngram去处理每个渔船的速度、经纬度数据，提取出每艘船的TF-IDF特征（ngram=1, 2, 3）。</p>
<p>2、    并利用非负矩阵分解(NMF)算法，对处理后的速度、经纬度进行降维生成一个主题分布向量。（此题目分成了8类）。</p>
<p>3、    对每个渔船的主题分布向量进行T-SNE降维，进行可视化。</p>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>该代码主要包括三个文件</p>
<ul>
<li>feature_selector.py 特征选择文件</li>
<li>nmf_list.py 处理轨迹文件</li>
<li>model.py 模型文件</li>
</ul>
<h3 id="1-feature-selector-py代码分析："><a href="#1-feature-selector-py代码分析：" class="headerlink" title="1. feature_selector.py代码分析："></a><strong>1. feature_selector.py代码分析：</strong></h3><pre><code># numpy and pandas for data manipulation
import pandas as pd
import numpy as np

# model used for feature importances
import lightgbm as lgb

# utility for early stopping with a validation set
from sklearn.model_selection import train_test_split

# visualizations
import matplotlib.pyplot as plt
import seaborn as sns

# memory management
import gc

# utilities
from itertools import chain
class FeatureSelector():
&quot;&quot;&quot;    
这个类用于为机器学习或数据预处理执行特征选择
实现五种不同的方法来识别要删除的特性
1、查找丢失百分比大于指定阈值的列
2、查找具有唯一值的列
3、找出相关系数大于指定相关系数的相关变量
4、从梯度增强机(gbm)中查找特性重要性为0.0的特性
5、从gbm中查找不影响指定的累积特性重要性的低重要性特性

参数
--------
data:dataframe
    一个数据集，行中有观察值，列中有特性
labels : array or series, default = None
    数组标签用于训练机器学习模型，以发现特征重要性。它们可以是二进制标签
    (如果任务是“分类”)或连续目标(如果任务是“回归”)。
    如果没有提供标签，那么基于特征重要性的方法是不可用的。

属性
--------
ops : dict
    运行的操作字典和要删除的特性
missing_stats : dataframe
    所有特征的缺失值的比例
record_missing : dataframe
    缺失值在阈值以上的特征的缺失值的比例
unique_stats: dataframe
    所有特性的唯一值的个数
record_single_unique: dataframe
    记录具有唯一值的特性
corr_matrix : dataframe
    数据中所有特征之间的所有相关性
record_collinear : dataframe
    记录相关系数高于阈值的相关变量对
feature_importances: dataframe
    从梯度增强机的所有特征的重要性
record_zero_importance: dataframe
    根据gbm记录数据中的零重要性特征
record_low_importance: dataframe
    根据gbm记录不需要达到累积重要性阈值的最低重要性特征

Notes
--------
    -所有5个操作都可以用identify_all方法运行。
    -如果使用特性重要度，则对创建新列的分类变量使用one-hot编码
&quot;&quot;&quot;
    def __init__(self, data, labels=None):            
        # 数据集和标签
        self.data = data
        self.labels = labels
        if labels is None:
            print(&apos;No labels provided. Feature importance based methods are not available.&apos;)    
        # 记录关于要删除的特性的信息    
        self.record_missing = None
        self.record_single_unique = None
        self.record_collinear = None
        self.record_zero_importance = None
        self.record_low_importance = None

        self.missing_stats = None
        self.unique_stats = None
        self.corr_matrix = None
        self.feature_importances = None
        # 用于保存删除操作的字典
        self.ops = {}

        self.one_hot_correlated = False

    def identify_missing(self, missing_threshold):

        # 找到丢失值大于&apos; missing_threshold &apos;的部分特征
        self.missing_threshold = missing_threshold
        # 计算每一列特征的缺失率
        missing_series = self.data.isnull().sum() / self.data.shape[0]
        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {&apos;index&apos;: &apos;feature&apos;, 0: &apos;missing_fraction&apos;})
        # 将特征的缺失率排序
        self.missing_stats = self.missing_stats.sort_values(&apos;missing_fraction&apos;, ascending = False)
        #找到缺失百分比大于阈值的列
        record_missing = pd.DataFrame(missing_series[missing_series &gt; missing_threshold]).reset_index().rename(columns = 
                                                                                                                   {&apos;index&apos;: &apos;feature&apos;, 
                                                                                                                    0: &apos;missing_fraction&apos;})
        to_drop = list(record_missing[&apos;feature&apos;])
        self.record_missing = record_missing
        self.ops[&apos;missing&apos;] = to_drop

        print(&apos;%d features with greater than %0.2f missing values.\n&apos; % (len(self.ops[&apos;missing&apos;]), self.missing_threshold))

    def identify_single_unique(self):
    # 查找只有一个唯一值的特征

        # 计算每个列中的惟一计数
        unique_counts = self.data.nunique()
        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {&apos;index&apos;: &apos;feature&apos;, 0: &apos;nunique&apos;})
        self.unique_stats = self.unique_stats.sort_values(&apos;nunique&apos;, ascending = True)

        # 查找只有惟一计数的列
        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {&apos;index&apos;: &apos;feature&apos;, 
                                                                                                               0: &apos;nunique&apos;})
        to_drop = list(record_single_unique[&apos;feature&apos;])
        self.record_single_unique = record_single_unique
        self.ops[&apos;single_unique&apos;] = to_drop
        print(&apos;%d features with a single unique value.\n&apos; % len(self.ops[&apos;single_unique&apos;]))

    def identify_collinear(self, correlation_threshold, one_hot=False):
        &quot;&quot;&quot;
        找寻相关系数大于“correlation_threshold”的特征并删除

        参数
        --------
        correlation_threshold : float between 0 and 1
        one_hot : boolean, default = False
        &quot;&quot;&quot;
        self.correlation_threshold = correlation_threshold
        self.one_hot_correlated = one_hot
        # 计算每一列之间的相关性
        if one_hot:

            # one_hot编码
            features = pd.get_dummies(self.data)
            self.one_hot_features = [column for column in features.columns if column not in self.base_features]

            # 向原始数据添加一个热编码数据
            self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)

            corr_matrix = pd.get_dummies(features).corr()

        else:
            corr_matrix = self.data.corr()

        self.corr_matrix = corr_matrix

        # 提取关联矩阵的上三角
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))

        # 选择相关性高于阈值的特性
         # 需要使用绝对值
        to_drop = [column for column in upper.columns if any(upper[column].abs() &gt; correlation_threshold)]

        # 保存线性相关特征
        record_collinear = pd.DataFrame(columns = [&apos;drop_feature&apos;, &apos;corr_feature&apos;, &apos;corr_value&apos;])

        # 遍历列以删除相关特性对
        for column in to_drop:

            # 找出相关特征
            corr_features = list(upper.index[upper[column].abs() &gt; correlation_threshold])

            # 找出相关系数
            corr_values = list(upper[column][upper[column].abs() &gt; correlation_threshold])
            drop_features = [column for _ in range(len(corr_features))]

            # 记录信息(现在需要一个临时df)
            temp_df = pd.DataFrame.from_dict({&apos;drop_feature&apos;: drop_features,
                                                 &apos;corr_feature&apos;: corr_features,
                                                 &apos;corr_value&apos;: corr_values})

            #添加到dataframe
            record_collinear = record_collinear.append(temp_df, ignore_index = True)

        self.record_collinear = record_collinear
        self.ops[&apos;collinear&apos;] = to_drop

        print(&apos;%d features with a correlation magnitude greater than %0.2f.\n&apos; % (len(self.ops[&apos;collinear&apos;]), self.correlation_threshold))

    def identify_zero_importance(self, task, eval_metric=None, 
                                 n_iterations=10, early_stopping = True):
        &quot;&quot;&quot;
        根据梯度增强机识别零重要性的特征。
        gbm可以使用验证集进行早期停止训练，以防止过拟合。
        在“n_iteration”上对特征重要性求平均值以减少方差。

        参数 
        --------
        eval_metric : string
            评价指标用于梯度提升机的早期停止，如果&apos; early_stopped &apos;为真，则必须提供
        task : string
            机器学习任务，是“classification”还是“regression”
        n_iterations : int, default = 10
            gbm的训练迭代次数
        early_stopping : boolean, default = True
            是否在训练时使用提前停止与验证集

        Notes
        --------
            - 在训练前热编码特征
            - gbm没有针对任何特定任务进行优化，可能需要进行一些超参数调优
            - 特性重要性，包括零重要性特性，可以在运行过程中改变
        &quot;&quot;&quot;
        if early_stopping and eval_metric is None:
            raise ValueError(&quot;&quot;&quot;eval metric must be provided with early stopping. Examples include &quot;auc&quot; for classification or
                             &quot;l2&quot; for regression.&quot;&quot;&quot;)
        if self.labels is None:
             raise ValueError(&quot;No training labels provided.&quot;)
        # 热编码特征
        features = pd.get_dummies(self.data)
        self.one_hot_features = [column for column in features.columns if column not in self.base_features]

        # 将热编码数据添加到原始数据中
        self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)

        # 提取特征名字
        feature_names = list(features.columns)

        # 转换成np array
        features = np.array(features)
        labels = np.array(self.labels).reshape((-1, ))

        # 创建特征重要性数组
        feature_importance_values = np.zeros(len(feature_names))

        print(&apos;Training Gradient Boosting Model\n&apos;)

        # 在每折中迭代
        for _ in range(n_iterations):
            if task == &apos;classification&apos;:
                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)

            elif task == &apos;regression&apos;:
                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)
            else:
                raise ValueError(&apos;Task must be either &quot;classification&quot; or &quot;regression&quot;&apos;)

            # 如果使用早期停止训练需要一个验证集
            if early_stopping:
                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15, stratify=labels)
                # 使用早停机制训练模型
                model.fit(train_features, train_labels, eval_metric = eval_metric,
                              eval_set = [(valid_features, valid_labels)],
                              early_stopping_rounds = 100, verbose = -1)
                # 清空内存
                gc.enable()
                del train_features, train_labels, valid_features, valid_labels
                gc.collect()
            else:
                model.fit(features, labels)

            # 记录特征重要性
            feature_importance_values += model.feature_importances_ / n_iterations

        feature_importances = pd.DataFrame({&apos;feature&apos;: feature_names, &apos;importance&apos;: feature_importance_values})

        # 根据特征重要性排序
        feature_importances = feature_importances.sort_values(&apos;importance&apos;, ascending = False).reset_index(drop = True)

        # 归一化特征重要性
        feature_importances[&apos;normalized_importance&apos;] = feature_importances[&apos;importance&apos;] / feature_importances[&apos;importance&apos;].sum()
        feature_importances[&apos;cumulative_importance&apos;] = np.cumsum(feature_importances[&apos;normalized_importance&apos;])

        # 提取特征重要性为0的特征
        record_zero_importance = feature_importances[feature_importances[&apos;importance&apos;] == 0.0]

        to_drop = list(record_zero_importance[&apos;feature&apos;])

        self.feature_importances = feature_importances
        self.record_zero_importance = record_zero_importance
        self.ops[&apos;zero_importance&apos;] = to_drop

        print(&apos;\n%d features with zero importance after one-hot encoding.\n&apos; % len(self.ops[&apos;zero_importance&apos;]))

    def identify_low_importance(self, cumulative_importance):
        &quot;&quot;&quot;
        找到特征重要性低于“cumulative_importance”的特征

        参数
        --------
        cumulative_importance : float between 0 and 1
            重要性分数
        &quot;&quot;&quot;
        self.cumulative_importance = cumulative_importance

        # 特征重要性需要在运行之前计算
        if self.feature_importances is None:
            raise NotImplementedError(&quot;&quot;&quot;Feature importances have not yet been determined. 
                                     Call the `identify_zero_importance` method first.&quot;&quot;&quot;)

        # 将特征重要性排序
        self.feature_importances = self.feature_importances.sort_values(&apos;cumulative_importance&apos;)

        # 识别出特征重要性低于设定阈值的特征
        record_low_importance = self.feature_importances[self.feature_importances[&apos;cumulative_importance&apos;] &gt; cumulative_importance]

        to_drop = list(record_low_importance[&apos;feature&apos;])

        self.record_low_importance = record_low_importance
        self.ops[&apos;low_importance&apos;] = to_drop

        print(&apos;%d features required for cumulative importance of %0.2f after one hot encoding.&apos; % (len(self.feature_importances) -
                                                                            len(self.record_low_importance), self.cumulative_importance))
        print(&apos;%d features do not contribute to cumulative importance of %0.2f.\n&apos; % (len(self.ops[&apos;low_importance&apos;]),
                                                                                               self.cumulative_importance))

    def identify_all(self, selection_params):
        &quot;&quot;&quot;
        使用所有五种方法来删除不需要的特征

        参数
        --------
        selection_params : dict
            在五种特征选择方法中使用的参数。
            参数必须包含键[&apos;missing_threshold&apos;， &apos;correlation_threshold&apos;， &apos;eval_metric&apos;， &apos;task&apos;， &apos; collecative_importance &apos;]

        # 检查所必要的参数
        for param in [&apos;missing_threshold&apos;, &apos;correlation_threshold&apos;, &apos;eval_metric&apos;, &apos;task&apos;, &apos;cumulative_importance&apos;]:
            if param not in selection_params.keys():
                raise ValueError(&apos;%s is a required parameter for this method.&apos; % param)

        # 实现五种方法
        self.identify_missing(selection_params[&apos;missing_threshold&apos;])
        self.identify_single_unique()
        self.identify_collinear(selection_params[&apos;correlation_threshold&apos;])
        self.identify_zero_importance(task = selection_params[&apos;task&apos;], eval_metric = selection_params[&apos;eval_metric&apos;])
        self.identify_low_importance(selection_params[&apos;cumulative_importance&apos;])

        # 查找要删除的特性的数量
        self.all_identified = set(list(chain(*list(self.ops.values()))))
        self.n_identified = len(self.all_identified)

        print(&apos;%d total features out of %d identified for removal after one-hot encoding.\n&apos; % (self.n_identified, 
                                                                                                  self.data_all.shape[1]))

     def check_removal(self, keep_one_hot=True):
         &quot;&quot;&quot;
         在删除前检查已识别的特征。返回一个列表的独特的功能识别。
         &quot;&quot;&quot;
        self.all_identified = set(list(chain(*list(self.ops.values()))))
        print(&apos;Total of %d features identified for removal&apos; % len(self.all_identified))

        if not keep_one_hot:
            if self.one_hot_features is None:
                print(&apos;Data has not been one-hot encoded&apos;)
            else:
                one_hot_to_remove = [x for x in self.one_hot_features if x not in self.all_identified]
                print(&apos;%d additional one-hot features can be removed&apos; % len(one_hot_to_remove))

        return list(self.all_identified)

    def remove(self, methods, keep_one_hot = True):
        &quot;&quot;&quot;
        根据指定的方法从数据中删除特征。

        参数
        --------
            methods : &apos;all&apos; or list of methods
                可以是[&apos;missing&apos;， &apos;single_unique&apos;， &apos;collinear&apos;， &apos;zero_importance&apos;， &apos;low_importance&apos;]
            keep_one_hot : boolean, default = True
                是否热编码

        返回
        --------
            data : dataframe
                删除了特征的数据

        Notes 
        --------
            -如果使用特性重要度，则一个热编码列将被添加到数据中(然后可能被删除)
            -在转换数据之前，检查将被删除的功能!
        features_to_drop = []

        if methods == &apos;all&apos;:

            # 热编码数据
            data = self.data_all

            print(&apos;{} methods have been run\n&apos;.format(list(self.ops.keys())))

            # 找到需要删除的特征
            features_to_drop = set(list(chain(*list(self.ops.values()))))

        else:
            # Need to use one-hot encoded data as well
            if &apos;zero_importance&apos; in methods or &apos;low_importance&apos; in methods or self.one_hot_correlated:
                data = self.data_all

            else:
                data = self.data

            # 遍历指定方法
            for method in methods:
                # 确定方法已经在运行
                if method not in self.ops.keys():
                    raise NotImplementedError(&apos;%s method has not been run&apos; % method)

                # 添加要删除的方法
                else:
                    features_to_drop.append(self.ops[method])

            # 找到要删除的特征
            features_to_drop = set(list(chain(*features_to_drop)))

        features_to_drop = list(features_to_drop)

        if not keep_one_hot:

            if self.one_hot_features is None:
                print(&apos;Data has not been one-hot encoded&apos;)
            else:

                features_to_drop = list(set(features_to_drop) | set(self.one_hot_features))

        # 在原数据中删除特征
        data = data.drop(columns = features_to_drop)
        self.removed_features = features_to_drop

        if not keep_one_hot:
            print(&apos;Removed %d features including one-hot features.&apos; % len(features_to_drop))
         else:
            print(&apos;Removed %d features.&apos; % len(features_to_drop))

        return data

    # 各种绘图函数
    def plot_missing(self):
        &quot;&quot;&quot;Histogram of missing fraction in each feature&quot;&quot;&quot;
        if self.record_missing is None:
            raise NotImplementedError(&quot;Missing values have not been calculated. Run `identify_missing`&quot;)

        self.reset_plot()

        # Histogram of missing values
        plt.style.use(&apos;seaborn-white&apos;)
        plt.figure(figsize = (7, 5))
        plt.hist(self.missing_stats[&apos;missing_fraction&apos;], bins = np.linspace(0, 1, 11), edgecolor = &apos;k&apos;, color = &apos;red&apos;, linewidth = 1.5)
        plt.xticks(np.linspace(0, 1, 11));
        plt.xlabel(&apos;Missing Fraction&apos;, size = 14); plt.ylabel(&apos;Count of Features&apos;, size = 14); 
        plt.title(&quot;Fraction of Missing Values Histogram&quot;, size = 16);


    def plot_unique(self):
        &quot;&quot;&quot;Histogram of number of unique values in each feature&quot;&quot;&quot;
        if self.record_single_unique is None:
            raise NotImplementedError(&apos;Unique values have not been calculated. Run `identify_single_unique`&apos;)

        self.reset_plot()

        # Histogram of number of unique values
        self.unique_stats.plot.hist(edgecolor = &apos;k&apos;, figsize = (7, 5))
        plt.ylabel(&apos;Frequency&apos;, size = 14); plt.xlabel(&apos;Unique Values&apos;, size = 14); 
        plt.title(&apos;Number of Unique Values Histogram&apos;, size = 16);


    def plot_collinear(self, plot_all = False):
        &quot;&quot;&quot;
        Heatmap of the correlation values. If plot_all = True plots all the correlations otherwise
        plots only those features that have a correlation above the threshold

        Notes
        --------
            - Not all of the plotted correlations are above the threshold because this plots
            all the variables that have been idenfitied as having even one correlation above the threshold
            - The features on the x-axis are those that will be removed. The features on the y-axis
            are the correlated features with those on the x-axis

        Code adapted from https://seaborn.pydata.org/examples/many_pairwise_correlations.html
        &quot;&quot;&quot;

        if self.record_collinear is None:
            raise NotImplementedError(&apos;Collinear features have not been idenfitied. Run `identify_collinear`.&apos;)

        if plot_all:
            corr_matrix_plot = self.corr_matrix
            title = &apos;All Correlations&apos;

        else:
            # Identify the correlations that were above the threshold
            # columns (x-axis) are features to drop and rows (y_axis) are correlated pairs
            corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear[&apos;corr_feature&apos;])), 
                                                    list(set(self.record_collinear[&apos;drop_feature&apos;]))]

            title = &quot;Correlations Above Threshold&quot;


        f, ax = plt.subplots(figsize=(10, 8))

        # Diverging colormap
        cmap = sns.diverging_palette(220, 10, as_cmap=True)

        # Draw the heatmap with a color bar
        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,
                    linewidths=.25, cbar_kws={&quot;shrink&quot;: 0.6})

        # Set the ylabels 
        ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])
        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));

        # Set the xlabels 
        ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])
        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));
        plt.title(title, size = 14)

    def plot_feature_importances(self, plot_n = 15, threshold = None):
        &quot;&quot;&quot;
        Plots `plot_n` most important features and the cumulative importance of features.
        If `threshold` is provided, prints the number of features needed to reach `threshold` cumulative importance.

        Parameters
        --------

        plot_n : int, default = 15
            Number of most important features to plot. Defaults to 15 or the maximum number of features whichever is smaller

        threshold : float, between 0 and 1 default = None
            Threshold for printing information about cumulative importances

        &quot;&quot;&quot;

        if self.record_zero_importance is None:
            raise NotImplementedError(&apos;Feature importances have not been determined. Run `idenfity_zero_importance`&apos;)

        # Need to adjust number of features if greater than the features in the data
        if plot_n &gt; self.feature_importances.shape[0]:
            plot_n = self.feature_importances.shape[0] - 1

        self.reset_plot()

        # Make a horizontal bar chart of feature importances
        plt.figure(figsize = (10, 6))
        ax = plt.subplot()

        # Need to reverse the index to plot most important on top
        # There might be a more efficient method to accomplish this
        ax.barh(list(reversed(list(self.feature_importances.index[:plot_n]))), 
                self.feature_importances[&apos;normalized_importance&apos;][:plot_n], 
                align = &apos;center&apos;, edgecolor = &apos;k&apos;)

        # Set the yticks and labels
        ax.set_yticks(list(reversed(list(self.feature_importances.index[:plot_n]))))
        ax.set_yticklabels(self.feature_importances[&apos;feature&apos;][:plot_n], size = 12)

        # Plot labeling
        plt.xlabel(&apos;Normalized Importance&apos;, size = 16); plt.title(&apos;Feature Importances&apos;, size = 18)
        plt.show()

        # Cumulative importance plot
        plt.figure(figsize = (6, 4))
        plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances[&apos;cumulative_importance&apos;], &apos;r-&apos;)
        plt.xlabel(&apos;Number of Features&apos;, size = 14); plt.ylabel(&apos;Cumulative Importance&apos;, size = 14); 
        plt.title(&apos;Cumulative Feature Importance&apos;, size = 16);

        if threshold:

            # Index of minimum number of features needed for cumulative importance threshold
            # np.where returns the index so need to add 1 to have correct number
            importance_index = np.min(np.where(self.feature_importances[&apos;cumulative_importance&apos;] &gt; threshold))
            plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles=&apos;--&apos;, colors = &apos;blue&apos;)
            plt.show();

            print(&apos;%d features required for %0.2f of cumulative importance&apos; % (importance_index + 1, threshold))

    def reset_plot(self):
        plt.rcParams = plt.rcParamsDefault</code></pre><h3 id="2-nmf-list-py-代码分析："><a href="#2-nmf-list-py-代码分析：" class="headerlink" title="2. nmf_list.py 代码分析："></a><strong>2. nmf_list.py 代码分析：</strong></h3><pre><code>import pickle
import numpy as np
import pandas as pd
from collections import Counter
from sklearn.decomposition import NMF
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import tqdm
from gensim.models import FastText, Word2Vec
import multiprocessing

class nmf_list(object):
    def __init__(self,data,by_name,to_list,nmf_n,top_n):
        self.data = data
        self.by_name = by_name
        self.to_list = to_list
        self.nmf_n = nmf_n
        self.top_n = top_n

    def run(self,tf_n):
        df_all = self.data.groupby(self.by_name)[self.to_list].apply(lambda x :&apos;|&apos;.join(x)).reset_index()
        self.data =df_all.copy()

        print(&apos;bulid word_fre&apos;)
    # 词频的构建
    def word_fre(x):
        word_dict = []
        x = x.split(&apos;|&apos;)
        docs = []
        for doc in x:
            doc = doc.split()
            docs.append(doc)
            word_dict.extend(doc)
        word_dict = Counter(word_dict)
        new_word_dict = {}
        for key,value in word_dict.items():
            new_word_dict[key] = [value,0]
        del word_dict  
        del x
        for doc in docs:
            doc = Counter(doc)
            for word in doc.keys():
                new_word_dict[word][1] += 1
        return new_word_dict 
    self.data[&apos;word_fre&apos;] = self.data[self.to_list].apply(word_fre)

    print(&apos;bulid top_&apos; + str(self.top_n))
    # 设定100个高频词
    def top_100(word_dict):
        return sorted(word_dict.items(),key = lambda x:(x[1][1],x[1][0]),reverse = True)[:self.top_n]
    self.data[&apos;top_&apos;+str(self.top_n)] = self.data[&apos;word_fre&apos;].apply(top_100)
    def top_100_word(word_list):
        words = []
        for i in word_list:
            i = list(i)
            words.append(i[0])
        return words 
    self.data[&apos;top_&apos;+str(self.top_n)+&apos;_word&apos;] = self.data[&apos;top_&apos; + str(self.top_n)].apply(top_100_word)
    # print(&apos;top_&apos;+str(self.top_n)+&apos;_word的shape&apos;)
    print(self.data.shape)

    word_list = []
    for i in self.data[&apos;top_&apos;+str(self.top_n)+&apos;_word&apos;].values:
        word_list.extend(i)
    word_list = Counter(word_list)
    word_list = sorted(word_list.items(),key = lambda x:x[1],reverse = True)
    user_fre = []
    for i in word_list:
        i = list(i)
        user_fre.append(i[1]/self.data[self.by_name].nunique())
    stop_words = []
    for i,j in zip(word_list,user_fre):
        if j&gt;0.5:
            i = list(i)
            stop_words.append(i[0])

    print(&apos;start title_feature&apos;)
    # 讲融合后的taglist当作一句话进行文本处理
    self.data[&apos;title_feature&apos;] = self.data[self.to_list].apply(lambda x: x.split(&apos;|&apos;))
    self.data[&apos;title_feature&apos;] = self.data[&apos;title_feature&apos;].apply(lambda line: [w for w in line if w not in stop_words])
    self.data[&apos;title_feature&apos;] = self.data[&apos;title_feature&apos;].apply(lambda x: &apos; &apos;.join(x))

    print(&apos;start NMF&apos;)
    # 使用tfidf对元素进行处理
    tfidf_vectorizer = TfidfVectorizer(ngram_range=(tf_n,tf_n))
    tfidf = tfidf_vectorizer.fit_transform(self.data[&apos;title_feature&apos;].values)
    #使用nmf算法，提取文本的主题分布
    text_nmf = NMF(n_components=self.nmf_n).fit_transform(tfidf)


    # 整理并输出文件
    name = [str(tf_n) + self.to_list + &apos;_&apos; +str(x) for x in range(1,self.nmf_n+1)]
    tag_list = pd.DataFrame(text_nmf)
    print(tag_list.shape)
    tag_list.columns = name
    tag_list[self.by_name] = self.data[self.by_name]
    column_name = [self.by_name] + name
    tag_list = tag_list[column_name]
    return tag_list</code></pre><h3 id="3-model-py-代码分析："><a href="#3-model-py-代码分析：" class="headerlink" title="3. model.py  代码分析："></a><strong>3. model.py  代码分析：</strong></h3><pre><code>import gc
import pandas as pd
import numpy as np
import os
import time
import lightgbm as lgb
from copy import deepcopy
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from sklearn import metrics
from sklearn.metrics import precision_recall_fscore_support
import warnings
from glob import glob
from scipy.sparse import csr_matrix

start_t = time.time()
print(&apos;ww_900_start&apos;)
pd.set_option(&apos;display.max_columns&apos;, 100)
warnings.filterwarnings(&apos;ignore&apos;)

# 添加需要提取的特征
def group_feature(df, key, target, aggs,flag):   
    agg_dict = {}
    for ag in aggs:
        agg_dict[&apos;{}_{}_{}&apos;.format(target,ag,flag)] = ag
    print(agg_dict)
    t = df.groupby(key)[target].agg(agg_dict).reset_index()
    return t

# 计算两个经纬度之间的haversine距离
def haversine_dist(lat1,lng1,lat2,lng2):
    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
    radius = 6371  # Earth&apos;s radius taken from google
    lat = lat2 - lat1
    lng = lng2 - lng1
    d = np.sin(lat/2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng/2) ** 2
    h = 2 * radius * np.arcsin(np.sqrt(d))
    return h

# 提取特征
def extract_feature(df, train, flag):

    if (flag == &apos;on_night&apos;) or (flag == &apos;on_day&apos;): 
        t = group_feature(df, &apos;ship&apos;,&apos;speed&apos;,[&apos;max&apos;,&apos;mean&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
        train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)

    if flag == &quot;0&quot;:
        t = group_feature(df, &apos;ship&apos;,&apos;direction&apos;,[&apos;max&apos;,&apos;median&apos;,&apos;mean&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
        train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;) 

    elif flag == &quot;1&quot;:
        t = group_feature(df, &apos;ship&apos;,&apos;speed&apos;,[&apos;max&apos;,&apos;mean&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
        train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)
        t = group_feature(df, &apos;ship&apos;,&apos;direction&apos;,[&apos;max&apos;,&apos;median&apos;,&apos;mean&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
        train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;) 
        hour_nunique = df.groupby(&apos;ship&apos;)[&apos;speed&apos;].nunique().to_dict()
        train[&apos;speed_nunique_{}&apos;.format(flag)] = train[&apos;ship&apos;].map(hour_nunique)   
        hour_nunique = df.groupby(&apos;ship&apos;)[&apos;direction&apos;].nunique().to_dict()
        train[&apos;direction_nunique_{}&apos;.format(flag)] = train[&apos;ship&apos;].map(hour_nunique)  

    t = group_feature(df, &apos;ship&apos;,&apos;x&apos;,[&apos;max&apos;,&apos;min&apos;,&apos;mean&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
    train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)
    t = group_feature(df, &apos;ship&apos;,&apos;y&apos;,[&apos;max&apos;,&apos;min&apos;,&apos;mean&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
    train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)
    t = group_feature(df, &apos;ship&apos;,&apos;base_dis_diff&apos;,[&apos;max&apos;,&apos;min&apos;,&apos;mean&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
    train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)

    train[&apos;x_max_x_min_{}&apos;.format(flag)] = train[&apos;x_max_{}&apos;.format(flag)] - train[&apos;x_min_{}&apos;.format(flag)]
    train[&apos;y_max_y_min_{}&apos;.format(flag)] = train[&apos;y_max_{}&apos;.format(flag)] - train[&apos;y_min_{}&apos;.format(flag)]
    train[&apos;y_max_x_min_{}&apos;.format(flag)] = train[&apos;y_max_{}&apos;.format(flag)] - train[&apos;x_min_{}&apos;.format(flag)]
    train[&apos;x_max_y_min_{}&apos;.format(flag)] = train[&apos;x_max_{}&apos;.format(flag)] - train[&apos;y_min_{}&apos;.format(flag)]
    train[&apos;slope_{}&apos;.format(flag)] = train[&apos;y_max_y_min_{}&apos;.format(flag)] / np.where(train[&apos;x_max_x_min_{}&apos;.format(flag)]==0, 0.001, train[&apos;x_max_x_min_{}&apos;.format(flag)])
    train[&apos;area_{}&apos;.format(flag)] = train[&apos;x_max_x_min_{}&apos;.format(flag)] * train[&apos;y_max_y_min_{}&apos;.format(flag)]

    mode_hour = df.groupby(&apos;ship&apos;)[&apos;hour&apos;].agg(lambda x:x.value_counts().index[0]).to_dict()
    train[&apos;mode_hour_{}&apos;.format(flag)] = train[&apos;ship&apos;].map(mode_hour)
    train[&apos;slope_median_{}&apos;.format(flag)] = train[&apos;y_median_{}&apos;.format(flag)] / np.where(train[&apos;x_median_{}&apos;.format(flag)]==0, 0.001, train[&apos;x_median_{}&apos;.format(flag)])

    return train

# 提取数据
def get_data(files, is_sort=True, sort_column=&quot;time&quot;):
    datas = [pd.read_csv(f) for f in files]
    if is_sort:
        dfs = [df.sort_values(by=sort_column, ascending=True, na_position=&apos;last&apos;) for df in datas]
    df = pd.concat(datas, axis=0, ignore_index=True)
    return df

# 处理提取文件数据
def extract_dt(df):
    df[&apos;time&apos;] = pd.to_datetime(df[&apos;time&apos;], format=&apos;%m%d %H:%M:%S&apos;)
    df[&apos;date&apos;] = df[&apos;time&apos;].dt.date
    df[&apos;hour&apos;] = df[&apos;time&apos;].dt.hour

    df[&apos;x_dis_diff&apos;] = (df[&apos;x&apos;] - 6165599).abs()
    df[&apos;y_dis_diff&apos;] = (df[&apos;y&apos;] - 5202660).abs()
    df[&apos;base_dis_diff&apos;] = ((df[&apos;x_dis_diff&apos;]**2)+(df[&apos;y_dis_diff&apos;]**2))**0.5    
    del df[&apos;x_dis_diff&apos;],df[&apos;y_dis_diff&apos;]    

    df[&quot;x&quot;] = df[&quot;x&quot;] / 1e6
    df[&quot;y&quot;] = df[&quot;y&quot;] / 1e6    
    df[&apos;day_nig&apos;] = 0
    df.loc[(df[&apos;hour&apos;] &gt; 5) &amp; (df[&apos;hour&apos;] &lt; 20),&apos;day_nig&apos;] = 1
    return df

train_files = glob(&quot;tcdata/hy_round2_train_20200225/*.csv&quot;)
test_files = glob(&quot;tcdata/hy_round2_testB_20200312/*.csv&quot;)
train_files = sorted(train_files)
test_files = sorted(test_files)

train = get_data(train_files)
train.columns = [&apos;ship&apos;,&apos;x&apos;,&apos;y&apos;,&apos;speed&apos;,&apos;direction&apos;,&apos;time&apos;,&apos;type&apos;]
test = get_data(test_files)
test.columns = [&apos;ship&apos;,&apos;x&apos;,&apos;y&apos;,&apos;speed&apos;,&apos;direction&apos;,&apos;time&apos;]

train = extract_dt(train)
test = extract_dt(test)
train_label = train.drop_duplicates([&apos;ship&apos;],keep = &apos;first&apos;)
test_label = test.drop_duplicates([&apos;ship&apos;],keep = &apos;first&apos;)
train_label[&apos;type&apos;] = train_label[&apos;type&apos;].map({&apos;围网&apos;:0,&apos;刺网&apos;:1,&apos;拖网&apos;:2})

num = train_label.shape[0]
data_label = train_label.append(test_label)
data =train.append(test)
# 将数据分成speed为0和非0、白天和晚上
data_1 = data[data[&apos;speed&apos;]==0]
data_2 = data[data[&apos;speed&apos;]!=0]
data_label = extract_feature(data_1, data_label,&quot;0&quot;)
data_label = extract_feature(data_2, data_label,&quot;1&quot;)

data_1 = data[data[&apos;day_nig&apos;] == 0]
data_2 = data[data[&apos;day_nig&apos;] == 1]
data_label = extract_feature(data_1, data_label,&quot;on_night&quot;)
data_label = extract_feature(data_2, data_label,&quot;on_day&quot;)

# 读取NMF降维后的特征
if os.path.isfile(&apos;nmf_testb.csv&apos;):
    nmf_fea = pd.read_csv(&apos;nmf_testb.csv&apos;)
    data_label = data_label.merge(nmf_fea,on=&apos;ship&apos;,how = &apos;left&apos;)
    del nmf_fea
else:
    for j in range(1,4):
        print(&apos;********* {} *******&apos;.format(j))
        for i in [&apos;speed&apos;,&apos;x&apos;,&apos;y&apos;]:
            data[i + &apos;_str&apos;] = data[i].astype(str)
            from nmf_list import nmf_list
            nmf = nmf_list(data,&apos;ship&apos;,i + &apos;_str&apos;,8,2)
            nmf_a = nmf.run(j)
            data_label = data_label.merge(nmf_a,on = &apos;ship&apos;,how = &apos;left&apos;)


first = &quot;0&quot;
second = &quot;1&quot;
data_label[&apos;direction_median_ratio&apos;] = data_label[&apos;direction_median_{}&apos;.format(second)] / data_label[&apos;direction_median_{}&apos;.format(first)]
data_label[&apos;slope_ratio&apos;] = data_label[&apos;slope_{}&apos;.format(second)] / data_label[&apos;slope_{}&apos;.format(first)] 
data_label[&apos;slope_mean_ratio&apos;] = data_label[&apos;slope_median_{}&apos;.format(second)] / data_label[&apos;slope_median_{}&apos;.format(first)]

first = &quot;on_night&quot;
second = &quot;on_day&quot;
data_label[&apos;speed_median_ratio&apos;] = data_label[&apos;speed_median_{}&apos;.format(second)] / data_label[&apos;speed_median_{}&apos;.format(first)] 
data_label[&apos;speed_std_ratio&apos;] = data_label[&apos;speed_std_{}&apos;.format(second)] / data_label[&apos;speed_std_{}&apos;.format(first)] 

# 计算特征
flag = &apos;all&apos;
for cc in [&apos;direction&apos;,&apos;speed&apos;]:
    t = group_feature(data_label,cc, &apos;ship&apos;,[&apos;count&apos;],flag +cc+ &apos;x&apos;)
    data_label = pd.merge(data_label, t, on=cc, how=&apos;left&apos;)  

for i in [&quot;0&quot;,&quot;1&quot;]:
    if i == &quot;1&quot;:
        for j in [
#                 &apos;slope_speed_cat_nunique_{}&apos;.format(i),
#                   &apos;slope_mean_speed_cat_nunique_{}&apos;.format(i),
                  &apos;speed_nunique_{}&apos;.format(i),
                  &apos;direction_nunique_{}&apos;.format(i)
                 ]:

            t = group_feature(data_label,j, &apos;ship&apos;,[&apos;count&apos;],j+&quot;_count&quot;)
            data_label = pd.merge(data_label, t, on=j, how=&apos;left&apos;) 
    for j in [
           &apos;slope_median_{}&apos;.format(i),
#               &apos;x_max_x_min_{}&apos;.format(i),
#               &apos;y_max_y_min_{}&apos;.format(i)
             ]:
#         t = group_feature(data_label,j, &apos;ship&apos;,[&apos;count&apos;],j+&quot;_count&quot;)
#         data_label = pd.merge(data_label, t, on=j, how=&apos;left&apos;) 
        t = group_feature(data_label,j, &apos;speed&apos;,[&apos;min&apos;,&apos;max&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],j+&quot;_tongji&quot;)
        data_label = pd.merge(data_label, t, on=j, how=&apos;left&apos;)
        # t = group_feature(data_label,j, &apos;direction&apos;,[&apos;min&apos;,&apos;max&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],j+&quot;_tongji&quot;)
        # data_label = pd.merge(data_label, t, on=j, how=&apos;left&apos;)

def cut_bins(raw_data, col_name=None, q=49):
    features, bins = pd.qcut(raw_data[col_name], q=q, retbins=True, duplicates=&quot;drop&quot;)
    labels = list(range(len(bins) - 1))
    features, bins = pd.qcut(raw_data[col_name], labels=labels, q=q, retbins=True, duplicates=&quot;drop&quot;)
    return features, bins, labels


MAX_CATE = 199
data[&quot;x_cate&quot;], x_bins, x_labels = cut_bins(data, col_name=&quot;x&quot;, q=MAX_CATE)
data[&quot;y_cate&quot;], y_bins, y_labels = cut_bins(data, col_name=&quot;y&quot;, q=MAX_CATE)
# data[&quot;x_sub_y_cate&quot;], x_sub_y_bins, x_sub_y_labels = cut_bins(data, col_name=&quot;x_sub_y&quot;, q=MAX_CATE)
data[&quot;distance_cate&quot;], dist_bins, dist_labels = cut_bins(data, col_name=&quot;base_dis_diff&quot;, q=MAX_CATE)

data[&quot;speed_cate&quot;], speed_bins, speed_labels = cut_bins(data, col_name=&quot;speed&quot;, q=MAX_CATE)

MAX_CATE = 120
data[&quot;direct_cate&quot;], speed_bins, speed_labels = cut_bins(data, col_name=&quot;direction&quot;, q=MAX_CATE)

if os.path.isfile(&apos;emb_testb.csv&apos;):
    w2v_fea = pd.read_csv(&apos;emb_testb.csv&apos;)
    data_label = data_label.merge(w2v_fea, on=&apos;ship&apos;, how=&apos;left&apos;)
    del w2v_fea
else:
    from gensim.models import Word2Vec
    import gc
    def emb(df, f1, f2):
        emb_size = 23
        print(&apos;====================================== {} {} ======================================&apos;.format(f1, f2))
        tmp = df.groupby(f1, as_index=False)[f2].agg({&apos;{}_{}_list&apos;.format(f1, f2): list})
        sentences = tmp[&apos;{}_{}_list&apos;.format(f1, f2)].values.tolist()
        del tmp[&apos;{}_{}_list&apos;.format(f1, f2)]
        for i in range(len(sentences)):
            sentences[i] = [str(x) for x in sentences[i]]
        model = Word2Vec(sentences, size=emb_size, window=5, min_count=3, sg=0, hs=1, seed=2222)
        emb_matrix = []
        for seq in sentences:
            vec = []
            for w in seq:
                if w in model:
                    vec.append(model[w])
            if len(vec) &gt; 0:
                emb_matrix.append(np.mean(vec, axis=0))
            else:
                emb_matrix.append([0] * emb_size)
        emb_matrix = np.array(emb_matrix)
        for i in range(emb_size):
            tmp[&apos;{}_{}_emb_{}&apos;.format(f1, f2, i)] = emb_matrix[:, i]
        del model, emb_matrix, sentences
        return tmp


    emb_cols = [
        [&apos;ship&apos;, &apos;x_cate&apos;],
        [&apos;ship&apos;, &apos;y_cate&apos;],
        [&apos;ship&apos;, &apos;speed_cate&apos;],
        [&apos;ship&apos;, &apos;distance_cate&apos;],
        # [&apos;ship&apos;, &apos;direct_cate&apos;],
    ]
    for f1, f2 in emb_cols:
        data_label = data_label.merge(emb(data, f1, f2), on=f1, how=&apos;left&apos;)

    gc.collect()

    # emb_list = [&apos;ship&apos;]
    # for i in data_label.columns:
    #     if &apos;_emb_&apos; in i:
    #         emb_list.append(i)

    # data_label[emb_list].to_csv(&apos;emb_testb.csv&apos;,index=False)


print(&apos;feature done&apos;)

train_label = data_label[:num]
test_label = data_label[num:]
features = [x for x in train_label.columns if x not in [&apos;ship&apos;,&apos;type&apos;,&apos;time&apos;,&apos;x&apos;,&apos;y&apos;,&apos;diff_time&apos;,&apos;date&apos;,&apos;day_nig&apos;,&apos;direction&apos;,&apos;speed&apos;,&apos;hour&apos;,
                                                       &apos;speed_many&apos;,&apos;dire_diff&apos;,&apos;direction_str&apos;,&apos;speed_str&apos;,&apos;dis&apos;,&apos;x_speed&apos;,&apos;y_speed&apos;] ]
target = &apos;type&apos;

# 特征选择
from feature_selector import FeatureSelector
fs = FeatureSelector(data = train_label[features], labels = train_label[target])
fs.identify_zero_importance(task = &apos;classification&apos;, eval_metric = &apos;multiclass&apos;,
                            n_iterations = 10, early_stopping = True)
fs.identify_low_importance(cumulative_importance = 0.97)
low_importance_features = fs.ops[&apos;low_importance&apos;]
print(&apos;====low_importance_features=====&apos;)
print(low_importance_features)
for i in low_importance_features:
features.remove(i)

print(&apos;feature number&apos;,len(features))
gc.collect()

# 评价指标
def macro_f1(y_hat, data):
    y_true = data.get_label()
    y_hat = y_hat.reshape(-1, y_true.shape[0])
    y_hat = np.argmax(y_hat, axis=0)
    f1_multi = precision_recall_fscore_support(y_true, y_hat, labels=[0, 1, 2])[2]
    f1_macro =  f1_score(y_true, y_hat, average =&quot;macro&quot;)
    assert np.mean(f1_multi) ==  f1_macro
    return &apos;f1&apos;, f1_macro, True


def f1_single(y_hat, data, index=0):
    y_true = data.get_label()
    y_hat = y_hat.reshape(-1, y_true.shape[0])
    y_hat = np.argmax(y_hat, axis=0)
    f1_multi = precision_recall_fscore_support(y_true, y_hat, labels=[0, 1, 2])[2]
    f1_s = round(f1_multi[index], 4)
    return &apos;f1_{}&apos;.format(index), f1_s, True

# 构造模型
train_X = train_label[features]
test_X = test_label[features]
print(train_X.shape, test_X.shape)
train_y = train_label[target]


params = {
        &apos;task&apos;:&apos;train&apos;, 
        &apos;num_leaves&apos;: 63,
        &apos;objective&apos;: &apos;multiclass&apos;,
        &apos;num_class&apos;: 3,
        &apos;metric&apos;: &apos;None&apos;, # [f1_0, f1_1, f1_2],
        &apos;min_data_in_leaf&apos;: 10,
        &apos;learning_rate&apos;: 0.01,
        &apos;feature_fraction&apos;: 0.7,
        &apos;bagging_fraction&apos;: 0.95,
        &apos;early_stopping_rounds&apos;: 2000,
#         &apos;lambda_l1&apos;: 0.1,
#         &apos;lambda_l2&apos;: 0.1,
        &quot;first_metric_only&quot;: True,
        &apos;bagging_freq&apos;: 3, 
        &apos;max_bin&apos;: 255,
        &apos;random_state&apos;: 42,
        &apos;verbose&apos; : -1
    }


models = []
test_preds = []
val_preds = []
oof_seed = np.zeros((len(train_label), 3))
seed = [2222,2018778]
for j in seed:
    print(&quot;+++++++++++++++++ seed {} ++++++++++++&quot;.format(str(j)))
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=j)
    oof = np.zeros((len(train_label), 3))
    for i, (trn_idx, val_idx) in enumerate(skf.split(train_X, train_y)):
        print(&quot;-&quot; * 81)
        print(&quot;[!] fold {}&quot;.format(i))
        lgb_params = deepcopy(params)
        # print(lgb_params)
        trn_X = csr_matrix(train_X)[trn_idx]
        trn_y = train_y.iloc[trn_idx]
        val_X = csr_matrix(train_X)[val_idx]
        val_y = train_y.iloc[val_idx]
        dtrain = lgb.Dataset(trn_X, trn_y) 
        dval = lgb.Dataset(val_X, val_y) 
        model = lgb.train(lgb_params, 
               dtrain, 
               num_boost_round=400000,
               valid_sets=[dval], 
               feval=lambda preds, train_data: [
                   macro_f1(preds, train_data),
                   f1_single(preds, train_data, index=0),
                   f1_single(preds, train_data, index=1),
                   f1_single(preds, train_data, index=2)],
               verbose_eval=-1)
        models.append(model)
        # print(model.best_iteration)
        val_pred = model.predict(val_X, iteration=model.best_iteration)
        oof[val_idx] = val_pred
        val_y = train_y.iloc[val_idx]
        val_pred = np.argmax(val_pred, axis=1)
        print(str(i), &apos;val f1&apos;, metrics.f1_score(val_y, val_pred, average=&apos;macro&apos;))
        test_preds.append(model.predict(test_X, iteration=model.best_iteration))
        print(&quot;[!] fold {} finish\n&quot;.format(i))
        del dtrain, dval
        gc.collect()
    val_pred = np.argmax(oof, axis=1)
    print(str(j), &apos;every_flod val f1&apos;, metrics.f1_score(train_y, val_pred, average=&apos;macro&apos;))
    oof_seed += oof/len(seed)

oof1 = np.argmax(oof_seed, axis=1)
print(&apos;oof f1&apos;, metrics.f1_score(oof1,train_y, average=&apos;macro&apos;))
val_score = np.round(metrics.f1_score(oof1, train_y, average=&apos;macro&apos;),6)

def ensemble_predictions(predictions, weights=None, type_=&quot;linear&quot;):
    if not weights:
        print(&quot;[!] AVE_WGT&quot;)
        weights = [1./ len(predictions) for _ in range(len(predictions))]
    assert len(predictions) == len(weights)
    if np.sum(weights) != 1.0:
        weights = [w / np.sum(weights) for w in weights]
    print(&quot;[!] weights = {}&quot;.format(weights))
    assert np.isclose(np.sum(weights), 1.0)
    if type_ == &quot;linear&quot;:
        res = np.average(predictions, weights=weights, axis=0)
    elif type_ == &quot;harmonic&quot;:
        res = np.average([1 / p for p in predictions], weights=weights, axis=0)
        return 1 / res
    elif type_ == &quot;geometric&quot;:
        numerator = np.average(
            [np.log(p) for p in predictions], weights=weights, axis=0
        )
        res = np.exp(numerator / sum(weights))
        return res
    elif type_ == &quot;rank&quot;:
        from scipy.stats import rankdata
        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)
        return res / (len(res) + 1)
    return res

    def merge(prob, number=-1, index=0):
        from copy import deepcopy
        new_prob = deepcopy(prob)
        top = np.argsort(prob[:, index])[::-1][: number]
        print(top[: 4])
        for i in range(len(new_prob)):
            pad_value = np.array([0, 0, 0])
            pad_value[index] = 1
            if i in top:
                new_prob[i, ] = pad_value
            else:
                new_prob[i, index] = 0.
        return new_prob


test_pred_prob = ensemble_predictions(test_preds)
test_pred = test_pred_prob.argmax(axis=1)

test_pro = test_label[[&apos;ship&apos;]]
test_pro[&apos;pro_1&apos;] = test_pred_prob[:,0]
test_pro[&apos;pro_2&apos;] = test_pred_prob[:,1]
test_pro[&apos;pro_3&apos;] = test_pred_prob[:,2]
pred_pro = merge(test_pro[[&apos;pro_1&apos;, &apos;pro_2&apos;, &apos;pro_3&apos;]].values, 900,0)
test_pred = pred_pro.argmax(axis=1)


test_data = test_label[[&apos;ship&apos;]]
test_data[&quot;label&quot;] = test_pred
test_data[&quot;label&quot;] = test_data[&quot;label&quot;].map({0:&apos;围网&apos;,1:&apos;刺网&apos;,2:&apos;拖网&apos;})
# test_data[&apos;label&apos;][:100] = &apos;刺网&apos;
test_data[[&quot;ship&quot;, &quot;label&quot;]].to_csv(&quot;result.csv&quot;, index=False, header=None)
print(test_data[&quot;label&quot;].value_counts())
print(&apos;runtime:&apos;, time.time() - start_t)</code></pre>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/" rel="tag"># 算法竞赛</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/29/hello-world/" rel="prev" title="学习日志：2020华为云大数据挑战赛（正式赛）">
      <i class="fa fa-chevron-left"></i> 学习日志：2020华为云大数据挑战赛（正式赛）
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/15/xingce/" rel="next" title="行测笔记（一）：判断推理">
      行测笔记（一）：判断推理 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#2020-5-30-赛题解读"><span class="nav-text">2020.5.30 赛题解读</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#赛题背景：渔船作业分类"><span class="nav-text">赛题背景：渔船作业分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#竞赛数据"><span class="nav-text">竞赛数据:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#评估指标"><span class="nav-text">评估指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型大致思路"><span class="nav-text">模型大致思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#具体分析"><span class="nav-text">具体分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码分析"><span class="nav-text">代码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-feature-selector-py代码分析："><span class="nav-text">1. feature_selector.py代码分析：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-nmf-list-py-代码分析："><span class="nav-text">2. nmf_list.py 代码分析：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-model-py-代码分析："><span class="nav-text">3. model.py  代码分析：</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiAnG"
      src="/images/timg.jpg">
  <p class="site-author-name" itemprop="name">LiAnG</p>
  <div class="site-description" itemprop="description">Stay hungry, stay foolish</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/LiAnGGGGGG" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;LiAnGGGGGG" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1318457129@qq.com" title="E-Mail → mailto:1318457129@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="rss → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i>rss</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script
  async
  src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"
></script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiAnG</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">239k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">3:37</span>
</div>
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("05/30/2020 13:14:21");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  
  <script src="/js/src/wobblewindow.js"></script>
  <script>
    //只在桌面版网页启用特效
    if (window.innerWidth > 768) {
      $(document).ready(function () {
        
        // 根据类名获取，而不是id
        $('.header').wobbleWindow({
          radius: 50,
          movementTop: false,
          movementLeft: false,
          movementRight: false,
          debug: false,
        });
        

        
         // 根据类名获取，而不是id
        $('.sidebar').wobbleWindow({
          radius: 50,
          movementLeft: false,
          movementTop: false,
          movementBottom: false,
          position: 'fixed',
          debug: false,
        });
        

        
         // 根据类名获取，而不是id
        $('.footer').wobbleWindow({
          radius: 50,
          movementBottom: false,
          movementLeft: false,
          movementRight: false,
          
          position: 'absolute',
          debug: false,
        });
        
      });
    }
  </script>


 
<script>
  var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });
</script>






  <script async src="/js/cursor/fireworks.js"></script>



<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'rueYojQYHpaShC9b3zwCTcgt-gzGzoHsz',
      appKey     : 'SuFkJxmzL9eehU2FBrkopQlJ',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>


</body>
</html>
