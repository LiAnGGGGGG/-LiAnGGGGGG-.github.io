<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>机器学习笔记（三）线性回归</title>
    <url>/2020/07/22/LinearRegression/</url>
    <content><![CDATA[<h1 id="1-多元线性回归LinearRegression"><a href="#1-多元线性回归LinearRegression" class="headerlink" title="1 多元线性回归LinearRegression"></a>1 多元线性回归LinearRegression</h1><h2 id="1-1-多元线性回归的基本原理"><a href="#1-1-多元线性回归的基本原理" class="headerlink" title="1.1 多元线性回归的基本原理"></a>1.1 多元线性回归的基本原理</h2><p>对于一个$n$个特征的样本$i$而言，回归方程为：<br>$$\hat y_i = w_0 +w_1x_{i1}+w_2x_{i2}+\dots+w_nx_{in}$$</p>
<p>$w_0$被称为截距，$w_1 \sim w_n$被称为回归系数，$w_{i1} \sim x_{in}$是样本$i$上的不同特征，如果考虑我们有m个样本，则回归可以被写作：<br>$$\hat y = w_0 +w_1x_{1}+w_2x_{2}+\dots+w_nx_{n}$$</p>
<a id="more"></a>

<p>其中$y$是包含了m个全部样本回归的列向量，$w$可以被看作是结构为$(n+1,1)$的列矩阵，$X$是一个结构为$(m,n+1)$的特征矩阵，则有：</p>
<p>$$\begin{bmatrix}<br>\hat y_1 \\<br>\hat y_2\\<br>\hat y_3\\<br>\dots\\<br>\hat y_m\\<br>\end{bmatrix}=\begin{bmatrix}<br>1&amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots &amp; x_{1n} \\<br>1&amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots &amp; x_{2n}\\<br>1&amp; x_{31} &amp; x_{32} &amp; x_{33} &amp; \dots &amp; x_{3n}\\<br>\dots\\<br>1&amp; x_{m1} &amp; x_{m2} &amp; x_{m3} &amp; \dots &amp; x_{mn}\\<br>\end{bmatrix} * \begin{bmatrix}<br>w_0 \\<br>w_1 \\<br>w_2 \\<br>\dots\\<br>w_n \\<br>\end{bmatrix}$$</p>
<p>$$\hat y = Xw$$</p>
<p>在多元线性回归中，定义损失函数如下：</p>
<p>$$\sum_{i=1}^m(y_i-\hat y_i)^2=\sum_{i=1}^m(y_i-X_iw)^2$$</p>
<p>其中$y_i$是样本$i$对应的真实标签，$\hat y_i$，也就是$X_iw$样本$i$在一组参数$w$下的预测标签。</p>
<p>因此，最小化损失，将求解目标转化为：<br>$$\min_w||y-Xw||^2_2$$</p>
<p>我们称这个式子为SEE(Sum of Sqaured Error, 误差平方和)或者RSS(Residual Sum of Squares 残差平方和)</p>
<h2 id="1-2-最小二乘法求解多元线性回归的参数"><a href="#1-2-最小二乘法求解多元线性回归的参数" class="headerlink" title="1.2 最小二乘法求解多元线性回归的参数"></a>1.2 最小二乘法求解多元线性回归的参数</h2><p>首先，我们对$w$求导：<br>$$<br>\begin{equation}\begin{split}<br>\frac{\partial RSS}{\partial w}&amp;=\frac{\partial ||y-Xw||^2_2}{\partial w} \\<br>&amp; =\frac{\partial (y-Xw)^T(y-Xw)}{\partial w}<br>\end{split}\end{equation}<br>$$<br>$$\because (A-B)^T = A^T-B^T 并且(AB)^T=B^T*A^T$$<br>$$\therefore =\frac{\partial (y^T-w^TX^T)(y-Xw)}{\partial w}$$<br>$$=\frac{\partial(y^Ty-w^TX^Ty-y^TXw+w^TX^TXw)}{\partial w}$$</p>
<p>$\because$ 矩阵求导中，$a$为常数，有如下规则：<br>$$\frac{\partial a}{\partial A}=0,\frac{\partial A^TB^TC}{\partial A}=B^TC,\frac{\partial C^TBA}{\partial A}=B^TC,\frac{\partial A^TBA}{\partial A}=(B+B^T)A$$</p>
<p>$$=0-X^Ty-X^Ty+2X^TXw$$<br>$$=X^TXw-X^Ty$$</p>
<p>然后让一阶导数为0：<br>$$X^TXw-X^Ty=0$$<br>$$X^TXw=X^Ty$$<br>左乘一个$(X^TX)^{-1}$则有：<br>$$w=(X^TX)^{-1}X^Ty$$</p>
<p>在这里，逆矩阵的充分必要条件是特征矩阵不存在多重共线性。并且在统计学中，使用最小二乘法求解线性回归方程式一种“无偏估计”的方法，也就是标签的分布必须服从正太分布。</p>
<h2 id="1-3-linear-model-LinearRegression"><a href="#1-3-linear-model-LinearRegression" class="headerlink" title="1.3 linear_model.LinearRegression"></a>1.3 linear_model.LinearRegression</h2><p>class sklearn.linear_model.LinearRegression (ﬁt_intercept=True, normalize=False, copy_X=True, n_jobs=None)</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ﬁt_intercept</td>
<td align="left">布尔值，可不填，默认为True是否计算此模型的截距。如果设置为False，则不会计算截距</td>
</tr>
<tr>
<td align="left">normalize</td>
<td align="left">布尔值，可不填，默认为False当ﬁt_intercept设置为False时，将忽略此参数。如果为True，则特征矩阵X在进入回归之前将会被减去均值（中心化）并除以L2范式（缩放）。如果你希望进行标准化，请在ﬁt数据之前   使用preprocessing模块中的标准化专用类StandardScaler</td>
</tr>
<tr>
<td align="left">copy_X</td>
<td align="left">布尔值，可不填，默认为True如果为真，将在X.copy()上进行操作，否则的话原本的特征矩阵X可能被线性回归影响并覆盖</td>
</tr>
<tr>
<td align="left">n_jobs</td>
<td align="left">整数或者None，可不填，默认为None用于计算的作业数。只在多标签的回归和数据量足够大的时候才生效。除非None在joblib.parallel_backend上下文中，否则None统一表示为1。如果输入 -1，则表示使用全部的CPU来进行计算。</td>
</tr>
</tbody></table>
<h1 id="2-多重共线性：岭回归与Lasso"><a href="#2-多重共线性：岭回归与Lasso" class="headerlink" title="2 多重共线性：岭回归与Lasso"></a>2 多重共线性：岭回归与Lasso</h1><h2 id="2-1-多重共线性"><a href="#2-1-多重共线性" class="headerlink" title="2.1 多重共线性"></a>2.1 多重共线性</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>行测笔记：判断推理（一）图形推理</title>
    <url>/2020/07/15/xingce/</url>
    <content><![CDATA[<h2 id="图形推理的命题形式"><a href="#图形推理的命题形式" class="headerlink" title="图形推理的命题形式"></a>图形推理的命题形式</h2><ol>
<li>一组图：从左往右读题，找整体规律的共同点。或间隔跳跃观察</li>
<li>两组图：找第一组规律，应用于第二组</li>
<li>九宫格：横行（最多）、数列、对角线观察规律</li>
<li>分组分类：找寻每组统一规律</li>
<li>空间类：折纸盒，转化成平面思维（截面图、三视图、立体拼合）<a id="more"></a>

</li>
</ol>
<h2 id="图形推理考什么？（学习重点：识别图形特征）"><a href="#图形推理考什么？（学习重点：识别图形特征）" class="headerlink" title="图形推理考什么？（学习重点：识别图形特征）"></a>图形推理考什么？（学习重点：识别图形特征）</h2><ol>
<li>位置规律</li>
<li>样式规律</li>
<li>属性规律</li>
<li>特殊规律</li>
<li>数量规律</li>
<li>空间规律</li>
</ol>
<h2 id="一、位置规律"><a href="#一、位置规律" class="headerlink" title="一、位置规律"></a>一、位置规律</h2><h3 id="位置类识别特征（各图元素组成相同）"><a href="#位置类识别特征（各图元素组成相同）" class="headerlink" title="位置类识别特征（各图元素组成相同）"></a>位置类识别特征（各图元素组成相同）</h3><h3 id="（1）平移："><a href="#（1）平移：" class="headerlink" title="（1）平移："></a>（1）平移：</h3><ol>
<li>方向 ：直线（上下、左右、对角线）、绕圈（顺/逆时针）</li>
<li>步数 ：恒定、递增（等差）、周期（考的少）</li>
</ol>
<p><strong>技巧：1.多个元素分开看，边做边排除 2.位置规律——就近走原则</strong></p>
<h3 id="宫格型黑块平移"><a href="#宫格型黑块平移" class="headerlink" title="宫格型黑块平移"></a>宫格型黑块平移</h3><p><strong>1.个别黑块可重合</strong></p>
<ul>
<li>题干和选项大部分元素组成完全一致，个别一两幅图少黑块</li>
<li>题干第一幅图的黑块一般不会重合</li>
</ul>
<p><strong>2.黑块走到头后怎么办？</strong></p>
<ul>
<li>循环走：从头开始</li>
<li>折返走：直接弹回</li>
</ul>
<p><strong>3.“双胞胎”黑块们如何分辨</strong>：就近走原则</p>
<h3 id="多宫格方向判定"><a href="#多宫格方向判定" class="headerlink" title="多宫格方向判定"></a>多宫格方向判定</h3><p>题型特征：16宫格图形多个黑块平移</p>
<ol>
<li>绕圈走：中间颜色数量相同，优先考虑内外圈分开看</li>
<li>走直线：中间颜色数量不同，有限考虑走直线</li>
</ol>
<h3 id="（2）旋转："><a href="#（2）旋转：" class="headerlink" title="（2）旋转："></a>（2）旋转：</h3><ol>
<li>方向：顺时针、逆时针</li>
<li>常见角度：45、60、90、180度等</li>
</ol>
<p><strong>技巧：元素多，没思路？相邻比较走起来！</strong></p>
<p><strong>钟表类：外面一个框，中心一个点，绕了一圈线——常考旋转</strong></p>
<h3 id="（3）翻转"><a href="#（3）翻转" class="headerlink" title="（3）翻转"></a>（3）翻转</h3><ol>
<li>左右翻转：竖轴对称</li>
<li>上下翻转：横轴对称</li>
</ol>
<p><strong>元素组成相同优先看位置：对称通常是图形自身的规律，而不是两幅图之间的翻转</strong></p>
<p><strong>技巧：先看容易看懂的，旋转180度看不出来就转卷子！</strong></p>
<p><strong>如何区分旋转与翻转</strong><br>只有左右互换（上下不变）——左右翻<br>只有上下互换（左右不变）——上下翻<br>上下、左右都互换——旋转180度</p>
<p><strong>思维导图</strong><br><img src="https://note.youdao.com/yws/api/personal/file/CC02E070330340E8880E45623154A7F9?method=download&shareKey=ab0d5ad994f7c4d39f8370b641a8faa8" alt></p>
<h2 id="二、样式规律"><a href="#二、样式规律" class="headerlink" title="二、样式规律"></a>二、样式规律</h2><h3 id="样式类别识别特征：元素组成相似"><a href="#样式类别识别特征：元素组成相似" class="headerlink" title="样式类别识别特征：元素组成相似"></a>样式类别识别特征：元素组成相似</h3><h3 id="（1）加、减、同、异"><a href="#（1）加、减、同、异" class="headerlink" title="（1）加、减、同、异"></a>（1）加、减、同、异</h3><p><strong>识别特征：相同线条重复出现</strong></p>
<ol>
<li>相加、相减</li>
<li>求异（保留不同）</li>
<li>求同（保留相同）</li>
</ol>
<p><strong>技巧：对比选项，从特殊线条入手（横线、竖线、最长最短线）</strong></p>
<p><strong>位置+样式类题目，先转谁呢？  谁搞特殊，先转谁！</strong></p>
<ol>
<li>图1与图2有相同部分——先求同求异再转</li>
<li>图2与图3有相同部分——转图1</li>
<li>图1与图3有想吐部分——转图2</li>
</ol>
<h3 id="（2）黑白运算"><a href="#（2）黑白运算" class="headerlink" title="（2）黑白运算"></a>（2）黑白运算</h3><p><strong>识别特征：图形轮廓和分割区域相同，内部的颜色不同</strong><br><strong>方法：相同位置颜色做加法（注意顺序）</strong></p>
<p><strong>注意：</strong></p>
<ol>
<li>运算规则具体题目具体找</li>
<li>黑+白 不等于 白+黑，要具体题目具体验证</li>
<li>与黑块平移区分：黑块数量相同，优先平移；黑块数量不同，优先黑白运算</li>
</ol>
<p><strong>技巧：在确定可能考黑白运算后，从问好处着手解题更快，边找边验证</strong></p>
<p><strong>思维导图</strong><br><img src="https://note.youdao.com/yws/api/personal/file/96D9D2A1C7614181B52F461B9C1872E7?method=download&shareKey=bb4bd7eaf38467812c12098b8c66fbff" alt></p>
<h2 id="三、属性规律"><a href="#三、属性规律" class="headerlink" title="三、属性规律"></a>三、属性规律</h2><h3 id="属性类别识别特征-元素组成不相同、不相似——优先属性"><a href="#属性类别识别特征-元素组成不相同、不相似——优先属性" class="headerlink" title="属性类别识别特征: 元素组成不相同、不相似——优先属性"></a>属性类别识别特征: 元素组成不相同、不相似——优先属性</h3><h3 id="（1）对称性"><a href="#（1）对称性" class="headerlink" title="（1）对称性"></a>（1）对称性</h3><ol>
<li>轴对称：”等腰”元素出现</li>
<li>中心对称：平行四边形，N,Z,S变形图出现</li>
<li>轴对称+中心对称：图形存在相互垂直的对称轴</li>
<li>对称性的细化考法<ul>
<li>对称轴的方向与数量</li>
<li>对称轴与图形中线的位置关系：重合、垂直、交点</li>
<li>角度：平行、垂直、交叉某角度</li>
</ul>
</li>
</ol>
<p><strong>优先考虑对称轴时，先画出图形对称轴！</strong></p>
<p><strong>技巧：整体没有规律，分开看</strong></p>
<h3 id="（2）曲直性"><a href="#（2）曲直性" class="headerlink" title="（2）曲直性"></a>（2）曲直性</h3><ol>
<li>全直线图形</li>
<li>全曲线图形</li>
<li>曲+直图形</li>
</ol>
<p><strong>常见考法：三种情况均出现；与其他考点结合命题</strong></p>
<p><strong>所有图形均有外框——分开看</strong></p>
<h3 id="（3）开闭性"><a href="#（3）开闭性" class="headerlink" title="（3）开闭性"></a>（3）开闭性</h3><p><strong>题型特征：完整图的图形留了小开口，可以考虑开闭性</strong></p>
<ol>
<li>全封闭图形</li>
<li>全开放图形</li>
<li>半封闭图形</li>
</ol>
<p><strong>黑色粗线条，生活化元素/图形留有小开口——考虑开闭性</strong></p>
<p><strong>思维导图</strong><br><img src="https://note.youdao.com/yws/api/personal/file/F59E60D6989F4394BE9C6BDDDC9887CF?method=download&shareKey=649c8319245813b64168677ca4240c22" alt></p>
<h2 id="四、特殊规律"><a href="#四、特殊规律" class="headerlink" title="四、特殊规律"></a>四、特殊规律</h2><h3 id="（1）图形间关系"><a href="#（1）图形间关系" class="headerlink" title="（1）图形间关系"></a>（1）图形间关系</h3><p><strong>识别特征：两个或多个封闭图形连在一起</strong></p>
<ol>
<li>相离</li>
<li>相交<ul>
<li>相交于点</li>
<li>相交于边（数量、样式：边的长短、整体还是部分）</li>
<li>相较于面（形状）</li>
</ul>
</li>
</ol>
<p><strong>一根线牵出多个图形考虑相交于点，其他考虑相交于边或面</strong></p>
<h3 id="（2）功能元素"><a href="#（2）功能元素" class="headerlink" title="（2）功能元素"></a>（2）功能元素</h3><p><strong>识别特征：黑点、白点、箭头、小图形</strong></p>
<ol>
<li>标记位置（上、下、左、右、内、外）</li>
<li>标记图形<ul>
<li>点：交点</li>
<li>线：直线、曲线 / 最长边、最短边</li>
<li>角：直角、锐角、钝角 / 最大角、最小角</li>
<li>面：相交面 / 最大面、最小面 / 直线面、曲线面 / 特殊形状面</li>
</ul>
</li>
</ol>
<p><strong>思维导图</strong><br><img src="https://note.youdao.com/yws/api/personal/file/84A38B8255CA4301907F28ECCE456AEE?method=download&shareKey=b6b8b1a759af355f000b27566005c2c3" alt></p>
<h2 id="五、数量规律"><a href="#五、数量规律" class="headerlink" title="五、数量规律"></a>五、数量规律</h2><p><strong>识别特征：元素组成不同、不相似、且属性没规律，数量规律明显</strong></p>
<h3 id="（1）面数量"><a href="#（1）面数量" class="headerlink" title="（1）面数量"></a>（1）面数量</h3><p><strong>注意：只有封闭的白色区域才算面</strong><br><strong>识别特征</strong><br><strong>1.图形被分割，封闭面明显</strong><br><strong>2.生活化图形、粗线条图形中留空白区域</strong></p>
<p>面的细化考法：数面特征图，但整体数面无答案</p>
<ol>
<li>面的形状：三角形、四边形</li>
<li>相同面数量</li>
<li>特殊面的形状：最大面、最小面、相交面</li>
</ol>
<h3 id="（2）线数量"><a href="#（2）线数量" class="headerlink" title="（2）线数量"></a>（2）线数量</h3><p><strong>识别特征</strong><br><strong>1.直线数特征图：多边形、单一直线</strong><br><strong>2.曲线数特征图：曲线图形（圆、弧、单一曲线）</strong></p>
<p><strong>直线和曲线分开数</strong></p>
<p><strong>线的细化考法</strong></p>
<ol>
<li>线的位置：边框线条、框内线条</li>
<li>直线的细化：横线/竖线</li>
</ol>
<p><strong>大部分图形题中有曲线，优先考虑曲线规律</strong></p>
<p><strong>3.线的特殊考点：笔画问题</strong><br><strong>识别特征</strong><br><strong>1.五角星、圆相切/相交、“日”变形、“田”变形</strong><br><strong>2.多端点、出头端点、圆相交(近年热点)</strong></p>
<ol>
<li>可以一笔画<ul>
<li>线条之间连通</li>
<li>奇点数量为0或2（奇点：以一个点为中心，发射出奇数条线）</li>
</ul>
</li>
</ol>
<p><strong>端点都是奇点，别忘了数！</strong></p>
<ol start="2">
<li>多笔画<ul>
<li>笔画数 = 奇点数/2 (奇点数一定是偶数)</li>
</ul>
</li>
</ol>
<p><strong>不连通图直接数笔画</strong></p>
<h3 id="（3）点数量"><a href="#（3）点数量" class="headerlink" title="（3）点数量"></a>（3）点数量</h3><p><strong>识别特征</strong><br>1.线条交叉明显（大树杈）<br>2.乱糟糟一团线交叉<br>3.相切较多<br><strong>注意：线与线的交点数量</strong><br>1.顶点、切点是交点<br>2.端点不是交点</p>
<p><strong>点的细化考法：数点特征图，但整体点无规律</strong></p>
<ol>
<li><p>按线的属性细化：曲直交点</p>
<ul>
<li>特征：圆或弧多，且存在曲直相交</li>
</ul>
</li>
<li><p>按线的位置细化：内外交点</p>
<ul>
<li>特征：图形都有外框（内外分开看思维）</li>
</ul>
</li>
</ol>
<h3 id="（4）素数量"><a href="#（4）素数量" class="headerlink" title="（4）素数量"></a>（4）素数量</h3><p><strong>识别特征：独立的小图形</strong></p>
<ol>
<li>元素种类</li>
<li>元素个数</li>
</ol>
<p><strong>形状一致，大小不一致也看作一种元素</strong></p>
<ol start="3">
<li>部分数：连在一起的就是一部分</li>
</ol>
<p><strong>识别特征：生活化图形、黑色粗线条图形</strong></p>
<p><strong>数量规律特征图总汇(按考频排序)</strong></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/56CAC826F4B14DAAB59B45411A39D32E?method=download&shareKey=1d6c6fbe4ab78574d6561879d6ab8f91" alt></p>
<p><strong>思维导图</strong></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/99C081A3826E4102AC4C11414BEA5180?method=download&shareKey=e819a2b5c9eb4b8fa4c807d63b28184a" alt></p>
<h3 id="六、空间规律"><a href="#六、空间规律" class="headerlink" title="六、空间规律"></a>六、空间规律</h3><h3 id="（1）相对面"><a href="#（1）相对面" class="headerlink" title="（1）相对面"></a>（1）相对面</h3><p>应用：一组相对面同时出现的选项排除</p>
<ol>
<li>同行同列相隔一个面</li>
<li>Z字形紧邻中线的两端</li>
</ol>
<p><strong>出题可能会有无中生有的面</strong></p>
<h3 id="（2）相邻面——公共边"><a href="#（2）相邻面——公共边" class="headerlink" title="（2）相邻面——公共边"></a>（2）相邻面——公共边</h3><p><strong>方法1：折叠前后相邻关系保持不变</strong></p>
<ol>
<li>平面图直接相邻的两个面的公共边</li>
<li>平面图中构成直角的两个边是同一条边</li>
<li>一排4个面，两头的两条边是同一条边（1-4-1）</li>
</ol>
<p><strong>方法2：画边法</strong></p>
<ol>
<li>结合选项，找一个特殊面的唯一点<ul>
<li>特殊面：只出现一次，有可以区分的唯一点</li>
<li>唯一点：是指该面中唯一的，没有与其一样的点</li>
</ul>
</li>
<li>顺（逆）时针方向画边，并标出序号</li>
</ol>
<p><strong>注意：同一个面、同一个点、同方向</strong><br>3. 题干与选项对应：面不一致排除</p>
<p><strong>思维导图</strong><br><img src="https://note.youdao.com/yws/api/personal/file/0E167A466553402EB97E034F3FA658C2?method=download&shareKey=0071ee3ee90162d908695fcb5c0c9bc4" alt></p>
]]></content>
      <categories>
        <category>行测笔试</category>
      </categories>
      <tags>
        <tag>行测笔试</tag>
      </tags>
  </entry>
  <entry>
    <title>行测笔记：判断推理（二）类比推理</title>
    <url>/2020/07/22/xingce2/</url>
    <content><![CDATA[<h1 id="一、语义关系"><a href="#一、语义关系" class="headerlink" title="一、语义关系"></a>一、语义关系</h1><h2 id="（1）近义关系、反义关系"><a href="#（1）近义关系、反义关系" class="headerlink" title="（1）近义关系、反义关系"></a>（1）近义关系、反义关系</h2><p>如果一级关系（近反义关系）选不出答案——进行二级辨析</p>
<p>常见的二级辨析：感情色彩（褒义、贬义、中性）</p>
<a id="more"></a>

<h2 id="（2）比喻义、象征义"><a href="#（2）比喻义、象征义" class="headerlink" title="（2）比喻义、象征义"></a>（2）比喻义、象征义</h2><h1 id="二、逻辑关系"><a href="#二、逻辑关系" class="headerlink" title="二、逻辑关系"></a>二、逻辑关系</h1><h2 id="（1）全同关系"><a href="#（1）全同关系" class="headerlink" title="（1）全同关系"></a>（1）全同关系</h2><h2 id="（2）并列关系"><a href="#（2）并列关系" class="headerlink" title="（2）并列关系"></a>（2）并列关系</h2><ol>
<li>矛盾关系（只有两者）</li>
<li>反对关系（有第三者）</li>
</ol>
<p><strong>并列的细分考点：并列关系+功能相同</strong></p>
<h2 id="（3）包容关系"><a href="#（3）包容关系" class="headerlink" title="（3）包容关系"></a>（3）包容关系</h2><ol>
<li>种属关系</li>
<li>组成关系</li>
</ol>
<p><strong>区分：能用“谁是谁”造句的就是种属关系</strong></p>
<h2 id="（4）交叉关系"><a href="#（4）交叉关系" class="headerlink" title="（4）交叉关系"></a>（4）交叉关系</h2><h2 id="（5）对应关系"><a href="#（5）对应关系" class="headerlink" title="（5）对应关系"></a>（5）对应关系</h2><ol>
<li>材料</li>
<li>工艺</li>
<li>属性</li>
<li>功能（二级辨析：主要功能、次要功能）</li>
<li>时间顺序：多个行为同时出现</li>
</ol>
<p><strong>当出现多个选项符合时间顺序时，考虑动作主体</strong><br>6. 因果关系（方式目的）</p>
<h1 id="三、语法关系"><a href="#三、语法关系" class="headerlink" title="三、语法关系"></a>三、语法关系</h1><h2 id="（1）主谓关系"><a href="#（1）主谓关系" class="headerlink" title="（1）主谓关系"></a>（1）主谓关系</h2><h2 id="（2）动宾关系"><a href="#（2）动宾关系" class="headerlink" title="（2）动宾关系"></a>（2）动宾关系</h2><h2 id="（3）主宾关系"><a href="#（3）主宾关系" class="headerlink" title="（3）主宾关系"></a>（3）主宾关系</h2><h1 id="四、词语拆分"><a href="#四、词语拆分" class="headerlink" title="四、词语拆分"></a>四、词语拆分</h1><h2 id="（1）单字拆分"><a href="#（1）单字拆分" class="headerlink" title="（1）单字拆分"></a>（1）单字拆分</h2><h2 id="（2）成语被拆分"><a href="#（2）成语被拆分" class="headerlink" title="（2）成语被拆分"></a>（2）成语被拆分</h2><h2 id="（3）两组词语之间没有明显逻辑关系"><a href="#（3）两组词语之间没有明显逻辑关系" class="headerlink" title="（3）两组词语之间没有明显逻辑关系"></a>（3）两组词语之间没有明显逻辑关系</h2><hr>
<h1 id="词语、成语积累："><a href="#词语、成语积累：" class="headerlink" title="词语、成语积累："></a>词语、成语积累：</h1><ol>
<li>马放南山：天下太平，不再用兵</li>
<li>拾人牙慧：窃取别人的语言和文字</li>
<li>如火如荼：军容之盛，形容大规模的行动气势旺盛</li>
<li>犬牙交错：比喻交界线曲折，像狗牙参差不齐，也比喻情况复杂</li>
<li>纷乱如麻：交错杂乱像一团乱麻，一般形容心情</li>
<li>燕归巢：比喻客居在外的游子喜归故里</li>
<li>睁眼瞎：比喻没有文化，不识字的人，缺乏知识的人</li>
<li>得陇望蜀：比喻得寸进尺，贪心不知足</li>
<li>纲举目张：比喻条理分明</li>
<li>惩前毖后：批判以前所犯的错误，吸取教训，使以后谨慎些，不致再犯</li>
</ol>
]]></content>
      <categories>
        <category>行测笔试</category>
      </categories>
      <tags>
        <tag>行测笔试</tag>
      </tags>
  </entry>
  <entry>
    <title>学习日志：2020智慧海洋建设top5方案学习</title>
    <url>/2020/05/30/My-New-Post/</url>
    <content><![CDATA[<h2 id="2020-5-30-赛题解读"><a href="#2020-5-30-赛题解读" class="headerlink" title="2020.5.30 赛题解读"></a>2020.5.30 赛题解读</h2><h3 id="赛题背景：渔船作业分类"><a href="#赛题背景：渔船作业分类" class="headerlink" title="赛题背景：渔船作业分类"></a>赛题背景：渔船作业分类</h3><p>本赛题基于位置数据对海上目标进行智能识别和作业行为分析，要求选手通过分析渔船北斗设备位置数据，得出该船的生产作业行为，具体判断出是拖网作业、围网作业还是流刺网作业。初赛将提供11000条(其中7000条训练数据、2000条testA、2000条testB)渔船轨迹北斗数据。</p>
<a id="more"></a>

<p>复赛考虑以往渔船在海上作业时主要依赖AIS数据，北斗相比AIS数据，数据上报频率和数据质量均低于AIS数据，因此复赛拟加入AIS轨迹数据辅助北斗数据更好的做渔船类型识别，其中AIS数据与北斗数据的匹配需选手自行实现，具体细节复赛开赛时更新。同时，希望选手通过数据可视化与分析，挖掘更多海洋通信导航设备的应用价值。</p>
<h3 id="竞赛数据"><a href="#竞赛数据" class="headerlink" title="竞赛数据:"></a>竞赛数据:</h3><p>提供11000条渔船北斗数据，数据包含脱敏后的渔船ID、经纬度坐标、上报时间、速度、航向信息，由于真实场景下海上环境复杂，经常出现信号丢失，设备故障等原因导致的上报坐标错误、上报数据丢失、甚至有些设备疯狂上报等。</p>
<p>数据示例：</p>
<table>
<thead>
<tr>
<th align="left">渔船ID</th>
<th align="center">x</th>
<th align="center">y</th>
<th align="center">速度</th>
<th align="center">方向</th>
<th align="center">time</th>
<th align="right">type</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1102</td>
<td align="center">6283649.656204367</td>
<td align="center">5284013.963699763</td>
<td align="center">3</td>
<td align="center">12.1</td>
<td align="center">0921 09:00</td>
<td align="right">围网</td>
</tr>
</tbody></table>
<p>渔船ID：渔船的唯一识别，结果文件以此ID为标示</p>
<p>x: 渔船在平面坐标系的x轴坐标</p>
<p>y: 渔船在平面坐标系的y轴坐标</p>
<p>速度：渔船当前时刻航速，单位节</p>
<p>方向：渔船当前时刻航首向，单位度</p>
<p>time：数据上报时刻，单位月日 时：分</p>
<p>type：渔船label，作业类型</p>
<p>原始数据经过脱敏处理，渔船信息被隐去，坐标等信息精度和位置被转换偏移。<br>选手可通过学习围网、刺网、拖网等专业知识辅助大赛数据处理。<br>AIS数据</p>
<table>
<thead>
<tr>
<th align="left">ais_id</th>
<th align="center">lon</th>
<th align="center">lat</th>
<th align="center">速度</th>
<th align="center">航向</th>
<th align="right">time</th>
</tr>
</thead>
<tbody><tr>
<td align="left">110</td>
<td align="center">119.6705</td>
<td align="center">26.5938</td>
<td align="center">3</td>
<td align="center">12.1</td>
<td align="right">0921 09:00</td>
</tr>
</tbody></table>
<p>ais_id：AIS设备的唯一识别ID</p>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>选手提交结果与实际渔船作业类型结果进行对比，以3种类别的各自F1值取平均做为评价指标，结果越大越好，具体计算公式如下：</p>
<p>$$Score ={F1_{围网}+F1_{刺网}+F1_{拖网}  \over 3}$$</p>
<p>$$F1 ={2\ast P\ast R\over P+R}$$</p>
<p>其中P为某类别的准确率，R为某类别的召回率，评测程序f1函数为sklearn.metrics.f1_score，average=’macro’。</p>
<h2 id="模型大致思路"><a href="#模型大致思路" class="headerlink" title="模型大致思路"></a>模型大致思路</h2><ul>
<li><p>将所有数据数据切入：速度等于0和非0，白天和黑夜，四个数据集对每艘船的速度，方向，xy进行统计。</p>
</li>
<li><p>采用TFIDF对速度和XY进行抽取特征并降维</p>
</li>
<li><p>采用自然语言思路对速度，xy进行嵌入</p>
</li>
<li><p>训练模型前采用Lightgbm进行初步的特征筛选</p>
</li>
<li><p>最后用Lightgbm进行模型训练</p>
</li>
</ul>
<h2 id="具体分析"><a href="#具体分析" class="headerlink" title="具体分析"></a>具体分析</h2><p><strong>1. 按照同一个渔船id速度为0和非0两部分进行分析</strong></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861" alt></p>
<p>思路：</p>
<p>1、针对同一艘渔船，将其数据分为 速度为0和非0两个部分。分别统计该船在速度为0 和 非0情况下做可视化分析，观察经纬度xy、方向direction这些原始特征的变化情况（均值、方差、极值、峰度、偏度等统计特征）</p>
<p>2、根据1构建的特征，原始特征被构造出一系列统计特征，一种含义的特征会被分成速度为0和非0情况。根据这个特点，对这些特征进行一个比值处理。</p>
<p><strong>2. 渔船在白天和黑夜会按照同一个渔船id白天和黑夜两部分进行分析</strong></p>
<p>早6点整至晚8点整设置为白天(图标识Day)</p>
<p>晚8点整至早6点整设置为黑夜(图标识Night)</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/5045B84827DC4E5B87F4E56EE349769F?method=download&shareKey=7afd0c781b27434a8a4595a52bbd0861" alt></p>
<p>思路：<br>1、    数据按照时间划分成白天和黑夜两部分，分别统计该船在不同时间做可视化分析，观察经纬度xy、方向direction这些原始特征的变化情况（均值、方差、极值、峰度、偏度等统计特征）</p>
<p>2、    根据1构造的两组时间特征，提取关键的速度speed、经纬度xy进行白天与黑夜特征的对比。</p>
<p><strong>3. 借鉴自然语言处理（NLP）角度去处理船的轨迹特征</strong></p>
<p>速度speed、经纬度xy按照作业时间排序，可以反映出每艘船的行为规律。而每种作业方式都有其内在的一些规律, 借鉴自然语言处理(NLP)的相关算法进行特征提取。利用nlp的算法对速度、经纬这些序列的学习，尝试挖掘出每艘船的行为特点。</p>
<p>思路一：TF-IDF + NMF(如图，从左到有分别是ngram=1, ngram=2,ngram=3，经过t-SNE降维的可视化结果)</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/BAC79B4C1B3440D6BFF6A1D51AC8A0FF?method=download&shareKey=16b799d9748d761fab551bec75914030" alt></p>
<p>1、    使用不同的ngram去处理每个渔船的速度、经纬度数据，提取出每艘船的TF-IDF特征（ngram=1, 2, 3）。</p>
<p>2、    并利用非负矩阵分解(NMF)算法，对处理后的速度、经纬度进行降维生成一个主题分布向量。（此题目分成了8类）。</p>
<p>3、    对每个渔船的主题分布向量进行T-SNE降维，进行可视化。</p>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>该代码主要包括三个文件</p>
<ul>
<li>feature_selector.py 特征选择文件</li>
<li>nmf_list.py 处理轨迹文件</li>
<li>model.py 模型文件</li>
</ul>
<h3 id="1-feature-selector-py代码分析："><a href="#1-feature-selector-py代码分析：" class="headerlink" title="1. feature_selector.py代码分析："></a><strong>1. feature_selector.py代码分析：</strong></h3><pre><code># numpy and pandas for data manipulation
import pandas as pd
import numpy as np

# model used for feature importances
import lightgbm as lgb

# utility for early stopping with a validation set
from sklearn.model_selection import train_test_split

# visualizations
import matplotlib.pyplot as plt
import seaborn as sns

# memory management
import gc

# utilities
from itertools import chain
class FeatureSelector():
&quot;&quot;&quot;    
这个类用于为机器学习或数据预处理执行特征选择
实现五种不同的方法来识别要删除的特性
1、查找丢失百分比大于指定阈值的列
2、查找具有唯一值的列
3、找出相关系数大于指定相关系数的相关变量
4、从梯度增强机(gbm)中查找特性重要性为0.0的特性
5、从gbm中查找不影响指定的累积特性重要性的低重要性特性

参数
--------
data:dataframe
    一个数据集，行中有观察值，列中有特性
labels : array or series, default = None
    数组标签用于训练机器学习模型，以发现特征重要性。它们可以是二进制标签
    (如果任务是“分类”)或连续目标(如果任务是“回归”)。
    如果没有提供标签，那么基于特征重要性的方法是不可用的。

属性
--------
ops : dict
    运行的操作字典和要删除的特性
missing_stats : dataframe
    所有特征的缺失值的比例
record_missing : dataframe
    缺失值在阈值以上的特征的缺失值的比例
unique_stats: dataframe
    所有特性的唯一值的个数
record_single_unique: dataframe
    记录具有唯一值的特性
corr_matrix : dataframe
    数据中所有特征之间的所有相关性
record_collinear : dataframe
    记录相关系数高于阈值的相关变量对
feature_importances: dataframe
    从梯度增强机的所有特征的重要性
record_zero_importance: dataframe
    根据gbm记录数据中的零重要性特征
record_low_importance: dataframe
    根据gbm记录不需要达到累积重要性阈值的最低重要性特征

Notes
--------
    -所有5个操作都可以用identify_all方法运行。
    -如果使用特性重要度，则对创建新列的分类变量使用one-hot编码
&quot;&quot;&quot;
    def __init__(self, data, labels=None):            
        # 数据集和标签
        self.data = data
        self.labels = labels
        if labels is None:
            print(&apos;No labels provided. Feature importance based methods are not available.&apos;)    
        # 记录关于要删除的特性的信息    
        self.record_missing = None
        self.record_single_unique = None
        self.record_collinear = None
        self.record_zero_importance = None
        self.record_low_importance = None

        self.missing_stats = None
        self.unique_stats = None
        self.corr_matrix = None
        self.feature_importances = None
        # 用于保存删除操作的字典
        self.ops = {}

        self.one_hot_correlated = False

    def identify_missing(self, missing_threshold):

        # 找到丢失值大于&apos; missing_threshold &apos;的部分特征
        self.missing_threshold = missing_threshold
        # 计算每一列特征的缺失率
        missing_series = self.data.isnull().sum() / self.data.shape[0]
        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {&apos;index&apos;: &apos;feature&apos;, 0: &apos;missing_fraction&apos;})
        # 将特征的缺失率排序
        self.missing_stats = self.missing_stats.sort_values(&apos;missing_fraction&apos;, ascending = False)
        #找到缺失百分比大于阈值的列
        record_missing = pd.DataFrame(missing_series[missing_series &gt; missing_threshold]).reset_index().rename(columns = 
                                                                                                                   {&apos;index&apos;: &apos;feature&apos;, 
                                                                                                                    0: &apos;missing_fraction&apos;})
        to_drop = list(record_missing[&apos;feature&apos;])
        self.record_missing = record_missing
        self.ops[&apos;missing&apos;] = to_drop

        print(&apos;%d features with greater than %0.2f missing values.\n&apos; % (len(self.ops[&apos;missing&apos;]), self.missing_threshold))

    def identify_single_unique(self):
    # 查找只有一个唯一值的特征

        # 计算每个列中的惟一计数
        unique_counts = self.data.nunique()
        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {&apos;index&apos;: &apos;feature&apos;, 0: &apos;nunique&apos;})
        self.unique_stats = self.unique_stats.sort_values(&apos;nunique&apos;, ascending = True)

        # 查找只有惟一计数的列
        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {&apos;index&apos;: &apos;feature&apos;, 
                                                                                                               0: &apos;nunique&apos;})
        to_drop = list(record_single_unique[&apos;feature&apos;])
        self.record_single_unique = record_single_unique
        self.ops[&apos;single_unique&apos;] = to_drop
        print(&apos;%d features with a single unique value.\n&apos; % len(self.ops[&apos;single_unique&apos;]))

    def identify_collinear(self, correlation_threshold, one_hot=False):
        &quot;&quot;&quot;
        找寻相关系数大于“correlation_threshold”的特征并删除

        参数
        --------
        correlation_threshold : float between 0 and 1
        one_hot : boolean, default = False
        &quot;&quot;&quot;
        self.correlation_threshold = correlation_threshold
        self.one_hot_correlated = one_hot
        # 计算每一列之间的相关性
        if one_hot:

            # one_hot编码
            features = pd.get_dummies(self.data)
            self.one_hot_features = [column for column in features.columns if column not in self.base_features]

            # 向原始数据添加一个热编码数据
            self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)

            corr_matrix = pd.get_dummies(features).corr()

        else:
            corr_matrix = self.data.corr()

        self.corr_matrix = corr_matrix

        # 提取关联矩阵的上三角
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))

        # 选择相关性高于阈值的特性
         # 需要使用绝对值
        to_drop = [column for column in upper.columns if any(upper[column].abs() &gt; correlation_threshold)]

        # 保存线性相关特征
        record_collinear = pd.DataFrame(columns = [&apos;drop_feature&apos;, &apos;corr_feature&apos;, &apos;corr_value&apos;])

        # 遍历列以删除相关特性对
        for column in to_drop:

            # 找出相关特征
            corr_features = list(upper.index[upper[column].abs() &gt; correlation_threshold])

            # 找出相关系数
            corr_values = list(upper[column][upper[column].abs() &gt; correlation_threshold])
            drop_features = [column for _ in range(len(corr_features))]

            # 记录信息(现在需要一个临时df)
            temp_df = pd.DataFrame.from_dict({&apos;drop_feature&apos;: drop_features,
                                                 &apos;corr_feature&apos;: corr_features,
                                                 &apos;corr_value&apos;: corr_values})

            #添加到dataframe
            record_collinear = record_collinear.append(temp_df, ignore_index = True)

        self.record_collinear = record_collinear
        self.ops[&apos;collinear&apos;] = to_drop

        print(&apos;%d features with a correlation magnitude greater than %0.2f.\n&apos; % (len(self.ops[&apos;collinear&apos;]), self.correlation_threshold))

    def identify_zero_importance(self, task, eval_metric=None, 
                                 n_iterations=10, early_stopping = True):
        &quot;&quot;&quot;
        根据梯度增强机识别零重要性的特征。
        gbm可以使用验证集进行早期停止训练，以防止过拟合。
        在“n_iteration”上对特征重要性求平均值以减少方差。

        参数 
        --------
        eval_metric : string
            评价指标用于梯度提升机的早期停止，如果&apos; early_stopped &apos;为真，则必须提供
        task : string
            机器学习任务，是“classification”还是“regression”
        n_iterations : int, default = 10
            gbm的训练迭代次数
        early_stopping : boolean, default = True
            是否在训练时使用提前停止与验证集

        Notes
        --------
            - 在训练前热编码特征
            - gbm没有针对任何特定任务进行优化，可能需要进行一些超参数调优
            - 特性重要性，包括零重要性特性，可以在运行过程中改变
        &quot;&quot;&quot;
        if early_stopping and eval_metric is None:
            raise ValueError(&quot;&quot;&quot;eval metric must be provided with early stopping. Examples include &quot;auc&quot; for classification or
                             &quot;l2&quot; for regression.&quot;&quot;&quot;)
        if self.labels is None:
             raise ValueError(&quot;No training labels provided.&quot;)
        # 热编码特征
        features = pd.get_dummies(self.data)
        self.one_hot_features = [column for column in features.columns if column not in self.base_features]

        # 将热编码数据添加到原始数据中
        self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)

        # 提取特征名字
        feature_names = list(features.columns)

        # 转换成np array
        features = np.array(features)
        labels = np.array(self.labels).reshape((-1, ))

        # 创建特征重要性数组
        feature_importance_values = np.zeros(len(feature_names))

        print(&apos;Training Gradient Boosting Model\n&apos;)

        # 在每折中迭代
        for _ in range(n_iterations):
            if task == &apos;classification&apos;:
                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)

            elif task == &apos;regression&apos;:
                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)
            else:
                raise ValueError(&apos;Task must be either &quot;classification&quot; or &quot;regression&quot;&apos;)

            # 如果使用早期停止训练需要一个验证集
            if early_stopping:
                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15, stratify=labels)
                # 使用早停机制训练模型
                model.fit(train_features, train_labels, eval_metric = eval_metric,
                              eval_set = [(valid_features, valid_labels)],
                              early_stopping_rounds = 100, verbose = -1)
                # 清空内存
                gc.enable()
                del train_features, train_labels, valid_features, valid_labels
                gc.collect()
            else:
                model.fit(features, labels)

            # 记录特征重要性
            feature_importance_values += model.feature_importances_ / n_iterations

        feature_importances = pd.DataFrame({&apos;feature&apos;: feature_names, &apos;importance&apos;: feature_importance_values})

        # 根据特征重要性排序
        feature_importances = feature_importances.sort_values(&apos;importance&apos;, ascending = False).reset_index(drop = True)

        # 归一化特征重要性
        feature_importances[&apos;normalized_importance&apos;] = feature_importances[&apos;importance&apos;] / feature_importances[&apos;importance&apos;].sum()
        feature_importances[&apos;cumulative_importance&apos;] = np.cumsum(feature_importances[&apos;normalized_importance&apos;])

        # 提取特征重要性为0的特征
        record_zero_importance = feature_importances[feature_importances[&apos;importance&apos;] == 0.0]

        to_drop = list(record_zero_importance[&apos;feature&apos;])

        self.feature_importances = feature_importances
        self.record_zero_importance = record_zero_importance
        self.ops[&apos;zero_importance&apos;] = to_drop

        print(&apos;\n%d features with zero importance after one-hot encoding.\n&apos; % len(self.ops[&apos;zero_importance&apos;]))

    def identify_low_importance(self, cumulative_importance):
        &quot;&quot;&quot;
        找到特征重要性低于“cumulative_importance”的特征

        参数
        --------
        cumulative_importance : float between 0 and 1
            重要性分数
        &quot;&quot;&quot;
        self.cumulative_importance = cumulative_importance

        # 特征重要性需要在运行之前计算
        if self.feature_importances is None:
            raise NotImplementedError(&quot;&quot;&quot;Feature importances have not yet been determined. 
                                     Call the `identify_zero_importance` method first.&quot;&quot;&quot;)

        # 将特征重要性排序
        self.feature_importances = self.feature_importances.sort_values(&apos;cumulative_importance&apos;)

        # 识别出特征重要性低于设定阈值的特征
        record_low_importance = self.feature_importances[self.feature_importances[&apos;cumulative_importance&apos;] &gt; cumulative_importance]

        to_drop = list(record_low_importance[&apos;feature&apos;])

        self.record_low_importance = record_low_importance
        self.ops[&apos;low_importance&apos;] = to_drop

        print(&apos;%d features required for cumulative importance of %0.2f after one hot encoding.&apos; % (len(self.feature_importances) -
                                                                            len(self.record_low_importance), self.cumulative_importance))
        print(&apos;%d features do not contribute to cumulative importance of %0.2f.\n&apos; % (len(self.ops[&apos;low_importance&apos;]),
                                                                                               self.cumulative_importance))

    def identify_all(self, selection_params):
        &quot;&quot;&quot;
        使用所有五种方法来删除不需要的特征

        参数
        --------
        selection_params : dict
            在五种特征选择方法中使用的参数。
            参数必须包含键[&apos;missing_threshold&apos;， &apos;correlation_threshold&apos;， &apos;eval_metric&apos;， &apos;task&apos;， &apos; collecative_importance &apos;]

        # 检查所必要的参数
        for param in [&apos;missing_threshold&apos;, &apos;correlation_threshold&apos;, &apos;eval_metric&apos;, &apos;task&apos;, &apos;cumulative_importance&apos;]:
            if param not in selection_params.keys():
                raise ValueError(&apos;%s is a required parameter for this method.&apos; % param)

        # 实现五种方法
        self.identify_missing(selection_params[&apos;missing_threshold&apos;])
        self.identify_single_unique()
        self.identify_collinear(selection_params[&apos;correlation_threshold&apos;])
        self.identify_zero_importance(task = selection_params[&apos;task&apos;], eval_metric = selection_params[&apos;eval_metric&apos;])
        self.identify_low_importance(selection_params[&apos;cumulative_importance&apos;])

        # 查找要删除的特性的数量
        self.all_identified = set(list(chain(*list(self.ops.values()))))
        self.n_identified = len(self.all_identified)

        print(&apos;%d total features out of %d identified for removal after one-hot encoding.\n&apos; % (self.n_identified, 
                                                                                                  self.data_all.shape[1]))

     def check_removal(self, keep_one_hot=True):
         &quot;&quot;&quot;
         在删除前检查已识别的特征。返回一个列表的独特的功能识别。
         &quot;&quot;&quot;
        self.all_identified = set(list(chain(*list(self.ops.values()))))
        print(&apos;Total of %d features identified for removal&apos; % len(self.all_identified))

        if not keep_one_hot:
            if self.one_hot_features is None:
                print(&apos;Data has not been one-hot encoded&apos;)
            else:
                one_hot_to_remove = [x for x in self.one_hot_features if x not in self.all_identified]
                print(&apos;%d additional one-hot features can be removed&apos; % len(one_hot_to_remove))

        return list(self.all_identified)

    def remove(self, methods, keep_one_hot = True):
        &quot;&quot;&quot;
        根据指定的方法从数据中删除特征。

        参数
        --------
            methods : &apos;all&apos; or list of methods
                可以是[&apos;missing&apos;， &apos;single_unique&apos;， &apos;collinear&apos;， &apos;zero_importance&apos;， &apos;low_importance&apos;]
            keep_one_hot : boolean, default = True
                是否热编码

        返回
        --------
            data : dataframe
                删除了特征的数据

        Notes 
        --------
            -如果使用特性重要度，则一个热编码列将被添加到数据中(然后可能被删除)
            -在转换数据之前，检查将被删除的功能!
        features_to_drop = []

        if methods == &apos;all&apos;:

            # 热编码数据
            data = self.data_all

            print(&apos;{} methods have been run\n&apos;.format(list(self.ops.keys())))

            # 找到需要删除的特征
            features_to_drop = set(list(chain(*list(self.ops.values()))))

        else:
            # Need to use one-hot encoded data as well
            if &apos;zero_importance&apos; in methods or &apos;low_importance&apos; in methods or self.one_hot_correlated:
                data = self.data_all

            else:
                data = self.data

            # 遍历指定方法
            for method in methods:
                # 确定方法已经在运行
                if method not in self.ops.keys():
                    raise NotImplementedError(&apos;%s method has not been run&apos; % method)

                # 添加要删除的方法
                else:
                    features_to_drop.append(self.ops[method])

            # 找到要删除的特征
            features_to_drop = set(list(chain(*features_to_drop)))

        features_to_drop = list(features_to_drop)

        if not keep_one_hot:

            if self.one_hot_features is None:
                print(&apos;Data has not been one-hot encoded&apos;)
            else:

                features_to_drop = list(set(features_to_drop) | set(self.one_hot_features))

        # 在原数据中删除特征
        data = data.drop(columns = features_to_drop)
        self.removed_features = features_to_drop

        if not keep_one_hot:
            print(&apos;Removed %d features including one-hot features.&apos; % len(features_to_drop))
         else:
            print(&apos;Removed %d features.&apos; % len(features_to_drop))

        return data

    # 各种绘图函数
    def plot_missing(self):
        &quot;&quot;&quot;Histogram of missing fraction in each feature&quot;&quot;&quot;
        if self.record_missing is None:
            raise NotImplementedError(&quot;Missing values have not been calculated. Run `identify_missing`&quot;)

        self.reset_plot()

        # Histogram of missing values
        plt.style.use(&apos;seaborn-white&apos;)
        plt.figure(figsize = (7, 5))
        plt.hist(self.missing_stats[&apos;missing_fraction&apos;], bins = np.linspace(0, 1, 11), edgecolor = &apos;k&apos;, color = &apos;red&apos;, linewidth = 1.5)
        plt.xticks(np.linspace(0, 1, 11));
        plt.xlabel(&apos;Missing Fraction&apos;, size = 14); plt.ylabel(&apos;Count of Features&apos;, size = 14); 
        plt.title(&quot;Fraction of Missing Values Histogram&quot;, size = 16);


    def plot_unique(self):
        &quot;&quot;&quot;Histogram of number of unique values in each feature&quot;&quot;&quot;
        if self.record_single_unique is None:
            raise NotImplementedError(&apos;Unique values have not been calculated. Run `identify_single_unique`&apos;)

        self.reset_plot()

        # Histogram of number of unique values
        self.unique_stats.plot.hist(edgecolor = &apos;k&apos;, figsize = (7, 5))
        plt.ylabel(&apos;Frequency&apos;, size = 14); plt.xlabel(&apos;Unique Values&apos;, size = 14); 
        plt.title(&apos;Number of Unique Values Histogram&apos;, size = 16);


    def plot_collinear(self, plot_all = False):
        &quot;&quot;&quot;
        Heatmap of the correlation values. If plot_all = True plots all the correlations otherwise
        plots only those features that have a correlation above the threshold

        Notes
        --------
            - Not all of the plotted correlations are above the threshold because this plots
            all the variables that have been idenfitied as having even one correlation above the threshold
            - The features on the x-axis are those that will be removed. The features on the y-axis
            are the correlated features with those on the x-axis

        Code adapted from https://seaborn.pydata.org/examples/many_pairwise_correlations.html
        &quot;&quot;&quot;

        if self.record_collinear is None:
            raise NotImplementedError(&apos;Collinear features have not been idenfitied. Run `identify_collinear`.&apos;)

        if plot_all:
            corr_matrix_plot = self.corr_matrix
            title = &apos;All Correlations&apos;

        else:
            # Identify the correlations that were above the threshold
            # columns (x-axis) are features to drop and rows (y_axis) are correlated pairs
            corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear[&apos;corr_feature&apos;])), 
                                                    list(set(self.record_collinear[&apos;drop_feature&apos;]))]

            title = &quot;Correlations Above Threshold&quot;


        f, ax = plt.subplots(figsize=(10, 8))

        # Diverging colormap
        cmap = sns.diverging_palette(220, 10, as_cmap=True)

        # Draw the heatmap with a color bar
        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,
                    linewidths=.25, cbar_kws={&quot;shrink&quot;: 0.6})

        # Set the ylabels 
        ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])
        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));

        # Set the xlabels 
        ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])
        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));
        plt.title(title, size = 14)

    def plot_feature_importances(self, plot_n = 15, threshold = None):
        &quot;&quot;&quot;
        Plots `plot_n` most important features and the cumulative importance of features.
        If `threshold` is provided, prints the number of features needed to reach `threshold` cumulative importance.

        Parameters
        --------

        plot_n : int, default = 15
            Number of most important features to plot. Defaults to 15 or the maximum number of features whichever is smaller

        threshold : float, between 0 and 1 default = None
            Threshold for printing information about cumulative importances

        &quot;&quot;&quot;

        if self.record_zero_importance is None:
            raise NotImplementedError(&apos;Feature importances have not been determined. Run `idenfity_zero_importance`&apos;)

        # Need to adjust number of features if greater than the features in the data
        if plot_n &gt; self.feature_importances.shape[0]:
            plot_n = self.feature_importances.shape[0] - 1

        self.reset_plot()

        # Make a horizontal bar chart of feature importances
        plt.figure(figsize = (10, 6))
        ax = plt.subplot()

        # Need to reverse the index to plot most important on top
        # There might be a more efficient method to accomplish this
        ax.barh(list(reversed(list(self.feature_importances.index[:plot_n]))), 
                self.feature_importances[&apos;normalized_importance&apos;][:plot_n], 
                align = &apos;center&apos;, edgecolor = &apos;k&apos;)

        # Set the yticks and labels
        ax.set_yticks(list(reversed(list(self.feature_importances.index[:plot_n]))))
        ax.set_yticklabels(self.feature_importances[&apos;feature&apos;][:plot_n], size = 12)

        # Plot labeling
        plt.xlabel(&apos;Normalized Importance&apos;, size = 16); plt.title(&apos;Feature Importances&apos;, size = 18)
        plt.show()

        # Cumulative importance plot
        plt.figure(figsize = (6, 4))
        plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances[&apos;cumulative_importance&apos;], &apos;r-&apos;)
        plt.xlabel(&apos;Number of Features&apos;, size = 14); plt.ylabel(&apos;Cumulative Importance&apos;, size = 14); 
        plt.title(&apos;Cumulative Feature Importance&apos;, size = 16);

        if threshold:

            # Index of minimum number of features needed for cumulative importance threshold
            # np.where returns the index so need to add 1 to have correct number
            importance_index = np.min(np.where(self.feature_importances[&apos;cumulative_importance&apos;] &gt; threshold))
            plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles=&apos;--&apos;, colors = &apos;blue&apos;)
            plt.show();

            print(&apos;%d features required for %0.2f of cumulative importance&apos; % (importance_index + 1, threshold))

    def reset_plot(self):
        plt.rcParams = plt.rcParamsDefault</code></pre><h3 id="2-nmf-list-py-代码分析："><a href="#2-nmf-list-py-代码分析：" class="headerlink" title="2. nmf_list.py 代码分析："></a><strong>2. nmf_list.py 代码分析：</strong></h3><pre><code>import pickle
import numpy as np
import pandas as pd
from collections import Counter
from sklearn.decomposition import NMF
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import tqdm
from gensim.models import FastText, Word2Vec
import multiprocessing

class nmf_list(object):
    def __init__(self,data,by_name,to_list,nmf_n,top_n):
        self.data = data
        self.by_name = by_name
        self.to_list = to_list
        self.nmf_n = nmf_n
        self.top_n = top_n

    def run(self,tf_n):
        df_all = self.data.groupby(self.by_name)[self.to_list].apply(lambda x :&apos;|&apos;.join(x)).reset_index()
        self.data =df_all.copy()

        print(&apos;bulid word_fre&apos;)
    # 词频的构建
    def word_fre(x):
        word_dict = []
        x = x.split(&apos;|&apos;)
        docs = []
        for doc in x:
            doc = doc.split()
            docs.append(doc)
            word_dict.extend(doc)
        word_dict = Counter(word_dict)
        new_word_dict = {}
        for key,value in word_dict.items():
            new_word_dict[key] = [value,0]
        del word_dict  
        del x
        for doc in docs:
            doc = Counter(doc)
            for word in doc.keys():
                new_word_dict[word][1] += 1
        return new_word_dict 
    self.data[&apos;word_fre&apos;] = self.data[self.to_list].apply(word_fre)

    print(&apos;bulid top_&apos; + str(self.top_n))
    # 设定100个高频词
    def top_100(word_dict):
        return sorted(word_dict.items(),key = lambda x:(x[1][1],x[1][0]),reverse = True)[:self.top_n]
    self.data[&apos;top_&apos;+str(self.top_n)] = self.data[&apos;word_fre&apos;].apply(top_100)
    def top_100_word(word_list):
        words = []
        for i in word_list:
            i = list(i)
            words.append(i[0])
        return words 
    self.data[&apos;top_&apos;+str(self.top_n)+&apos;_word&apos;] = self.data[&apos;top_&apos; + str(self.top_n)].apply(top_100_word)
    # print(&apos;top_&apos;+str(self.top_n)+&apos;_word的shape&apos;)
    print(self.data.shape)

    word_list = []
    for i in self.data[&apos;top_&apos;+str(self.top_n)+&apos;_word&apos;].values:
        word_list.extend(i)
    word_list = Counter(word_list)
    word_list = sorted(word_list.items(),key = lambda x:x[1],reverse = True)
    user_fre = []
    for i in word_list:
        i = list(i)
        user_fre.append(i[1]/self.data[self.by_name].nunique())
    stop_words = []
    for i,j in zip(word_list,user_fre):
        if j&gt;0.5:
            i = list(i)
            stop_words.append(i[0])

    print(&apos;start title_feature&apos;)
    # 讲融合后的taglist当作一句话进行文本处理
    self.data[&apos;title_feature&apos;] = self.data[self.to_list].apply(lambda x: x.split(&apos;|&apos;))
    self.data[&apos;title_feature&apos;] = self.data[&apos;title_feature&apos;].apply(lambda line: [w for w in line if w not in stop_words])
    self.data[&apos;title_feature&apos;] = self.data[&apos;title_feature&apos;].apply(lambda x: &apos; &apos;.join(x))

    print(&apos;start NMF&apos;)
    # 使用tfidf对元素进行处理
    tfidf_vectorizer = TfidfVectorizer(ngram_range=(tf_n,tf_n))
    tfidf = tfidf_vectorizer.fit_transform(self.data[&apos;title_feature&apos;].values)
    #使用nmf算法，提取文本的主题分布
    text_nmf = NMF(n_components=self.nmf_n).fit_transform(tfidf)


    # 整理并输出文件
    name = [str(tf_n) + self.to_list + &apos;_&apos; +str(x) for x in range(1,self.nmf_n+1)]
    tag_list = pd.DataFrame(text_nmf)
    print(tag_list.shape)
    tag_list.columns = name
    tag_list[self.by_name] = self.data[self.by_name]
    column_name = [self.by_name] + name
    tag_list = tag_list[column_name]
    return tag_list</code></pre><h3 id="3-model-py-代码分析："><a href="#3-model-py-代码分析：" class="headerlink" title="3. model.py  代码分析："></a><strong>3. model.py  代码分析：</strong></h3><pre><code>import gc
import pandas as pd
import numpy as np
import os
import time
import lightgbm as lgb
from copy import deepcopy
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from sklearn import metrics
from sklearn.metrics import precision_recall_fscore_support
import warnings
from glob import glob
from scipy.sparse import csr_matrix

start_t = time.time()
print(&apos;ww_900_start&apos;)
pd.set_option(&apos;display.max_columns&apos;, 100)
warnings.filterwarnings(&apos;ignore&apos;)

# 添加需要提取的特征
def group_feature(df, key, target, aggs,flag):   
    agg_dict = {}
    for ag in aggs:
        agg_dict[&apos;{}_{}_{}&apos;.format(target,ag,flag)] = ag
    print(agg_dict)
    t = df.groupby(key)[target].agg(agg_dict).reset_index()
    return t

# 计算两个经纬度之间的haversine距离
def haversine_dist(lat1,lng1,lat2,lng2):
    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
    radius = 6371  # Earth&apos;s radius taken from google
    lat = lat2 - lat1
    lng = lng2 - lng1
    d = np.sin(lat/2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng/2) ** 2
    h = 2 * radius * np.arcsin(np.sqrt(d))
    return h

# 提取特征
def extract_feature(df, train, flag):

    if (flag == &apos;on_night&apos;) or (flag == &apos;on_day&apos;): 
        t = group_feature(df, &apos;ship&apos;,&apos;speed&apos;,[&apos;max&apos;,&apos;mean&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
        train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)

    if flag == &quot;0&quot;:
        t = group_feature(df, &apos;ship&apos;,&apos;direction&apos;,[&apos;max&apos;,&apos;median&apos;,&apos;mean&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
        train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;) 

    elif flag == &quot;1&quot;:
        t = group_feature(df, &apos;ship&apos;,&apos;speed&apos;,[&apos;max&apos;,&apos;mean&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
        train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)
        t = group_feature(df, &apos;ship&apos;,&apos;direction&apos;,[&apos;max&apos;,&apos;median&apos;,&apos;mean&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
        train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;) 
        hour_nunique = df.groupby(&apos;ship&apos;)[&apos;speed&apos;].nunique().to_dict()
        train[&apos;speed_nunique_{}&apos;.format(flag)] = train[&apos;ship&apos;].map(hour_nunique)   
        hour_nunique = df.groupby(&apos;ship&apos;)[&apos;direction&apos;].nunique().to_dict()
        train[&apos;direction_nunique_{}&apos;.format(flag)] = train[&apos;ship&apos;].map(hour_nunique)  

    t = group_feature(df, &apos;ship&apos;,&apos;x&apos;,[&apos;max&apos;,&apos;min&apos;,&apos;mean&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
    train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)
    t = group_feature(df, &apos;ship&apos;,&apos;y&apos;,[&apos;max&apos;,&apos;min&apos;,&apos;mean&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
    train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)
    t = group_feature(df, &apos;ship&apos;,&apos;base_dis_diff&apos;,[&apos;max&apos;,&apos;min&apos;,&apos;mean&apos;,&apos;std&apos;,&apos;skew&apos;],flag)
    train = pd.merge(train, t, on=&apos;ship&apos;, how=&apos;left&apos;)

    train[&apos;x_max_x_min_{}&apos;.format(flag)] = train[&apos;x_max_{}&apos;.format(flag)] - train[&apos;x_min_{}&apos;.format(flag)]
    train[&apos;y_max_y_min_{}&apos;.format(flag)] = train[&apos;y_max_{}&apos;.format(flag)] - train[&apos;y_min_{}&apos;.format(flag)]
    train[&apos;y_max_x_min_{}&apos;.format(flag)] = train[&apos;y_max_{}&apos;.format(flag)] - train[&apos;x_min_{}&apos;.format(flag)]
    train[&apos;x_max_y_min_{}&apos;.format(flag)] = train[&apos;x_max_{}&apos;.format(flag)] - train[&apos;y_min_{}&apos;.format(flag)]
    train[&apos;slope_{}&apos;.format(flag)] = train[&apos;y_max_y_min_{}&apos;.format(flag)] / np.where(train[&apos;x_max_x_min_{}&apos;.format(flag)]==0, 0.001, train[&apos;x_max_x_min_{}&apos;.format(flag)])
    train[&apos;area_{}&apos;.format(flag)] = train[&apos;x_max_x_min_{}&apos;.format(flag)] * train[&apos;y_max_y_min_{}&apos;.format(flag)]

    mode_hour = df.groupby(&apos;ship&apos;)[&apos;hour&apos;].agg(lambda x:x.value_counts().index[0]).to_dict()
    train[&apos;mode_hour_{}&apos;.format(flag)] = train[&apos;ship&apos;].map(mode_hour)
    train[&apos;slope_median_{}&apos;.format(flag)] = train[&apos;y_median_{}&apos;.format(flag)] / np.where(train[&apos;x_median_{}&apos;.format(flag)]==0, 0.001, train[&apos;x_median_{}&apos;.format(flag)])

    return train

# 提取数据
def get_data(files, is_sort=True, sort_column=&quot;time&quot;):
    datas = [pd.read_csv(f) for f in files]
    if is_sort:
        dfs = [df.sort_values(by=sort_column, ascending=True, na_position=&apos;last&apos;) for df in datas]
    df = pd.concat(datas, axis=0, ignore_index=True)
    return df

# 处理提取文件数据
def extract_dt(df):
    df[&apos;time&apos;] = pd.to_datetime(df[&apos;time&apos;], format=&apos;%m%d %H:%M:%S&apos;)
    df[&apos;date&apos;] = df[&apos;time&apos;].dt.date
    df[&apos;hour&apos;] = df[&apos;time&apos;].dt.hour

    df[&apos;x_dis_diff&apos;] = (df[&apos;x&apos;] - 6165599).abs()
    df[&apos;y_dis_diff&apos;] = (df[&apos;y&apos;] - 5202660).abs()
    df[&apos;base_dis_diff&apos;] = ((df[&apos;x_dis_diff&apos;]**2)+(df[&apos;y_dis_diff&apos;]**2))**0.5    
    del df[&apos;x_dis_diff&apos;],df[&apos;y_dis_diff&apos;]    

    df[&quot;x&quot;] = df[&quot;x&quot;] / 1e6
    df[&quot;y&quot;] = df[&quot;y&quot;] / 1e6    
    df[&apos;day_nig&apos;] = 0
    df.loc[(df[&apos;hour&apos;] &gt; 5) &amp; (df[&apos;hour&apos;] &lt; 20),&apos;day_nig&apos;] = 1
    return df

train_files = glob(&quot;tcdata/hy_round2_train_20200225/*.csv&quot;)
test_files = glob(&quot;tcdata/hy_round2_testB_20200312/*.csv&quot;)
train_files = sorted(train_files)
test_files = sorted(test_files)

train = get_data(train_files)
train.columns = [&apos;ship&apos;,&apos;x&apos;,&apos;y&apos;,&apos;speed&apos;,&apos;direction&apos;,&apos;time&apos;,&apos;type&apos;]
test = get_data(test_files)
test.columns = [&apos;ship&apos;,&apos;x&apos;,&apos;y&apos;,&apos;speed&apos;,&apos;direction&apos;,&apos;time&apos;]

train = extract_dt(train)
test = extract_dt(test)
train_label = train.drop_duplicates([&apos;ship&apos;],keep = &apos;first&apos;)
test_label = test.drop_duplicates([&apos;ship&apos;],keep = &apos;first&apos;)
train_label[&apos;type&apos;] = train_label[&apos;type&apos;].map({&apos;围网&apos;:0,&apos;刺网&apos;:1,&apos;拖网&apos;:2})

num = train_label.shape[0]
data_label = train_label.append(test_label)
data =train.append(test)
# 将数据分成speed为0和非0、白天和晚上
data_1 = data[data[&apos;speed&apos;]==0]
data_2 = data[data[&apos;speed&apos;]!=0]
data_label = extract_feature(data_1, data_label,&quot;0&quot;)
data_label = extract_feature(data_2, data_label,&quot;1&quot;)

data_1 = data[data[&apos;day_nig&apos;] == 0]
data_2 = data[data[&apos;day_nig&apos;] == 1]
data_label = extract_feature(data_1, data_label,&quot;on_night&quot;)
data_label = extract_feature(data_2, data_label,&quot;on_day&quot;)

# 读取NMF降维后的特征
if os.path.isfile(&apos;nmf_testb.csv&apos;):
    nmf_fea = pd.read_csv(&apos;nmf_testb.csv&apos;)
    data_label = data_label.merge(nmf_fea,on=&apos;ship&apos;,how = &apos;left&apos;)
    del nmf_fea
else:
    for j in range(1,4):
        print(&apos;********* {} *******&apos;.format(j))
        for i in [&apos;speed&apos;,&apos;x&apos;,&apos;y&apos;]:
            data[i + &apos;_str&apos;] = data[i].astype(str)
            from nmf_list import nmf_list
            nmf = nmf_list(data,&apos;ship&apos;,i + &apos;_str&apos;,8,2)
            nmf_a = nmf.run(j)
            data_label = data_label.merge(nmf_a,on = &apos;ship&apos;,how = &apos;left&apos;)


first = &quot;0&quot;
second = &quot;1&quot;
data_label[&apos;direction_median_ratio&apos;] = data_label[&apos;direction_median_{}&apos;.format(second)] / data_label[&apos;direction_median_{}&apos;.format(first)]
data_label[&apos;slope_ratio&apos;] = data_label[&apos;slope_{}&apos;.format(second)] / data_label[&apos;slope_{}&apos;.format(first)] 
data_label[&apos;slope_mean_ratio&apos;] = data_label[&apos;slope_median_{}&apos;.format(second)] / data_label[&apos;slope_median_{}&apos;.format(first)]

first = &quot;on_night&quot;
second = &quot;on_day&quot;
data_label[&apos;speed_median_ratio&apos;] = data_label[&apos;speed_median_{}&apos;.format(second)] / data_label[&apos;speed_median_{}&apos;.format(first)] 
data_label[&apos;speed_std_ratio&apos;] = data_label[&apos;speed_std_{}&apos;.format(second)] / data_label[&apos;speed_std_{}&apos;.format(first)] 

# 计算特征
flag = &apos;all&apos;
for cc in [&apos;direction&apos;,&apos;speed&apos;]:
    t = group_feature(data_label,cc, &apos;ship&apos;,[&apos;count&apos;],flag +cc+ &apos;x&apos;)
    data_label = pd.merge(data_label, t, on=cc, how=&apos;left&apos;)  

for i in [&quot;0&quot;,&quot;1&quot;]:
    if i == &quot;1&quot;:
        for j in [
#                 &apos;slope_speed_cat_nunique_{}&apos;.format(i),
#                   &apos;slope_mean_speed_cat_nunique_{}&apos;.format(i),
                  &apos;speed_nunique_{}&apos;.format(i),
                  &apos;direction_nunique_{}&apos;.format(i)
                 ]:

            t = group_feature(data_label,j, &apos;ship&apos;,[&apos;count&apos;],j+&quot;_count&quot;)
            data_label = pd.merge(data_label, t, on=j, how=&apos;left&apos;) 
    for j in [
           &apos;slope_median_{}&apos;.format(i),
#               &apos;x_max_x_min_{}&apos;.format(i),
#               &apos;y_max_y_min_{}&apos;.format(i)
             ]:
#         t = group_feature(data_label,j, &apos;ship&apos;,[&apos;count&apos;],j+&quot;_count&quot;)
#         data_label = pd.merge(data_label, t, on=j, how=&apos;left&apos;) 
        t = group_feature(data_label,j, &apos;speed&apos;,[&apos;min&apos;,&apos;max&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],j+&quot;_tongji&quot;)
        data_label = pd.merge(data_label, t, on=j, how=&apos;left&apos;)
        # t = group_feature(data_label,j, &apos;direction&apos;,[&apos;min&apos;,&apos;max&apos;,&apos;median&apos;,&apos;std&apos;,&apos;skew&apos;],j+&quot;_tongji&quot;)
        # data_label = pd.merge(data_label, t, on=j, how=&apos;left&apos;)

def cut_bins(raw_data, col_name=None, q=49):
    features, bins = pd.qcut(raw_data[col_name], q=q, retbins=True, duplicates=&quot;drop&quot;)
    labels = list(range(len(bins) - 1))
    features, bins = pd.qcut(raw_data[col_name], labels=labels, q=q, retbins=True, duplicates=&quot;drop&quot;)
    return features, bins, labels


MAX_CATE = 199
data[&quot;x_cate&quot;], x_bins, x_labels = cut_bins(data, col_name=&quot;x&quot;, q=MAX_CATE)
data[&quot;y_cate&quot;], y_bins, y_labels = cut_bins(data, col_name=&quot;y&quot;, q=MAX_CATE)
# data[&quot;x_sub_y_cate&quot;], x_sub_y_bins, x_sub_y_labels = cut_bins(data, col_name=&quot;x_sub_y&quot;, q=MAX_CATE)
data[&quot;distance_cate&quot;], dist_bins, dist_labels = cut_bins(data, col_name=&quot;base_dis_diff&quot;, q=MAX_CATE)

data[&quot;speed_cate&quot;], speed_bins, speed_labels = cut_bins(data, col_name=&quot;speed&quot;, q=MAX_CATE)

MAX_CATE = 120
data[&quot;direct_cate&quot;], speed_bins, speed_labels = cut_bins(data, col_name=&quot;direction&quot;, q=MAX_CATE)

if os.path.isfile(&apos;emb_testb.csv&apos;):
    w2v_fea = pd.read_csv(&apos;emb_testb.csv&apos;)
    data_label = data_label.merge(w2v_fea, on=&apos;ship&apos;, how=&apos;left&apos;)
    del w2v_fea
else:
    from gensim.models import Word2Vec
    import gc
    def emb(df, f1, f2):
        emb_size = 23
        print(&apos;====================================== {} {} ======================================&apos;.format(f1, f2))
        tmp = df.groupby(f1, as_index=False)[f2].agg({&apos;{}_{}_list&apos;.format(f1, f2): list})
        sentences = tmp[&apos;{}_{}_list&apos;.format(f1, f2)].values.tolist()
        del tmp[&apos;{}_{}_list&apos;.format(f1, f2)]
        for i in range(len(sentences)):
            sentences[i] = [str(x) for x in sentences[i]]
        model = Word2Vec(sentences, size=emb_size, window=5, min_count=3, sg=0, hs=1, seed=2222)
        emb_matrix = []
        for seq in sentences:
            vec = []
            for w in seq:
                if w in model:
                    vec.append(model[w])
            if len(vec) &gt; 0:
                emb_matrix.append(np.mean(vec, axis=0))
            else:
                emb_matrix.append([0] * emb_size)
        emb_matrix = np.array(emb_matrix)
        for i in range(emb_size):
            tmp[&apos;{}_{}_emb_{}&apos;.format(f1, f2, i)] = emb_matrix[:, i]
        del model, emb_matrix, sentences
        return tmp


    emb_cols = [
        [&apos;ship&apos;, &apos;x_cate&apos;],
        [&apos;ship&apos;, &apos;y_cate&apos;],
        [&apos;ship&apos;, &apos;speed_cate&apos;],
        [&apos;ship&apos;, &apos;distance_cate&apos;],
        # [&apos;ship&apos;, &apos;direct_cate&apos;],
    ]
    for f1, f2 in emb_cols:
        data_label = data_label.merge(emb(data, f1, f2), on=f1, how=&apos;left&apos;)

    gc.collect()

    # emb_list = [&apos;ship&apos;]
    # for i in data_label.columns:
    #     if &apos;_emb_&apos; in i:
    #         emb_list.append(i)

    # data_label[emb_list].to_csv(&apos;emb_testb.csv&apos;,index=False)


print(&apos;feature done&apos;)

train_label = data_label[:num]
test_label = data_label[num:]
features = [x for x in train_label.columns if x not in [&apos;ship&apos;,&apos;type&apos;,&apos;time&apos;,&apos;x&apos;,&apos;y&apos;,&apos;diff_time&apos;,&apos;date&apos;,&apos;day_nig&apos;,&apos;direction&apos;,&apos;speed&apos;,&apos;hour&apos;,
                                                       &apos;speed_many&apos;,&apos;dire_diff&apos;,&apos;direction_str&apos;,&apos;speed_str&apos;,&apos;dis&apos;,&apos;x_speed&apos;,&apos;y_speed&apos;] ]
target = &apos;type&apos;

# 特征选择
from feature_selector import FeatureSelector
fs = FeatureSelector(data = train_label[features], labels = train_label[target])
fs.identify_zero_importance(task = &apos;classification&apos;, eval_metric = &apos;multiclass&apos;,
                            n_iterations = 10, early_stopping = True)
fs.identify_low_importance(cumulative_importance = 0.97)
low_importance_features = fs.ops[&apos;low_importance&apos;]
print(&apos;====low_importance_features=====&apos;)
print(low_importance_features)
for i in low_importance_features:
features.remove(i)

print(&apos;feature number&apos;,len(features))
gc.collect()

# 评价指标
def macro_f1(y_hat, data):
    y_true = data.get_label()
    y_hat = y_hat.reshape(-1, y_true.shape[0])
    y_hat = np.argmax(y_hat, axis=0)
    f1_multi = precision_recall_fscore_support(y_true, y_hat, labels=[0, 1, 2])[2]
    f1_macro =  f1_score(y_true, y_hat, average =&quot;macro&quot;)
    assert np.mean(f1_multi) ==  f1_macro
    return &apos;f1&apos;, f1_macro, True


def f1_single(y_hat, data, index=0):
    y_true = data.get_label()
    y_hat = y_hat.reshape(-1, y_true.shape[0])
    y_hat = np.argmax(y_hat, axis=0)
    f1_multi = precision_recall_fscore_support(y_true, y_hat, labels=[0, 1, 2])[2]
    f1_s = round(f1_multi[index], 4)
    return &apos;f1_{}&apos;.format(index), f1_s, True

# 构造模型
train_X = train_label[features]
test_X = test_label[features]
print(train_X.shape, test_X.shape)
train_y = train_label[target]


params = {
        &apos;task&apos;:&apos;train&apos;, 
        &apos;num_leaves&apos;: 63,
        &apos;objective&apos;: &apos;multiclass&apos;,
        &apos;num_class&apos;: 3,
        &apos;metric&apos;: &apos;None&apos;, # [f1_0, f1_1, f1_2],
        &apos;min_data_in_leaf&apos;: 10,
        &apos;learning_rate&apos;: 0.01,
        &apos;feature_fraction&apos;: 0.7,
        &apos;bagging_fraction&apos;: 0.95,
        &apos;early_stopping_rounds&apos;: 2000,
#         &apos;lambda_l1&apos;: 0.1,
#         &apos;lambda_l2&apos;: 0.1,
        &quot;first_metric_only&quot;: True,
        &apos;bagging_freq&apos;: 3, 
        &apos;max_bin&apos;: 255,
        &apos;random_state&apos;: 42,
        &apos;verbose&apos; : -1
    }


models = []
test_preds = []
val_preds = []
oof_seed = np.zeros((len(train_label), 3))
seed = [2222,2018778]
for j in seed:
    print(&quot;+++++++++++++++++ seed {} ++++++++++++&quot;.format(str(j)))
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=j)
    oof = np.zeros((len(train_label), 3))
    for i, (trn_idx, val_idx) in enumerate(skf.split(train_X, train_y)):
        print(&quot;-&quot; * 81)
        print(&quot;[!] fold {}&quot;.format(i))
        lgb_params = deepcopy(params)
        # print(lgb_params)
        trn_X = csr_matrix(train_X)[trn_idx]
        trn_y = train_y.iloc[trn_idx]
        val_X = csr_matrix(train_X)[val_idx]
        val_y = train_y.iloc[val_idx]
        dtrain = lgb.Dataset(trn_X, trn_y) 
        dval = lgb.Dataset(val_X, val_y) 
        model = lgb.train(lgb_params, 
               dtrain, 
               num_boost_round=400000,
               valid_sets=[dval], 
               feval=lambda preds, train_data: [
                   macro_f1(preds, train_data),
                   f1_single(preds, train_data, index=0),
                   f1_single(preds, train_data, index=1),
                   f1_single(preds, train_data, index=2)],
               verbose_eval=-1)
        models.append(model)
        # print(model.best_iteration)
        val_pred = model.predict(val_X, iteration=model.best_iteration)
        oof[val_idx] = val_pred
        val_y = train_y.iloc[val_idx]
        val_pred = np.argmax(val_pred, axis=1)
        print(str(i), &apos;val f1&apos;, metrics.f1_score(val_y, val_pred, average=&apos;macro&apos;))
        test_preds.append(model.predict(test_X, iteration=model.best_iteration))
        print(&quot;[!] fold {} finish\n&quot;.format(i))
        del dtrain, dval
        gc.collect()
    val_pred = np.argmax(oof, axis=1)
    print(str(j), &apos;every_flod val f1&apos;, metrics.f1_score(train_y, val_pred, average=&apos;macro&apos;))
    oof_seed += oof/len(seed)

oof1 = np.argmax(oof_seed, axis=1)
print(&apos;oof f1&apos;, metrics.f1_score(oof1,train_y, average=&apos;macro&apos;))
val_score = np.round(metrics.f1_score(oof1, train_y, average=&apos;macro&apos;),6)

def ensemble_predictions(predictions, weights=None, type_=&quot;linear&quot;):
    if not weights:
        print(&quot;[!] AVE_WGT&quot;)
        weights = [1./ len(predictions) for _ in range(len(predictions))]
    assert len(predictions) == len(weights)
    if np.sum(weights) != 1.0:
        weights = [w / np.sum(weights) for w in weights]
    print(&quot;[!] weights = {}&quot;.format(weights))
    assert np.isclose(np.sum(weights), 1.0)
    if type_ == &quot;linear&quot;:
        res = np.average(predictions, weights=weights, axis=0)
    elif type_ == &quot;harmonic&quot;:
        res = np.average([1 / p for p in predictions], weights=weights, axis=0)
        return 1 / res
    elif type_ == &quot;geometric&quot;:
        numerator = np.average(
            [np.log(p) for p in predictions], weights=weights, axis=0
        )
        res = np.exp(numerator / sum(weights))
        return res
    elif type_ == &quot;rank&quot;:
        from scipy.stats import rankdata
        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)
        return res / (len(res) + 1)
    return res

    def merge(prob, number=-1, index=0):
        from copy import deepcopy
        new_prob = deepcopy(prob)
        top = np.argsort(prob[:, index])[::-1][: number]
        print(top[: 4])
        for i in range(len(new_prob)):
            pad_value = np.array([0, 0, 0])
            pad_value[index] = 1
            if i in top:
                new_prob[i, ] = pad_value
            else:
                new_prob[i, index] = 0.
        return new_prob


test_pred_prob = ensemble_predictions(test_preds)
test_pred = test_pred_prob.argmax(axis=1)

test_pro = test_label[[&apos;ship&apos;]]
test_pro[&apos;pro_1&apos;] = test_pred_prob[:,0]
test_pro[&apos;pro_2&apos;] = test_pred_prob[:,1]
test_pro[&apos;pro_3&apos;] = test_pred_prob[:,2]
pred_pro = merge(test_pro[[&apos;pro_1&apos;, &apos;pro_2&apos;, &apos;pro_3&apos;]].values, 900,0)
test_pred = pred_pro.argmax(axis=1)


test_data = test_label[[&apos;ship&apos;]]
test_data[&quot;label&quot;] = test_pred
test_data[&quot;label&quot;] = test_data[&quot;label&quot;].map({0:&apos;围网&apos;,1:&apos;刺网&apos;,2:&apos;拖网&apos;})
# test_data[&apos;label&apos;][:100] = &apos;刺网&apos;
test_data[[&quot;ship&quot;, &quot;label&quot;]].to_csv(&quot;result.csv&quot;, index=False, header=None)
print(test_data[&quot;label&quot;].value_counts())
print(&apos;runtime:&apos;, time.time() - start_t)</code></pre>]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>算法竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title>学习日志：2020华为云大数据挑战赛（正式赛）</title>
    <url>/2020/05/29/hello-world/</url>
    <content><![CDATA[<h2 id="2020-5-29-正式赛题解读"><a href="#2020-5-29-正式赛题解读" class="headerlink" title="2020.5.29 正式赛题解读"></a>2020.5.29 正式赛题解读</h2><h3 id="赛题背景：船运到达时间预测"><a href="#赛题背景：船运到达时间预测" class="headerlink" title="赛题背景：船运到达时间预测"></a>赛题背景：船运到达时间预测</h3><p>通过船运的历史数据构建模型，对目的港到达时间进行预测，预测时间简称为ETA（estimated time of arrival），目的港到达时间预测为ARRIVAL_ETA。</p>
<a id="more"></a>
<p>本次大赛提供<strong>历史运单GPS数据、历史运单事件数据、港口坐标数据</strong>，预测货物运单的到达时间，对应“历史运单事件”数据中EVENT_CODE字段值为ARRIVAL AT PORT时EVENT_CONVOLUTION_DATE的时间值。</p>
<h3 id="比赛数据"><a href="#比赛数据" class="headerlink" title="比赛数据"></a>比赛数据</h3><p><strong>1. GPS数据（建议重点参考）</strong></p>
<table>
<thead>
<tr>
<th align="left">列名</th>
<th align="center">类型</th>
<th align="right">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">loadingOrder</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的主运单，货物的运单编号，类似快递单号</td>
</tr>
<tr>
<td align="left">carrierName</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的承运商名称，类似快递公司名称</td>
</tr>
<tr>
<td align="left">timestamp</td>
<td align="center">DATE</td>
<td align="right">时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-05T16:33:17.000Z</td>
</tr>
<tr>
<td align="left">longitude</td>
<td align="center">NUMBER</td>
<td align="right">货物在运输过程中，当前船舶所处的经度坐标，如114.234567</td>
</tr>
<tr>
<td align="left">latitude</td>
<td align="center">NUMBER</td>
<td align="right">货物在运输过程中，当前船舶所处的纬度坐标，如21.234567</td>
</tr>
<tr>
<td align="left">vesselMMSI</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的船舶海上移动业务识别码MMSI， 唯一标识，对应到每一艘船</td>
</tr>
<tr>
<td align="left">speed</td>
<td align="center">NUMBER</td>
<td align="right">单位km/h，货物在运输过程中，当前船舶的瞬时速度，部分数据未提供的可自行计算。</td>
</tr>
<tr>
<td align="left">direction</td>
<td align="center">NUMBER</td>
<td align="right">当前船舶的行驶方向，正北是0度，31480代表西北方向314.80度，900代表正北偏东9度。</td>
</tr>
<tr>
<td align="left">vesselNextport</td>
<td align="center">VARCHAR2</td>
<td align="right">船舶将要到达的下一港口，港口名称可能不规范，如CNQIN、CN QIN、CN QINGDAO都代表下一站为中国青岛港口。</td>
</tr>
<tr>
<td align="left">vesselNextportETA</td>
<td align="center">DATE</td>
<td align="right">船运公司给出的到“下一个港口”预计到达时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-12T16:33:17.000Z</td>
</tr>
<tr>
<td align="left">vesselStatus</td>
<td align="center">VARCHAR2</td>
<td align="right">当前船舶航行状态，主要包括：moored、under way using engine、not under command、at anchor、under way sailing、constrained by her draught</td>
</tr>
<tr>
<td align="left">TRANSPORT_TRACE</td>
<td align="center">VARCHAR2</td>
<td align="right">船的路由，由“-”连接组成，例如CNSHK-MYPKG-MYTPP。由承运商预先录入，实际小概率存在不按此路由行驶（如遇塞港时），但最终会到达目的港口。</td>
</tr>
</tbody></table>
<p><strong>数据说明：</strong></p>
<p>每个运单表示一次运输的运输单号，不会重复使用，一次运输过程中的多条GPS数据拥有相同的运输单号。船号为运单货物所在的船编号，会重复出现在不同次运输的GPS数据中。需要注意的是GPS数据中可能会有异常的GPS，可能且不限于如下问题：</p>
<p>（1） GPS坐标在陆地，或者有些港口是内陆的港口。</p>
<p>（2） GPS漂移：两点距离过大，超过船的行驶能力。</p>
<p>（3） GPS在部分地区的比较稀疏（比如南半球、敏感海域）。</p>
<p>（4） 最后的GPS点可能和港口的距离较远（比如塞港时，或者临近目的港时已无GPS数据）。</p>
<p>（5） speed字段之后数据可能会有少量缺失（如GPS设备短暂异常）。</p>
<p><strong>注意 :</strong> 字段vesselNextport、vesselNextportETA、TRANSPORT_TRACE为手工输入，误填可能性较大。</p>
<p><strong>2. 历史运单时间数据（手工录入，辅助参考）</strong></p>
<p>历史运单事件数据描述每个运单在船运的过程中，与港口相关的关键信息，如离开起运港、到达目的港等。</p>
<table>
<thead>
<tr>
<th align="left">列名</th>
<th align="center">类型</th>
<th align="right">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">loadingOrder</td>
<td align="center">VARCHAR2</td>
<td align="right">运单号，与历史运单GPS数据中的loadingOrder字段一致</td>
</tr>
<tr>
<td align="left">EVENT_CODE</td>
<td align="center">VARCHAR2</td>
<td align="right">事件编码，主要事件包括：TRANSIT PORT ATD实际离开中转港、SHIPMENT ONBOARD DATE实际离开起运港、TRANSIT PORT ATA实际到达中转港、ARRIVAL AT PORT实际到达目的港、注：部分船可能没有中转港</td>
</tr>
<tr>
<td align="left">EVENT_LOCATION_ID</td>
<td align="center">VARCHAR2</td>
<td align="right">港口名称，对应“港口坐标据”表中的字段TRANS_NODE_NAME</td>
</tr>
<tr>
<td align="left">EVENT_CONVOLUTION_DATE</td>
<td align="center">DATE</td>
<td align="right">事件发生的时间，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格）。例如Event_code为“SHIPMENT ONBOARD DATE”时，此字段表示船从起运港出发的时间。EVENT_CODE为“ARRIVAL AT PORT”时，此字段表示船到达目的港的时间。</td>
</tr>
</tbody></table>
<p><strong>3. 港口坐标数据</strong><br>港口坐标数据描述每个运单在船运的过程中涉及的港口位置信息。</p>
<table>
<thead>
<tr>
<th align="left">列名</th>
<th align="center">类型</th>
<th align="right">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">TRANS_NODE_NAME</td>
<td align="center">VARCHAR2</td>
<td align="right">港口名称，如：WAREHOUSE_TURKEYMOSCOW_RUSSIAN FEDERATION，CHIWAN(44)，SHEKOU，深圳蛇口港等</td>
</tr>
<tr>
<td align="left">LONGITUDE</td>
<td align="center">VARCHAR2</td>
<td align="right">港口的经度坐标</td>
</tr>
<tr>
<td align="left">LATITUDE</td>
<td align="center">VARCHAR2</td>
<td align="right">港口的纬度坐标</td>
</tr>
<tr>
<td align="left">COUNTRY</td>
<td align="center">VARCHAR2</td>
<td align="right">国家</td>
</tr>
<tr>
<td align="left">STATE</td>
<td align="center">VARCHAR2</td>
<td align="right">省、州</td>
</tr>
<tr>
<td align="left">CITY</td>
<td align="center">VARCHAR2</td>
<td align="right">城市</td>
</tr>
<tr>
<td align="left">REGION</td>
<td align="center">VARCHAR2</td>
<td align="right">县、区</td>
</tr>
<tr>
<td align="left">ADDRESS</td>
<td align="center">VARCHAR2</td>
<td align="right">详细地址。</td>
</tr>
<tr>
<td align="left">PORT_CODE</td>
<td align="center">VARCHAR2</td>
<td align="right">港口编码，即港口的字母简码，如CNSHK代表中国蛇口港</td>
</tr>
</tbody></table>
<p><strong>数据说明：</strong><br>（1） 重点为NAME和经纬度数据</p>
<p>（2） 一个港口可能会有多个NAME表示，且不按五位编码表示</p>
<p>（3） 经纬度如果出现负数则为错误信息，可以删除或自行补充</p>
<p><strong>4. 测试运单数据</strong></p>
<p>测试运单数据为运单运输过程中的不同位置点所构成，供选手测试对应的ETA时间。测试运单数据如下表描述。</p>
<table>
<thead>
<tr>
<th align="left">列名</th>
<th align="center">类型</th>
<th align="right">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">loadingOrder</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的主运单，货物的运单编号，类似快递单号</td>
</tr>
<tr>
<td align="left">timestamp</td>
<td align="center">DATE</td>
<td align="right">时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-05T16:33:17.000Z</td>
</tr>
<tr>
<td align="left">longitude</td>
<td align="center">NUMBER</td>
<td align="right">货物在运输过程中，当前船舶所处的经度坐标，如114.234567</td>
</tr>
<tr>
<td align="left">latitude</td>
<td align="center">NUMBER</td>
<td align="right">货物在运输过程中，当前船舶所处的纬度坐标，如21.234567</td>
</tr>
<tr>
<td align="left">speed</td>
<td align="center">NUMBER</td>
<td align="right">单位km/h，货物在运输过程中，当前船舶的瞬时速度，部分数据未提供的可自行计算。</td>
</tr>
<tr>
<td align="left">direction</td>
<td align="center">NUMBER</td>
<td align="right">当前船舶的行驶方向，正北是0度，31480代表西北方向314.80度，900代表正北偏东9度。</td>
</tr>
<tr>
<td align="left">carrierName</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的承运商名称，类似快递公司名称</td>
</tr>
<tr>
<td align="left">vesselMMSI</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的船舶海上移动业务识别码MMSI， 唯一标识，对应到每一艘船</td>
</tr>
<tr>
<td align="left">onboardDate</td>
<td align="center">DATE</td>
<td align="right">离开起运港时间，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格），如2019/09/05 16:33:17</td>
</tr>
<tr>
<td align="left">TRANSPORT_TRACE</td>
<td align="center">VARCHAR2</td>
<td align="right">船的路由，由“-”连接组成，例如CNSHK-MYPKG-MYTPP。由承运商预先录入，实际小概率存在不按此路由行驶（如遇塞港时），但最终会到达目的港口。</td>
</tr>
</tbody></table>
<p><strong>数据说明：</strong><br>（1） 根据航程提供起始一段形成GPS数据，用于预测到达目的港时间（按不同航线，提供10%~50%左右的GPS数据）</p>
<p><strong>5. 提交数据</strong></p>
<p>所有参与竞赛的选手登录到大赛平台，提交结果数据，具体提交格式要求：</p>
<table>
<thead>
<tr>
<th align="left">列名</th>
<th align="center">类型</th>
<th align="right">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">loadingOrder</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的主运单，货物的运单编号，类似快递单号</td>
</tr>
<tr>
<td align="left">timestamp</td>
<td align="center">DATE</td>
<td align="right">时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-05T16:33:17.000Z</td>
</tr>
<tr>
<td align="left">longitude</td>
<td align="center">NUMBER</td>
<td align="right">货物在运输过程中，当前船舶所处的经度坐标，如114.234567</td>
</tr>
<tr>
<td align="left">latitude</td>
<td align="center">NUMBER</td>
<td align="right">货物在运输过程中，当前船舶所处的纬度坐标，如21.234567</td>
</tr>
<tr>
<td align="left">carrierName</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的承运商名称，类似快递公司名称</td>
</tr>
<tr>
<td align="left">vesselMMSI</td>
<td align="center">VARCHAR2</td>
<td align="right">脱敏后的船舶海上移动业务识别码MMSI， 唯一标识，对应到每一艘船</td>
</tr>
<tr>
<td align="left">onboardDate</td>
<td align="center">DATE</td>
<td align="right">离开起运港时间，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格），如2019/09/05 16:33:17</td>
</tr>
<tr>
<td align="left">ETA</td>
<td align="center">DATE</td>
<td align="right">到达目的港口的ETA，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格），如2019/09/18 22:28:46</td>
</tr>
<tr>
<td align="left">creatDate</td>
<td align="center">DATE</td>
<td align="right">当前表创建时间，格式为：yyyy/MM/dd HH:mm:ss（dd与HH之间为两个空格），如2020/05/05 16:33:17</td>
</tr>
</tbody></table>
<p><strong>数据说明：</strong><br>（1） 前7列基于测试数据，ETA列为预测时间</p>
<p>（2） creatDate为文件生成时间，判分不关注。</p>
<h2 id="2020-6-2-baseline分析"><a href="#2020-6-2-baseline分析" class="headerlink" title="2020.6.2 baseline分析"></a>2020.6.2 baseline分析</h2><p>分析正式赛提供的baseline </p>
<p><strong>MSE : 176495.2554</strong></p>
<h3 id="1-导入所需要的包"><a href="#1-导入所需要的包" class="headerlink" title="1. 导入所需要的包"></a>1. 导入所需要的包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"># from tqdm import tqdm</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from sklearn.metrics import mean_squared_error,explained_variance_score</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">import lightgbm as lgb</span><br><span class="line"></span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&#39;ignore&#39;)</span><br></pre></td></tr></table></figure>

<h3 id="2-加载数据"><a href="#2-加载数据" class="headerlink" title="2. 加载数据"></a>2. 加载数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># baseline只用到gps定位数据，即train_gps_path</span><br><span class="line">train_gps_path &#x3D; &#39;dataset&#x2F;train0523.csv&#39;</span><br><span class="line">test_data_path &#x3D; &#39;test&#x2F;A_testData0531.csv&#39;</span><br><span class="line">order_data_path &#x3D; &#39;dataset&#x2F;loadingOrderEvent.csv&#39;</span><br><span class="line">port_data_path &#x3D; &#39;dataset&#x2F;port.csv&#39;</span><br><span class="line"></span><br><span class="line"># 取前1000000行</span><br><span class="line">debug &#x3D; True</span><br><span class="line">NDATA &#x3D; 1000000</span><br><span class="line"></span><br><span class="line">if debug:</span><br><span class="line">    train_data &#x3D; pd.read_csv(train_gps_path,nrows&#x3D;NDATA,header&#x3D;None)</span><br><span class="line">else:</span><br><span class="line">    train_data &#x3D; pd.read_csv(train_gps_path,header&#x3D;None)</span><br><span class="line"></span><br><span class="line">train_data.columns &#x3D; [&#39;loadingOrder&#39;,&#39;carrierName&#39;,&#39;timestamp&#39;,&#39;longitude&#39;,</span><br><span class="line">                  &#39;latitude&#39;,&#39;vesselMMSI&#39;,&#39;speed&#39;,&#39;direction&#39;,&#39;vesselNextport&#39;,</span><br><span class="line">                  &#39;vesselNextportETA&#39;,&#39;vesselStatus&#39;,&#39;vesselDatasource&#39;,&#39;TRANSPORT_TRACE&#39;]</span><br><span class="line">test_data &#x3D; pd.read_csv(test_data_path)</span><br><span class="line"></span><br><span class="line">def get_data(data, mode&#x3D;&#39;train&#39;):</span><br><span class="line"></span><br><span class="line">	assert mode&#x3D;&#x3D;&#39;train&#39; or mode&#x3D;&#x3D;&#39;test&#39;</span><br><span class="line">	</span><br><span class="line">	if mode&#x3D;&#x3D;&#39;train&#39;:</span><br><span class="line">	    data[&#39;vesselNextportETA&#39;] &#x3D; pd.to_datetime(data[&#39;vesselNextportETA&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">	elif mode&#x3D;&#x3D;&#39;test&#39;:</span><br><span class="line">	    data[&#39;temp_timestamp&#39;] &#x3D; data[&#39;timestamp&#39;]</span><br><span class="line">	    data[&#39;onboardDate&#39;] &#x3D; pd.to_datetime(data[&#39;onboardDate&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">	data[&#39;timestamp&#39;] &#x3D; pd.to_datetime(data[&#39;timestamp&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">	data[&#39;longitude&#39;] &#x3D; data[&#39;longitude&#39;].astype(float)</span><br><span class="line">	data[&#39;loadingOrder&#39;] &#x3D; data[&#39;loadingOrder&#39;].astype(str)</span><br><span class="line">	data[&#39;latitude&#39;] &#x3D; data[&#39;latitude&#39;].astype(float)</span><br><span class="line">	data[&#39;speed&#39;] &#x3D; data[&#39;speed&#39;].astype(float)</span><br><span class="line">	data[&#39;direction&#39;] &#x3D; data[&#39;direction&#39;].astype(float)</span><br><span class="line">	</span><br><span class="line">	return data</span><br><span class="line"></span><br><span class="line">train_data &#x3D; get_data(train_data, mode&#x3D;&#39;train&#39;)</span><br><span class="line">test_data &#x3D; get_data(test_data, mode&#x3D;&#39;test&#39;)</span><br></pre></td></tr></table></figure>

<p>由于训练集过于庞大，因此baseline只选取了前100万条数据，因此，我们应该可以通过清洗数据来选取合适的训练数据来降低MSE。</p>
<h3 id="3-获取特征，目前baseline的特征只选择经纬度、速度-方向。我们需要进一步可视化数据集，构造更加合理的特征。"><a href="#3-获取特征，目前baseline的特征只选择经纬度、速度-方向。我们需要进一步可视化数据集，构造更加合理的特征。" class="headerlink" title="3. 获取特征，目前baseline的特征只选择经纬度、速度\方向。我们需要进一步可视化数据集，构造更加合理的特征。"></a>3. 获取特征，目前baseline的特征只选择经纬度、速度\方向。我们需要进一步可视化数据集，构造更加合理的特征。</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 代码参考：https:&#x2F;&#x2F;github.com&#x2F;juzstu&#x2F;TianChi_HaiYang</span><br><span class="line">def get_feature(df, mode&#x3D;&#39;train&#39;):</span><br><span class="line">    </span><br><span class="line">    assert mode&#x3D;&#x3D;&#39;train&#39; or mode&#x3D;&#x3D;&#39;test&#39;</span><br><span class="line">    </span><br><span class="line">    df.sort_values([&#39;loadingOrder&#39;, &#39;timestamp&#39;], inplace&#x3D;True)</span><br><span class="line">    # 特征只选择经纬度、速度\方向</span><br><span class="line">    df[&#39;lat_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;latitude&#39;].diff(1)</span><br><span class="line">    df[&#39;lon_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;longitude&#39;].diff(1)</span><br><span class="line">    df[&#39;speed_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;speed&#39;].diff(1)</span><br><span class="line">    df[&#39;diff_minutes&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].diff(1).dt.total_seconds() &#x2F;&#x2F; 60</span><br><span class="line">    df[&#39;anchor&#39;] &#x3D; df.apply(lambda x: 1 if x[&#39;lat_diff&#39;] &lt;&#x3D; 0.03 and x[&#39;lon_diff&#39;] &lt;&#x3D; 0.03</span><br><span class="line">                            and x[&#39;speed_diff&#39;] &lt;&#x3D; 0.3 and x[&#39;diff_minutes&#39;] &lt;&#x3D; 10 else 0, axis&#x3D;1)</span><br><span class="line">    </span><br><span class="line">    if mode&#x3D;&#x3D;&#39;train&#39;:</span><br><span class="line">        group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(mmax&#x3D;&#39;max&#39;, count&#x3D;&#39;count&#39;, mmin&#x3D;&#39;min&#39;).reset_index()</span><br><span class="line">        # 读取数据的最大值-最小值，即确认时间间隔为label</span><br><span class="line">        group_df[&#39;label&#39;] &#x3D; (group_df[&#39;mmax&#39;] - group_df[&#39;mmin&#39;]).dt.total_seconds()</span><br><span class="line">    elif mode&#x3D;&#x3D;&#39;test&#39;:</span><br><span class="line">        group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(count&#x3D;&#39;count&#39;).reset_index()</span><br><span class="line">        </span><br><span class="line">    anchor_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;anchor&#39;].agg(&#39;sum&#39;).reset_index()</span><br><span class="line">    anchor_df.columns &#x3D; [&#39;loadingOrder&#39;, &#39;anchor_cnt&#39;]</span><br><span class="line">    group_df &#x3D; group_df.merge(anchor_df, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line">    group_df[&#39;anchor_ratio&#39;] &#x3D; group_df[&#39;anchor_cnt&#39;] &#x2F; group_df[&#39;count&#39;]</span><br><span class="line"></span><br><span class="line">    agg_function &#x3D; [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;median&#39;]</span><br><span class="line">    agg_col &#x3D; [&#39;latitude&#39;, &#39;longitude&#39;, &#39;speed&#39;, &#39;direction&#39;]</span><br><span class="line"></span><br><span class="line">    group &#x3D; df.groupby(&#39;loadingOrder&#39;)[agg_col].agg(agg_function).reset_index()</span><br><span class="line">    group.columns &#x3D; [&#39;loadingOrder&#39;] + [&#39;&#123;&#125;_&#123;&#125;&#39;.format(i, j) for i in agg_col for j in agg_function]</span><br><span class="line">    group_df &#x3D; group_df.merge(group, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line"></span><br><span class="line">    return group_df</span><br><span class="line">    </span><br><span class="line">train &#x3D; get_feature(train_data, mode&#x3D;&#39;train&#39;)</span><br><span class="line">test &#x3D; get_feature(test_data, mode&#x3D;&#39;test&#39;)</span><br><span class="line">features &#x3D; [c for c in train.columns if c not in [&#39;loadingOrder&#39;, &#39;label&#39;, &#39;mmin&#39;, &#39;mmax&#39;, &#39;count&#39;]]</span><br></pre></td></tr></table></figure>

<p>这部分代码可能由于pandas版本问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(mmax&#x3D;&#39;max&#39;, count&#x3D;&#39;count&#39;, mmin&#x3D;&#39;min&#39;).reset_index()</span><br></pre></td></tr></table></figure>

<p>需要修改为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(&#39;mmax&#39;:&#39;max&#39;, &#39;count&#39;:&#39;count&#39;, &#39;mmin&#39;:&#39;min&#39;).reset_index()</span><br></pre></td></tr></table></figure>

<h3 id="4-使用十折交叉验证构造模型"><a href="#4-使用十折交叉验证构造模型" class="headerlink" title="4. 使用十折交叉验证构造模型"></a>4. 使用十折交叉验证构造模型</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def mse_score_eval(preds, valid):</span><br><span class="line">    labels &#x3D; valid.get_label()</span><br><span class="line">    scores &#x3D; mean_squared_error(y_true&#x3D;labels, y_pred&#x3D;preds)</span><br><span class="line">    return &#39;mse_score&#39;, scores, True</span><br><span class="line"></span><br><span class="line">def build_model(train, test, pred, label, seed&#x3D;1080, is_shuffle&#x3D;True):</span><br><span class="line">    train_pred &#x3D; np.zeros((train.shape[0], ))</span><br><span class="line">    test_pred &#x3D; np.zeros((test.shape[0], ))</span><br><span class="line">    n_splits &#x3D; 10</span><br><span class="line">    # Kfold</span><br><span class="line">    fold &#x3D; KFold(n_splits&#x3D;n_splits, shuffle&#x3D;is_shuffle, random_state&#x3D;seed)</span><br><span class="line">    kf_way &#x3D; fold.split(train[pred])</span><br><span class="line">    # params</span><br><span class="line">    params &#x3D; &#123;</span><br><span class="line">        &#39;learning_rate&#39;: 0.01,</span><br><span class="line">        &#39;boosting_type&#39;: &#39;gbdt&#39;,</span><br><span class="line">        &#39;objective&#39;: &#39;regression&#39;,</span><br><span class="line">        &#39;num_leaves&#39;: 36,</span><br><span class="line">        &#39;feature_fraction&#39;: 0.6,</span><br><span class="line">        &#39;bagging_fraction&#39;: 0.7,</span><br><span class="line">        &#39;bagging_freq&#39;: 6,</span><br><span class="line">        &#39;seed&#39;: 8,</span><br><span class="line">        &#39;bagging_seed&#39;: 1,</span><br><span class="line">        &#39;feature_fraction_seed&#39;: 7,</span><br><span class="line">        &#39;min_data_in_leaf&#39;: 20,</span><br><span class="line">        &#39;nthread&#39;: 8,</span><br><span class="line">        &#39;verbose&#39;: 1,</span><br><span class="line">    &#125;</span><br><span class="line">    # train</span><br><span class="line">    for n_fold, (train_idx, valid_idx) in enumerate(kf_way, start&#x3D;1):</span><br><span class="line">        train_x, train_y &#x3D; train[pred].iloc[train_idx], train[label].iloc[train_idx]</span><br><span class="line">        valid_x, valid_y &#x3D; train[pred].iloc[valid_idx], train[label].iloc[valid_idx]</span><br><span class="line">        # 数据加载</span><br><span class="line">        n_train &#x3D; lgb.Dataset(train_x, label&#x3D;train_y)</span><br><span class="line">        n_valid &#x3D; lgb.Dataset(valid_x, label&#x3D;valid_y)</span><br><span class="line"></span><br><span class="line">        clf &#x3D; lgb.train(</span><br><span class="line">            params&#x3D;params,</span><br><span class="line">            train_set&#x3D;n_train,</span><br><span class="line">            num_boost_round&#x3D;3000,</span><br><span class="line">            valid_sets&#x3D;[n_valid],</span><br><span class="line">            early_stopping_rounds&#x3D;100,</span><br><span class="line">            verbose_eval&#x3D;100,</span><br><span class="line">            feval&#x3D;mse_score_eval</span><br><span class="line">        )</span><br><span class="line">        train_pred[valid_idx] &#x3D; clf.predict(valid_x, num_iteration&#x3D;clf.best_iteration)</span><br><span class="line">        test_pred +&#x3D; clf.predict(test[pred], num_iteration&#x3D;clf.best_iteration)&#x2F;fold.n_splits</span><br><span class="line">    </span><br><span class="line">    test[&#39;label&#39;] &#x3D; test_pred</span><br><span class="line">    </span><br><span class="line">    return test[[&#39;loadingOrder&#39;, &#39;label&#39;]]</span><br><span class="line"></span><br><span class="line">result &#x3D; build_model(train, test, features, &#39;label&#39;, is_shuffle&#x3D;True)</span><br></pre></td></tr></table></figure>
<h3 id="5-生成最终的测试文件"><a href="#5-生成最终的测试文件" class="headerlink" title="5. 生成最终的测试文件"></a>5. 生成最终的测试文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">test_data &#x3D; test_data.merge(result, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line">test_data[&#39;ETA&#39;] &#x3D; (test_data[&#39;onboardDate&#39;] + test_data[&#39;label&#39;].apply(lambda x:pd.Timedelta(seconds&#x3D;x))).apply(lambda x:x.strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;))</span><br><span class="line">test_data.drop([&#39;direction&#39;,&#39;TRANSPORT_TRACE&#39;],axis&#x3D;1,inplace&#x3D;True)</span><br><span class="line">test_data[&#39;onboardDate&#39;] &#x3D; test_data[&#39;onboardDate&#39;].apply(lambda x:x.strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;))</span><br><span class="line">test_data[&#39;creatDate&#39;] &#x3D; pd.datetime.now().strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;)</span><br><span class="line">test_data[&#39;timestamp&#39;] &#x3D; test_data[&#39;temp_timestamp&#39;]</span><br><span class="line"># 整理columns顺序</span><br><span class="line">result &#x3D; test_data[[&#39;loadingOrder&#39;, &#39;timestamp&#39;, &#39;longitude&#39;, &#39;latitude&#39;, &#39;carrierName&#39;, &#39;vesselMMSI&#39;, &#39;onboardDate&#39;, &#39;ETA&#39;, &#39;creatDate&#39;]]</span><br><span class="line"></span><br><span class="line">result.to_csv(&#39;result.csv&#39;, index&#x3D;False)</span><br></pre></td></tr></table></figure>


<h2 id="2020-6-4-构建自己的方案"><a href="#2020-6-4-构建自己的方案" class="headerlink" title="2020.6.4 构建自己的方案"></a>2020.6.4 构建自己的方案</h2><h3 id="1-首先对测试数据进行分析，观察测试数据海域"><a href="#1-首先对测试数据进行分析，观察测试数据海域" class="headerlink" title="1 首先对测试数据进行分析，观察测试数据海域"></a>1 首先对测试数据进行分析，观察测试数据海域</h3><p>观察测试数据的经纬度坐标</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/3DDC4958791846399E78159CE41D9B88?method=download&shareKey=5f64e5f51a764a54adc51abe88526fb7" alt></p>
<p>可以看出测试航线的行驶方向几乎覆盖了全球，也可以看到有少量的GPS数据发生了漂移</p>
<p>首先我们先对数据进行清洗</p>
<ul>
<li><p>删除字段中存在null的数据</p>
</li>
<li><p>提取训练数据中与测试数据航线路由一致的数据<br>测试数据航线路由：<br>[‘CNYTN-MXZLO’ ‘CNSHK-MYTPP’ ‘CNSHK-SGSIN’ ‘CNSHK-CLVAP’ ‘CNYTN-ARENA’<br>‘CNYTN-MATNG’ ‘CNSHK-GRPIR’ ‘CNSHK-PKQCT’ ‘COBUN-HKHKG’ ‘CNYTN-PAONX’<br>‘CNSHK-SIKOP’ ‘CNYTN-CAVAN’ ‘CNSHK-ESALG’ ‘CNYTN-MTMLA’ ‘CNSHK-ZADUR’<br>‘CNSHK-LBBEY’ ‘CNSHA-SGSIN’ ‘CNYTN-RTM’ ‘CNHKG-MXZLO’ ‘HKHKG-FRFOS’<br>‘CNYTN-NZAKL’ ‘CNSHA-PAMIT’]</p>
</li>
</ul>
<ul>
<li><p>处理所有的数据，保存筛选出来的训练数据</p>
<p>（周星星第三名提供一种思路：用无监督学习聚类与测试样本相似轨迹的训练数据）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"># from tqdm import tqdm</span><br><span class="line">import numpy as np</span><br><span class="line"># 释放内存</span><br><span class="line">import gc</span><br><span class="line">from sklearn.metrics import mean_squared_error,explained_variance_score</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">import lightgbm as lgb</span><br><span class="line">from pandas.core.frame import DataFrame</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&#39;ignore&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># baseline只用到gps定位数据，即train_gps_path</span><br><span class="line">train_gps_path &#x3D; &#39;data&#x2F;train0523.csv&#39;</span><br><span class="line">test_data_path &#x3D; &#39;data&#x2F;A_testData0531.csv&#39;</span><br><span class="line">order_data_path &#x3D; &#39;data&#x2F;loadingOrderEvent.csv&#39;</span><br><span class="line">port_data_path &#x3D; &#39;data&#x2F;port.csv&#39;</span><br><span class="line"></span><br><span class="line">def get_data(data, mode&#x3D;&#39;train&#39;):</span><br><span class="line">    </span><br><span class="line">    assert mode&#x3D;&#x3D;&#39;train&#39; or mode&#x3D;&#x3D;&#39;test&#39;</span><br><span class="line">    </span><br><span class="line">    if mode&#x3D;&#x3D;&#39;train&#39;:</span><br><span class="line">        data[&#39;vesselNextportETA&#39;] &#x3D; pd.to_datetime(data[&#39;vesselNextportETA&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">    elif mode&#x3D;&#x3D;&#39;test&#39;:</span><br><span class="line">        data[&#39;temp_timestamp&#39;] &#x3D; data[&#39;timestamp&#39;]</span><br><span class="line">        data[&#39;onboardDate&#39;] &#x3D; pd.to_datetime(data[&#39;onboardDate&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">    data[&#39;timestamp&#39;] &#x3D; pd.to_datetime(data[&#39;timestamp&#39;], infer_datetime_format&#x3D;True)</span><br><span class="line">    data[&#39;longitude&#39;] &#x3D; data[&#39;longitude&#39;].astype(float)</span><br><span class="line">    data[&#39;loadingOrder&#39;] &#x3D; data[&#39;loadingOrder&#39;].astype(str)</span><br><span class="line">    data[&#39;latitude&#39;] &#x3D; data[&#39;latitude&#39;].astype(float)</span><br><span class="line">    data[&#39;speed&#39;] &#x3D; data[&#39;speed&#39;].astype(float)</span><br><span class="line">    data[&#39;direction&#39;] &#x3D; data[&#39;direction&#39;].astype(float)</span><br><span class="line"></span><br><span class="line">    return dat</span><br><span class="line">def process(train_data):</span><br><span class="line">    train_data &#x3D; get_data(train_data, mode&#x3D;&#39;train&#39;)</span><br><span class="line">    train_data &#x3D; train_data.dropna(axis&#x3D;0, how&#x3D;&#39;any&#39;)</span><br><span class="line">    train_data &#x3D; train_data.loc[train_data[&#39;TRANSPORT_TRACE&#39;].isin([&#39;CNYTN-MXZLO&#39;,&#39;CNSHK-MYTPP&#39;,&#39;CNSHK-SGSIN&#39;,&#39;CNSHK-CLVAP&#39;,&#39;CNYTN-ARENA&#39;,</span><br><span class="line">                                                                     &#39;CNYTN-MATNG&#39;,&#39;CNSHK-GRPIR&#39;,&#39;CNSHK-PKQCT&#39;,&#39;COBUN-HKHKG&#39;,&#39;CNYTN-PAONX&#39;,</span><br><span class="line">                                                                     &#39;CNSHK-SIKOP&#39;,&#39;CNYTN-CAVAN&#39;,&#39;CNSHK-ESALG&#39;,&#39;CNYTN-MTMLA&#39;,&#39;CNSHK-ZADUR&#39;,</span><br><span class="line">                                                                     &#39;CNSHK-LBBEY&#39;,&#39;CNSHA-SGSIN&#39;,&#39;CNYTN-RTM&#39;,&#39;CNHKG-MXZLO&#39;,&#39;HKHKG-FRFOS&#39;,</span><br><span class="line">                                                                     &#39;CNYTN-NZAKL&#39;,&#39;CNSHA-PAMIT&#39;])]</span><br><span class="line">    return train_data</span><br><span class="line"></span><br><span class="line">def get_train_data()</span><br><span class="line">    mylist&#x3D;[]</span><br><span class="line">    temp&#x3D;[]</span><br><span class="line">    i&#x3D;0 </span><br><span class="line">    for chunk in pd.read_csv(train_gps_path,chunksize&#x3D;1000000):</span><br><span class="line">        mylist&#x3D;chunk</span><br><span class="line">        mylist.columns &#x3D; [&#39;loadingOrder&#39;,&#39;carrierName&#39;,&#39;timestamp&#39;,&#39;longitude&#39;,</span><br><span class="line">                          &#39;latitude&#39;,&#39;vesselMMSI&#39;,&#39;speed&#39;,&#39;direction&#39;,&#39;vesselNextport&#39;,</span><br><span class="line">                          &#39;vesselNextportETA&#39;,&#39;vesselStatus&#39;,&#39;vesselDatasource&#39;,&#39;TRANSPORT_TRACE&#39;]</span><br><span class="line">        mylist&#x3D;process(mylist)</span><br><span class="line">        temp.append(mylist)</span><br><span class="line"></span><br><span class="line">    temp_df &#x3D; pd.concat(temp,axis&#x3D;0,ignore_index&#x3D;True)</span><br><span class="line">    temp_df.to_csv(&#39;data&#x2F;used&#x2F;train.csv&#39;)</span><br></pre></td></tr></table></figure>
<p>然后我们用筛选出的数据来分析</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_path &#x3D; &#39;data&#x2F;used&#x2F;train.csv&#39;</span><br><span class="line">train_data &#x3D; pd.read_csv(train_path,index_col&#x3D;0)</span><br><span class="line">train_data &#x3D; get_data(train_data, mode&#x3D;&#39;train&#39;)</span><br><span class="line"></span><br><span class="line">test_data &#x3D; pd.read_csv(test_data_path)</span><br><span class="line">test_data &#x3D; get_data(test_data, mode&#x3D;&#39;test&#39;)</span><br><span class="line"></span><br><span class="line"># 代码参考：https:&#x2F;&#x2F;github.com&#x2F;juzstu&#x2F;TianChi_HaiYang</span><br><span class="line">def get_feature(df, mode&#x3D;&#39;train&#39;):</span><br><span class="line">    </span><br><span class="line">    assert mode&#x3D;&#x3D;&#39;train&#39; or mode&#x3D;&#x3D;&#39;test&#39;</span><br><span class="line">    </span><br><span class="line">    df.sort_values([&#39;loadingOrder&#39;, &#39;timestamp&#39;], inplace&#x3D;True)</span><br><span class="line">    # 特征只选择经纬度、速度\方向</span><br><span class="line">    df[&#39;lat_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;latitude&#39;].diff(1)</span><br><span class="line">    df[&#39;lon_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;longitude&#39;].diff(1)</span><br><span class="line">    df[&#39;speed_diff&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;speed&#39;].diff(1)</span><br><span class="line">    df[&#39;diff_minutes&#39;] &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].diff(1).dt.total_seconds() &#x2F;&#x2F; 60</span><br><span class="line">    df[&#39;anchor&#39;] &#x3D; df.apply(lambda x: 1 if x[&#39;lat_diff&#39;] &lt;&#x3D; 0.03 and x[&#39;lon_diff&#39;] &lt;&#x3D; 0.03</span><br><span class="line">                            and x[&#39;speed_diff&#39;] &lt;&#x3D; 0.3 and x[&#39;diff_minutes&#39;] &lt;&#x3D; 10 else 0, axis&#x3D;1)</span><br><span class="line">    </span><br><span class="line">    if mode&#x3D;&#x3D;&#39;train&#39;:</span><br><span class="line">        group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(&#123;&#39;mmax&#39;:&#39;max&#39;, &#39;count&#39;:&#39;count&#39;, &#39;mmin&#39;:&#39;min&#39;&#125;).reset_index()</span><br><span class="line">        # 读取数据的最大值-最小值，即确认时间间隔为label</span><br><span class="line">        group_df[&#39;label&#39;] &#x3D; (group_df[&#39;mmax&#39;] - group_df[&#39;mmin&#39;]).dt.total_seconds()</span><br><span class="line">    elif mode&#x3D;&#x3D;&#39;test&#39;:</span><br><span class="line">        group_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;timestamp&#39;].agg(&#123;&#39;count&#39;:&#39;count&#39;&#125;).reset_index()</span><br><span class="line">        </span><br><span class="line">    anchor_df &#x3D; df.groupby(&#39;loadingOrder&#39;)[&#39;anchor&#39;].agg(&#39;sum&#39;).reset_index()</span><br><span class="line">    anchor_df.columns &#x3D; [&#39;loadingOrder&#39;, &#39;anchor_cnt&#39;]</span><br><span class="line">    group_df &#x3D; group_df.merge(anchor_df, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line">    group_df[&#39;anchor_ratio&#39;] &#x3D; group_df[&#39;anchor_cnt&#39;] &#x2F; group_df[&#39;count&#39;]</span><br><span class="line"></span><br><span class="line">    agg_function &#x3D; [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;median&#39;] 。 </span><br><span class="line">    agg_col &#x3D; [&#39;latitude&#39;, &#39;longitude&#39;, &#39;speed&#39;, &#39;direction&#39;]</span><br><span class="line"></span><br><span class="line">    group &#x3D; df.groupby(&#39;loadingOrder&#39;)[agg_col].agg(agg_function).reset_index()</span><br><span class="line">    group.columns &#x3D; [&#39;loadingOrder&#39;] + [&#39;&#123;&#125;_&#123;&#125;&#39;.format(i, j) for i in agg_col for j in agg_function]</span><br><span class="line">    group_df &#x3D; group_df.merge(group, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line"></span><br><span class="line">    return group_df</span><br><span class="line">    </span><br><span class="line">train &#x3D; get_feature(train_data, mode&#x3D;&#39;train&#39;)</span><br><span class="line">test &#x3D; get_feature(test_data, mode&#x3D;&#39;test&#39;)</span><br><span class="line">features &#x3D; [c for c in train.columns if c not in [&#39;loadingOrder&#39;, &#39;label&#39;, &#39;mmin&#39;, &#39;mmax&#39;, &#39;count&#39;]]</span><br><span class="line"></span><br><span class="line">def mse_score_eval(preds, valid):</span><br><span class="line">    labels &#x3D; valid.get_label()</span><br><span class="line">    scores &#x3D; mean_squared_error(y_true&#x3D;labels, y_pred&#x3D;preds)</span><br><span class="line">    return &#39;mse_score&#39;, scores, True</span><br><span class="line"></span><br><span class="line">def build_model(train, test, pred, label, seed&#x3D;1080, is_shuffle&#x3D;True):</span><br><span class="line">    train_pred &#x3D; np.zeros((train.shape[0], ))</span><br><span class="line">    test_pred &#x3D; np.zeros((test.shape[0], ))</span><br><span class="line">    n_splits &#x3D; 10</span><br><span class="line">    # Kfold</span><br><span class="line">    fold &#x3D; KFold(n_splits&#x3D;n_splits, shuffle&#x3D;is_shuffle, random_state&#x3D;seed)</span><br><span class="line">    kf_way &#x3D; fold.split(train[pred])</span><br><span class="line">    # params</span><br><span class="line">    params &#x3D; &#123;</span><br><span class="line">        &#39;learning_rate&#39;: 0.01,</span><br><span class="line">        &#39;boosting_type&#39;: &#39;gbdt&#39;,</span><br><span class="line">        &#39;objective&#39;: &#39;regression&#39;,</span><br><span class="line">        &#39;num_leaves&#39;: 36,</span><br><span class="line">        &#39;feature_fraction&#39;: 0.6,</span><br><span class="line">        &#39;bagging_fraction&#39;: 0.7,</span><br><span class="line">        &#39;bagging_freq&#39;: 6,</span><br><span class="line">        &#39;seed&#39;: 8,</span><br><span class="line">        &#39;bagging_seed&#39;: 1,</span><br><span class="line">        &#39;feature_fraction_seed&#39;: 7,</span><br><span class="line">        &#39;min_data_in_leaf&#39;: 20,</span><br><span class="line">        &#39;nthread&#39;: 8,</span><br><span class="line">        &#39;verbose&#39;: 1,</span><br><span class="line">    &#125;</span><br><span class="line">    # train</span><br><span class="line">    for n_fold, (train_idx, valid_idx) in enumerate(kf_way, start&#x3D;1):</span><br><span class="line">        train_x, train_y &#x3D; train[pred].iloc[train_idx], train[label].iloc[train_idx]</span><br><span class="line">        valid_x, valid_y &#x3D; train[pred].iloc[valid_idx], train[label].iloc[valid_idx]</span><br><span class="line">        # 数据加载</span><br><span class="line">        n_train &#x3D; lgb.Dataset(train_x, label&#x3D;train_y)</span><br><span class="line">        n_valid &#x3D; lgb.Dataset(valid_x, label&#x3D;valid_y)</span><br><span class="line"></span><br><span class="line">        clf &#x3D; lgb.train(</span><br><span class="line">            params&#x3D;params,</span><br><span class="line">            train_set&#x3D;n_train,</span><br><span class="line">            num_boost_round&#x3D;3000,</span><br><span class="line">            valid_sets&#x3D;[n_valid],</span><br><span class="line">            early_stopping_rounds&#x3D;100,</span><br><span class="line">            verbose_eval&#x3D;100,</span><br><span class="line">            feval&#x3D;mse_score_eval</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        train_pred[valid_idx] &#x3D; clf.predict(valid_x, num_iteration&#x3D;clf.best_iteration)</span><br><span class="line">        test_pred +&#x3D; clf.predict(test[pred], num_iteration&#x3D;clf.best_iteration)&#x2F;fold.n_splits</span><br><span class="line">    </span><br><span class="line">    test[&#39;label&#39;] &#x3D; test_pred</span><br><span class="line">    </span><br><span class="line">    return test[[&#39;loadingOrder&#39;, &#39;label&#39;]]</span><br><span class="line"></span><br><span class="line">result &#x3D; build_model(train, test, features, &#39;label&#39;, is_shuffle&#x3D;True)</span><br><span class="line"></span><br><span class="line">test_data &#x3D; test_data.merge(result, on&#x3D;&#39;loadingOrder&#39;, how&#x3D;&#39;left&#39;)</span><br><span class="line">test_data[&#39;ETA&#39;] &#x3D; (test_data[&#39;onboardDate&#39;] + test_data[&#39;label&#39;].apply(lambda x:pd.Timedelta(seconds&#x3D;x))).apply(lambda x:x.strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;))</span><br><span class="line">test_data.drop([&#39;direction&#39;,&#39;TRANSPORT_TRACE&#39;],axis&#x3D;1,inplace&#x3D;True)</span><br><span class="line">test_data[&#39;onboardDate&#39;] &#x3D; test_data[&#39;onboardDate&#39;].apply(lambda x:x.strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;))</span><br><span class="line">test_data[&#39;creatDate&#39;] &#x3D; pd.datetime.now().strftime(&#39;%Y&#x2F;%m&#x2F;%d  %H:%M:%S&#39;)</span><br><span class="line">test_data[&#39;timestamp&#39;] &#x3D; test_data[&#39;temp_timestamp&#39;]</span><br><span class="line"># 整理columns顺序</span><br><span class="line">result &#x3D; test_data[[&#39;loadingOrder&#39;, &#39;timestamp&#39;, &#39;longitude&#39;, &#39;latitude&#39;, &#39;carrierName&#39;, &#39;vesselMMSI&#39;, &#39;onboardDate&#39;, &#39;ETA&#39;, &#39;creatDate&#39;]]</span><br><span class="line"></span><br><span class="line">result.to_csv(&#39;result.csv&#39;, index&#x3D;False)</span><br></pre></td></tr></table></figure>
<p>经过初步数据筛选后，得出评分MSE：46448.0950</p>
</li>
</ul>
<p>相较于Baseline有较大提升</p>
<h3 id="2-特征工程"><a href="#2-特征工程" class="headerlink" title="2. 特征工程"></a>2. 特征工程</h3><p>在清洗完数据后，我们应该结合数据挑选出相应特征来表征数据，特征的选取十分重要，直接影响到了最后模型的预测结果</p>
<h3 id="2-1-baseline里所使用的特征"><a href="#2-1-baseline里所使用的特征" class="headerlink" title="2.1 baseline里所使用的特征"></a>2.1 baseline里所使用的特征</h3><ol>
<li>anchor_cnt : 所有航次的锚点数，停船数</li>
<li>anchor_ratio : 停船率</li>
<li>latitude_min </li>
<li>latitude_max</li>
<li>latitude_mean</li>
<li>latitude_median</li>
<li>longitude_min</li>
<li>longitude_max</li>
<li>longitude_mean</li>
<li>longitude_median</li>
<li>speed_min</li>
<li>speed_max</li>
<li>speed_mean</li>
<li>speed_median</li>
<li>direction_min</li>
<li>direction_max</li>
<li>direction_mean</li>
<li>direction_median</li>
</ol>
<p>basline利用了基本特征的各种统计量</p>
<h3 id="2-2-合理构建新特征"><a href="#2-2-合理构建新特征" class="headerlink" title="2.2 合理构建新特征"></a>2.2 合理构建新特征</h3><p><strong>根据2020智慧海洋top5方案，添加相应特征：</strong></p>
<ol>
<li><p>在统计量中添加峰度和偏度</p>
</li>
<li><p>两个经纬之间的haversine距离和其相关统计量</p>
</li>
</ol>
<p><strong>根据周星星第二名</strong></p>
<ol>
<li>添加加速度及相关统计量</li>
</ol>
<p><strong>根据2020智慧海洋top1方案</strong></p>
<h3 id="1-基于轨迹序列和相对位置的符合向量编码"><a href="#1-基于轨迹序列和相对位置的符合向量编码" class="headerlink" title="1. 基于轨迹序列和相对位置的符合向量编码"></a>1. 基于轨迹序列和相对位置的符合向量编码</h3><p><img src="https://note.youdao.com/yws/api/personal/file/830EA0D9A5A744ACA6E5C939C18354AC?method=download&shareKey=1651238ae54291e492f2e568d8e3e179" alt="%E7%BC%96%E7%A0%81.jpg"></p>
<p>因为经纬度常规的统计特征对船舶轨迹的表征能力其实是有限的，其设计了一种轨道序列的编码方式，进一步刻画了轨迹的动态信息和船舶经过的每个点之间的联系。</p>
<p><strong>输入层</strong><br>渔船的轨迹序列</p>
<p><strong>数据处理层</strong><br><strong>1. Geohash7编码：</strong>Geohash其实是将地图拆分成了一个个矩形网络，当经纬度落入到某个网格内时，则使用网格的编码代替经纬度，相当于是一种聚类方式。这里我们不直接使用经纬度（细粒度）的好处就是可以提高特征的泛化能力。<br><img src="https://note.youdao.com/yws/api/personal/file/D10C2F58138443F9A336388638489DBE?method=download&shareKey=6b7bda804562a28ea5b906df4f9fa092" alt="geohash.jpg"></p>
<p>除此之外，还采用了全集和下采样两种提取方式来提取Geohash7编码后的轨迹序列，下采样的好处是缓解船舶位置信息频繁上报而产生的噪音。</p>
<p><strong>2. 梯度编码：</strong>轨迹序列的梯度是指后一个位置相对于前一个位置的变化，目的是为了获取相对位置信息</p>
<p>$$f_i^{(1)}=\frac{f(x_{i+1})-f(x_{x_i-1})}{2h}+O(h^2)$$</p>
<p><strong>特征编码层</strong></p>
<p><strong>1. Word2Vec：将船舶的轨迹序列当成文本，每个地点则是一个“词语”，使用Word2Vec进行向量表征</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def w2v_feat(df, group_id, feat, length):</span><br><span class="line">    print(&#39;start word2vec ...&#39;)</span><br><span class="line">    data_frame &#x3D; df.groupby(group_id)[feat].agg(list).reset_index()</span><br><span class="line">    model &#x3D; Word2Vec(data_frame[feat].values, size&#x3D;length, window&#x3D;5, min_count&#x3D;1, sg&#x3D;1, hs&#x3D;1,</span><br><span class="line">                     workers&#x3D;1, iter&#x3D;10, seed&#x3D;1, hashfxn&#x3D;hashfxn)</span><br><span class="line">    data_frame[feat] &#x3D; data_frame[feat].apply(lambda x: pd.DataFrame([model[c] for c in x]))</span><br><span class="line">    for m in range(length):</span><br><span class="line">        data_frame[&#39;w2v_&#123;&#125;_mean&#39;.format(m)] &#x3D; data_frame[feat].apply(lambda x: x[m].mean())</span><br><span class="line">    del data_frame[feat]</span><br><span class="line">    return data_frame</span><br></pre></td></tr></table></figure>
<p><strong>2. Node2Vec：</strong>把渔船经过的地点当成图中的“点”，而不同地点之间的关系则是“边”，使用Node2Vec进行向量表征。</p>
<p><strong>3. TFIDF&amp;CountVec：</strong>借鉴了文本的常用处理方法，提取了渔船轨迹序列中Geohash的频次信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def tfidf(input_values, output_num, output_prefix, seed&#x3D;1024):</span><br><span class="line">    tfidf_enc &#x3D; TfidfVectorizer()</span><br><span class="line">    tfidf_vec &#x3D; tfidf_enc.fit_transform(input_values)</span><br><span class="line">    svd_enc &#x3D; TruncatedSVD(n_components&#x3D;output_num, n_iter&#x3D;20, random_state&#x3D;seed)</span><br><span class="line">    svd_tmp &#x3D; svd_enc.fit_transform(tfidf_vec)</span><br><span class="line">    svd_tmp &#x3D; pd.DataFrame(svd_tmp)</span><br><span class="line">    svd_tmp.columns &#x3D; [&#39;&#123;&#125;_tfidf_&#123;&#125;&#39;.format(output_prefix, i) for i in range(output_num)]</span><br><span class="line">    return svd_tmp</span><br></pre></td></tr></table></figure>

<p><strong>输出层：</strong></p>
<p>轨迹序列Embedding向量</p>
<p>（top1和top5方案均采用了编码、降维方法，可以借鉴）</p>
<h3 id="2-Turning状态特征"><a href="#2-Turning状态特征" class="headerlink" title="2. Turning状态特征"></a>2. Turning状态特征</h3><p>将数值的方向特征进行离散化处理后，统计船舶航行过程中不同方向的频次及比例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def turning(data):</span><br><span class="line">    data[&#39;方向&#39;] &#x3D; data[&#39;方向&#39;].apply(lambda x:(int(round(x&#x2F;30)))*30)</span><br><span class="line">    degree_df &#x3D; data.pivot_table(index&#x3D;&#39;渔船ID&#39;,columns&#x3D;&#39;方向&#39;,values&#x3D;&#39;lat&#39;, dropna&#x3D;False, aggfunc&#x3D;&#39;count&#39;).fillna(0)</span><br><span class="line">    degree_df.columns &#x3D; [str(f)+&#39;_方向_count&#39; for f in degree_df.columns]</span><br><span class="line">    degree_df.reset_index(inplace&#x3D;True)</span><br><span class="line">    return degree_df</span><br></pre></td></tr></table></figure>

<h3 id="3-速度的分位数特征"><a href="#3-速度的分位数特征" class="headerlink" title="3. 速度的分位数特征"></a>3. 速度的分位数特征</h3><p>船舶数据中速度存在大量为0的记录，可能会对均值方差等统计量产生较大的影响，是否单独取速度大于0的样本对速度构造统计特征，有待进一步的实验</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def spped(data):</span><br><span class="line">    group_df &#x3D; data[data[&#39;速度&#39;]&gt;0].groupby([&#39;渔船ID&#39;])[&#39;速度&#39;].agg(&#123;</span><br><span class="line">        &#39;速度_mean_new&#39;: &#39;mean&#39;,</span><br><span class="line">        &#39;速度_q10_new&#39;: lambda x: np.quantile(x, 0.10),</span><br><span class="line">        &#39;速度_q20_new&#39;: lambda x: np.quantile(x, 0.20),</span><br><span class="line">        &#39;速度_q30_new&#39;: lambda x: np.quantile(x, 0.30),</span><br><span class="line">        &#39;速度_q40_new&#39;: lambda x: np.quantile(x, 0.40),</span><br><span class="line">        &#39;速度_q50_new&#39;: lambda x: np.quantile(x, 0.50),</span><br><span class="line">        &#39;速度_q60_new&#39;: lambda x: np.quantile(x, 0.60),</span><br><span class="line">        &#39;速度_q70_new&#39;: lambda x: np.quantile(x, 0.70),</span><br><span class="line">        &#39;速度_q80_new&#39;: lambda x: np.quantile(x, 0.80),</span><br><span class="line">        &#39;速度_q90_new&#39;: lambda x: np.quantile(x, 0.90),</span><br><span class="line">    &#125;).reset_index()</span><br><span class="line">    return group_df</span><br></pre></td></tr></table></figure>
<p><strong>根据参考文献</strong></p>
<ol>
<li>航速变化率和航向变化率</li>
</ol>
<p><strong>其它想法</strong></p>
<ol>
<li>航线编码： 因为不同航线的轨迹变化较大，对航线进行离散化处理并热编码，添加航线特征</li>
</ol>
<h3 id="2-3-特征筛选"><a href="#2-3-特征筛选" class="headerlink" title="2.3 特征筛选"></a>2.3 特征筛选</h3><p>根据上述构建并添加相应特征，并在众多特征中，利用特征重要性和相关性来筛选出对预测结果影响较大的特征，构建模型。</p>
<h3 id="1-基于学习模型的特征排序（Model-base-ranking）"><a href="#1-基于学习模型的特征排序（Model-base-ranking）" class="headerlink" title="1 基于学习模型的特征排序（Model base ranking）"></a>1 基于学习模型的特征排序（Model base ranking）</h3><p>这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。</p>
<p>因此我们尝试使用lgbm来生成特征的重要性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line">X_train, X_test, y_train, y_test &#x3D; train_test_split(train[features], train[&#39;label&#39;], test_size&#x3D;0.2, random_state&#x3D;42)</span><br><span class="line">print(&quot;X_train shape is: &quot; + str(X_train.shape))</span><br><span class="line">print(&quot;X_test shape is: &quot; + str(X_test.shape))</span><br><span class="line"></span><br><span class="line">gbm &#x3D; lgb.LGBMRegressor(objective&#x3D;&#39;regression&#39;,</span><br><span class="line">                            max_depth&#x3D;6, </span><br><span class="line">                            learning_rate&#x3D;0.1, </span><br><span class="line">                            n_estimators&#x3D;150, </span><br><span class="line">                            min_child_weight&#x3D;4)</span><br><span class="line">lgb_model &#x3D; gbm.fit(X_train, y_train)</span><br><span class="line">y_predict &#x3D; gbm.predict(X_test)</span><br><span class="line">mse &#x3D; mean_squared_error(y_test, y_predict)</span><br><span class="line">print(&quot;MSE: %.4f&quot; % mse)</span><br><span class="line"></span><br><span class="line">lgb_predictors &#x3D; [i for i in train[features].columns]</span><br><span class="line">lgb_feat_imp &#x3D; pd.Series(lgb_model.feature_importances_, lgb_predictors).sort_values(ascending&#x3D;False)</span><br><span class="line">lgb_feat_imp.to_csv(&#39;data&#x2F;features&#x2F;lgb_feat_imp.csv&#39;)</span><br></pre></td></tr></table></figure>
<p>生成特征重要性表后，我们尝试使用前30个特征</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">features &#x3D; pd.read_csv(&#39;data&#x2F;features&#x2F;lgb_feat_imp.csv&#39;, header&#x3D;None)</span><br><span class="line">features &#x3D; features.iloc[0:30,0]</span><br><span class="line">features &#x3D; features.values.tolist()</span><br><span class="line"></span><br><span class="line">train_path &#x3D; &#39;data&#x2F;used&#x2F;train.csv&#39;</span><br><span class="line">test_data_path &#x3D; &#39;data&#x2F;A_testData0531.csv&#39;</span><br><span class="line"></span><br><span class="line">train_data &#x3D; pd.read_csv(train_path, index_col&#x3D;0)</span><br><span class="line">train_data &#x3D; get_data(train_data, mode&#x3D;&#39;train&#39;)</span><br><span class="line"></span><br><span class="line">test_data &#x3D; pd.read_csv(test_data_path)</span><br><span class="line">test_data &#x3D; get_data(test_data, mode&#x3D;&#39;test&#39;)</span><br></pre></td></tr></table></figure>

<p>至此，对于赛题的完整分析流程已经结束，对于之前选取的训练集对训练集的拟合效果大致维持在这个水平上</p>
<p>想要进一步较大的提升精度，有以下优化改进</p>
<h3 id="1-进一步观察数据集，清洗出对测试集拟合效果更好的训练数据"><a href="#1-进一步观察数据集，清洗出对测试集拟合效果更好的训练数据" class="headerlink" title="1. 进一步观察数据集，清洗出对测试集拟合效果更好的训练数据"></a>1. 进一步观察数据集，清洗出对测试集拟合效果更好的训练数据</h3><p>根据之前删除任意字段存在空值的数据，并筛选出对应航线后，只剩下718个不同航次的训练数据，这显然有点太少了，因此需要将筛选数据条件放宽限一点</p>
<h3 id="2-优化训练集标签Label值的计算方式"><a href="#2-优化训练集标签Label值的计算方式" class="headerlink" title="2. 优化训练集标签Label值的计算方式"></a>2. 优化训练集标签Label值的计算方式</h3><p>baseline在标定label时，直接使用的是船舶AIS信息中时间戳的最大值-最小值，这种计算方式可能不太合理，有可能船舶缺失到港数据，又或者船舶到港时间停留太久，都会造成label的不准确，因此我们需要进一步优化计算label的方法</p>
]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>算法竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记（一）决策树</title>
    <url>/2020/07/15/tree/</url>
    <content><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="1-决策树原理"><a href="#1-决策树原理" class="headerlink" title="1 决策树原理"></a>1 决策树原理</h2><h3 id="1-1-决策树是如何工作的"><a href="#1-1-决策树是如何工作的" class="headerlink" title="1.1 决策树是如何工作的"></a>1.1 决策树是如何工作的</h3><p>决策树（Decision Tree）是一种非常参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。决策树算法容易理解，适用各种数据，在解决各种问题时都有良好表现，尤其是以树模型为核心的各种集成算法，在各个行业和领域都有广泛的应用。</p>
<a id="more"></a>

<p><strong>决策树算法的核心是要解决两个问题：</strong><br>1）如何从数据表中找出最佳结点和最佳分枝<br>2）如何让决策树停止生长，防止过拟合？</p>
<p>几乎所有决策树有关的模型调整方法，都围绕这两个问题展开。</p>
<h3 id="1-2-构建决策树"><a href="#1-2-构建决策树" class="headerlink" title="1.2 构建决策树"></a>1.2 构建决策树</h3><p>原则上讲，任意一个数据集上的所有特征都可以被拿来分枝，特征上的任意结点又可以自由组合，所以一个数据集上可以发展处非常多测决策树，其数量可达指数级。但在这些树中，总有那么一棵树比其他的树分类效力都好，那样的树叫做“全局最优树”。</p>
<table>
<thead>
<tr>
<th align="left">关键概念：全局最优，局部最优</th>
</tr>
</thead>
<tbody><tr>
<td align="left">全局最优：经过组合形成的，整体来说分类效果最好的模型</td>
</tr>
<tr>
<td align="left">局部最优：每一次分枝的时候都向着更好的分类效果分枝，但无法确认如此生成的树在全局上是否是最优的</td>
</tr>
</tbody></table>
<p>要在这么多决策树中去一次性找到分类效果最佳的那一棵是不可能的，如果通过排列组合来进行筛选，计算量过于大而低效。<br>因此，机器学习研究者们开发了一些有效的算法，能够在合理的时间内构造出具有一定准确率的次最优决策树。这些算法基本都执行“贪心策略”，即通过局部最优来达到接近全局最优的结果</p>
<table>
<thead>
<tr>
<th align="left">关键概念：贪心算法</th>
</tr>
</thead>
<tbody><tr>
<td align="left">通过实现局部最优来达到接近全局最优结果的算法，所有的树模型都是这样的算法</td>
</tr>
</tbody></table>
<h3 id="1-2-1-ID3算法构建决策树"><a href="#1-2-1-ID3算法构建决策树" class="headerlink" title="1.2.1 ID3算法构建决策树"></a>1.2.1 ID3算法构建决策树</h3><p>ID3算法原型为J.R Quinlan的博士论文，是基础理论较为完善，使用较为广泛的决策树模型，并在此基础上优化退出了C4.5和C5.0决策树算法，后两者已经成为最流行的决策树算法。</p>
<p>决策树需要找出最佳节点和最佳的分枝方法，而衡量这个“最佳”的指标叫做“不纯度”。<br>不纯度基于叶子节点来计算的，所以树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的。</p>
<table>
<thead>
<tr>
<th align="left">关键概念：不纯度</th>
</tr>
</thead>
<tbody><tr>
<td align="left">如果有某一类标签占有较大的比例，我们就说叶子节点“纯”，分枝分得好。若各类标签都很平均，则说明叶子节点“不纯”</td>
</tr>
</tbody></table>
<h4 id="怎样计算不纯度？"><a href="#怎样计算不纯度？" class="headerlink" title="怎样计算不纯度？"></a>怎样计算不纯度？</h4><p>对于节点不纯度的计算和表示方法因决策树模型而异，但不管不纯度的度量方法如何，都是由误差率衍生而来，其计算公式如下：<br>$$Classification error(t) = 1-\max_{i=1}[p(i|t)]$$</p>
<p>误差率越低，则纯度越高。由此还衍生出了其他两个常用指标，一个是ID3重的Information gain（信息增益）的计算方法可用Entropy推导，即最为人熟知的信息熵，又叫香农熵，其计算公式如下：<br>$$Entropy(t)=-\sum^{c-1}_{i=0}p(i|t)\log_2p(i|t)$$</p>
<p>其中c表示叶子节点上标签类别的个数，c-1表示标签的索引</p>
<p><strong>从第0类标签开始计算，设定$\log_20=0$</strong></p>
<p>另一个指标则是<strong>Gini（基尼）指数</strong>，主要用于CART决策树的纯度判定中，其计算公式如下：<br>$$Gini = 1-\sum_{i=0}^{c-1}[p(i|t)]^2$$</p>
<p>决策树最终的优化目标是使得叶节点的总不纯度最低。因此ID3决策树在觉得是否对某节点进性切分的时候，会尽可能选取使得该节点对应的子节点信息熵最小的特征进行切分，换而言之，就是要求父节点信息熵和子节点总信息熵之差要最大，对ID3而言，二者之差就是信息增益。</p>
<p><strong>一个父节点下可能有多个子节点，所以信息增益为父节点信息熵-所有子节点信息熵的加权平均</strong></p>
<p>$$I(child) = \sum_{j=1}^k\frac{N(v_j)}{N}I(v_j)$$</p>
<p>而父节点和子节点的不纯度下降数可由下属公式进行计算：<br>$$\Delta = I(parent)-I(child)$$</p>
<p>$I(.)$是给定节点的不纯性度量，$N$是父节点上的样本树，$k$是这一层上子节点的个数，$N(v_j)$是与子节点$v_j$相关联的子样本个数</p>
<p>决策树算法会选择最大化增益的条件，因为对任何分枝过程来说，$I(parent)$都是一个不变的值，所以最大化增益等价于最小化子节点的不纯性衡量的加权平均。</p>
<h3 id="1-2-3-ID3的局限性"><a href="#1-2-3-ID3的局限性" class="headerlink" title="1.2.3 ID3的局限性"></a>1.2.3 ID3的局限性</h3><p>ID3局限性主要源于局部最优化条件，即信息增益的计算方法，其局限性主要有以下几点：</p>
<ul>
<li>分支度越高（分类水平越多）离散变量往往子节点的总信息熵会更小，ID3是按照某一列进行切分，在极限情况下取ID作为切分字段，每个分类的纯度都是100%，因此这样的分类方式是没有效益的</li>
<li>不能直接处理连续型变量，若使用ID3处理连续型变量，则首先需要对连续型变量进行离散化</li>
<li>对缺失值比较敏感，使用ID3之前需要提前对缺失值进行处理</li>
<li>没有剪枝的设置，容易导致过拟合，即在训练集上表现很好，测试集上表现很差</li>
</ul>
<h3 id="1-3-C4-5算法-amp-CART算法"><a href="#1-3-C4-5算法-amp-CART算法" class="headerlink" title="1.3 C4.5算法 &amp; CART算法"></a>1.3 C4.5算法 &amp; CART算法</h3><h3 id="1-3-1-修改局部最优化条件"><a href="#1-3-1-修改局部最优化条件" class="headerlink" title="1.3.1 修改局部最优化条件"></a>1.3.1 修改局部最优化条件</h3><p>在C4.5中，首先通过引入分支度（IV:Information Value）概念，对信息增益的计算方法进行修正。</p>
<p>将信息熵计算公式中的$p(i|t)$（<strong>即某类别样例占总样例数</strong>）改成了$P(v_i)$，即<strong>某子节点的总样本数占父节点总样本数的比例</strong></p>
<p>$$Information Value = -\sum_{i=1}^kP(v_i)\log_2P(v_i)$$</p>
<p>其中，$i$表示父节点的第$i$个子节点，$v_i$表示第$i$个子节点样例数，$P(v_i)$表示第$i$个子节点拥有样例数占父节点总样例数的比例。</p>
<p>最终，在C4.5中，使用之前的信息增益除以分支度作为选取切分字段的参考指标，该指标被称作Gain Ratio(增益率)，计算公式如下：<br>$$Gain Ratio = \frac{Information Gain}{Information Value}$$</p>
<p>本质是信息增益最大，分支度又比较小的列（也就是纯度提升很快，但又不是靠着把类别分特别细来提升的那些特征）</p>
<h3 id="1-3-2-连续变量处理手段"><a href="#1-3-2-连续变量处理手段" class="headerlink" title="1.3.2 连续变量处理手段"></a>1.3.2 连续变量处理手段</h3><p>在C4.5中，同样还增加了针对连续变量的处理手段。如果输入特征字段是连续型变量，则有下列步骤：</p>
<ol>
<li>算法首先会对这一列数进行从小到大排序</li>
<li>选取相邻的两个数的中间数作为切分数据集的备选点，若一个连续变量有N个值，则在C4.5的处理过程中将产生N-1个备选切点，并且每个切分点都代表着一种二叉树的切分方案</li>
</ol>
<p>因此在对于包含连续变量的数据集进行树模型构建的过程中要消耗更多的运算资源。但与此同时，我们也会发现，当连续变量的某中间点参与到决策树的二分过程中，往往代表该店对于最终分类结果有较大影响，这也为我们连续变量的分箱压缩提供了指导性的意见。也是最重要的模型指导分箱方法。</p>
<p>CART树本质其实和C4.5区别不大，只不过CART树所有的层都是二叉树</p>
<ol>
<li>首先特征从小到大一次排列</li>
<li>计算两两相邻的均值</li>
<li>按均值所在的点，对连续型变量进行二分，二分得到的点交做决策树的“树桩”</li>
<li>找每种二分切分方案的获益比例，获益比例最大的切分点，就是切点</li>
<li>切完之后，计算加权信息熵，计算信息增益，引入分支度，计算增益比例</li>
</ol>
<hr>
<h2 id="2-sklearn中的决策树"><a href="#2-sklearn中的决策树" class="headerlink" title="2 sklearn中的决策树"></a>2 sklearn中的决策树</h2><ul>
<li>模块sklearn.tree</li>
</ul>
<p>模块总共包含五个类：</p>
<table>
<thead>
<tr>
<th align="left">tree.DecisionTreeClassifier</th>
<th align="left">分类树</th>
</tr>
</thead>
<tbody><tr>
<td align="left">tree.DecisionTreeRegressor</td>
<td align="left">回归树</td>
</tr>
<tr>
<td align="left">tree.export_graphviz</td>
<td align="left">将生成的决策树导出为DOT格式</td>
</tr>
<tr>
<td align="left">tree.ExtraTreeClassifier</td>
<td align="left">高随机版本的分类树</td>
</tr>
<tr>
<td align="left">tree.ExtraTreeRegressor</td>
<td align="left">高随机版本的回归树</td>
</tr>
</tbody></table>
<h3 id="2-1-重要参数"><a href="#2-1-重要参数" class="headerlink" title="2.1 重要参数"></a>2.1 重要参数</h3><h3 id="2-2-1-criterion"><a href="#2-2-1-criterion" class="headerlink" title="2.2.1 criterion"></a>2.2.1 criterion</h3><p>这个参数用来决定不纯度的计算方法</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">criterion</th>
</tr>
</thead>
<tbody><tr>
<td align="left">如何影响模型?</td>
<td align="left">确定不纯度的计算方法，帮忙找出最佳节点和最佳分枝，不纯度越低，决策树对训练集 的拟合越好</td>
</tr>
<tr>
<td align="left">可能的输入有哪些？</td>
<td align="left">不填默认基尼系数，填写gini使用基尼系数，填写entropy使用信息增益</td>
</tr>
<tr>
<td align="left">怎样选取参数？</td>
<td align="left">通常使用基尼系数</td>
</tr>
</tbody></table>
<h3 id="2-2-2-random-state-amp-splitter"><a href="#2-2-2-random-state-amp-splitter" class="headerlink" title="2.2.2 random_state &amp; splitter"></a>2.2.2 random_state &amp; splitter</h3><ul>
<li><p>random_state 用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。</p>
</li>
<li><p>splitter 也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合。</p>
</li>
</ul>
<h3 id="2-2-3-剪枝参数"><a href="#2-2-3-剪枝参数" class="headerlink" title="2.2.3 剪枝参数"></a>2.2.3 剪枝参数</h3><ul>
<li><p>max_depth 限制树的最大深度，超过设定深度的树枝全部剪掉这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所  以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效  果再决定是否增加设定深度。</p>
</li>
<li><p>min_samples_leaf 限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分  枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很 大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题  中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。</p>
</li>
<li><p>min_samples_split 限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则  分枝就不会发生。</p>
</li>
<li><p>max_features 限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工， max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。</p>
</li>
<li><p>min_impurity_decrease 限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的    功能，在0.19版本之前时使用min_impurity_split。</p>
</li>
</ul>
<h3 id="2-2-4-目标权重参数"><a href="#2-2-4-目标权重参数" class="headerlink" title="2.2.4 目标权重参数"></a>2.2.4 目标权重参数</h3><ul>
<li><p>class_weight 完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给  与数据集中的所有标签相同的权重。</p>
</li>
<li><p>min_weight_fraction_leaf 有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_ weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_ fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。</p>
</li>
</ul>
<h2 id="3-决策树的优缺点"><a href="#3-决策树的优缺点" class="headerlink" title="3 决策树的优缺点"></a>3 决策树的优缺点</h2><h3 id="3-1-决策树优点"><a href="#3-1-决策树优点" class="headerlink" title="3.1 决策树优点"></a>3.1 决策树优点</h3><ol>
<li>易于理解和解释，因为树木可以画出来被看见</li>
<li>需要很少的数据准备。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等。但请注意，sklearn中的决策树模块不支持对缺失值的处理。</li>
<li>使用树的成本（比如说，在预测数据的时候）是用于训练树的数据点的数量的对数，相比于其他算法，这是一个很低的成本。</li>
<li>能够同时处理数字和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类型的数据集。</li>
<li>能够处理多输出问题，即含有多个标签的问题，注意与一个标签中含有多种标签分类的问题区别开</li>
<li>是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松解释条件。相反，在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。</li>
<li>可以使用统计测试验证模型，这让我们可以考虑模型的可靠性。</li>
<li>即使其假设在某种程度上违反了生成数据的真实模型，也能够表现良好。</li>
</ol>
<h3 id="3-2-决策树的缺点"><a href="#3-2-决策树的缺点" class="headerlink" title="3.2 决策树的缺点"></a>3.2 决策树的缺点</h3><ol>
<li>决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据。这称为过度拟合。修剪，设置叶节点所需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说会比较晦涩</li>
<li>决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树，这个问题需要通过集成算法来解决。</li>
<li>决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法不能保证返回全局最优决策树。这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过程中被随机采样。</li>
<li>有些概念很难学习，因为决策树不容易表达它们，例如XOR，奇偶校验或多路复用器问题。</li>
<li>如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。因此，建议在拟合决策树之前平衡数据集。</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记（二）支持向量机（SVM）</title>
    <url>/2020/07/16/svm/</url>
    <content><![CDATA[<h1 id="1-支持向量机（SVM）原理"><a href="#1-支持向量机（SVM）原理" class="headerlink" title="1 支持向量机（SVM）原理"></a>1 支持向量机（SVM）原理</h1><p>从算法的功能来看，SVM几乎涵盖了机器学习的所有算法需求</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">功能</th>
</tr>
</thead>
<tbody><tr>
<td align="left">有监督学习</td>
<td align="left">线性二分类与多分类（Linear Support Vector Classiﬁcation）</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">非线性二分类与多分类（Support Vector Classiﬁcation, SVC）</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">普通连续型变量的回归（Support Vector Regression）</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">概率型连续变量的回归（Bayesian SVM）</td>
</tr>
<tr>
<td align="left">无监督学习</td>
<td align="left">支持向量聚类（Support Vector Clustering，SVC）</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">异常值检测（One-class SVM）</td>
</tr>
<tr>
<td align="left">半监督学习</td>
<td align="left">转导支持向量机（Transductive Support Vector Machines，TSVM）</td>
</tr>
</tbody></table>
<a id="more"></a>
<h2 id="1-1-线性SVM用于分类的原理"><a href="#1-1-线性SVM用于分类的原理" class="headerlink" title="1.1 线性SVM用于分类的原理"></a>1.1 线性SVM用于分类的原理</h2><p>要理解SVM的损失函数，先定义决策边界，假设数据中共计有$N$个训练样本，每个训练样本$i$可以被表示为$(x_i,y_i)(i=1,2,\dots N)$，其中$x_i$是$(x_{1i},x_{2i}\dots x_{ni})^T$这样一个特征向量，每个样本总共包含$n$个特征。</p>
<p>二分类标签$y_i$取值为${-1,1}$</p>
<p>若$n=2$，则有$i=(x_{1i},x_{2i},y_i)^T$，在二维平面上，以$x_2$为横坐标，$x_1$为从坐标，$y$为颜色，来可视化所有的$N$个样本</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/A4A7E90B88454F098F8BDBDCCF02B27D?method=download&shareKey=a5c90f8c069094d544bacfbb471dbd02" alt></p>
<p>所以要在这个数据集上找寻一个决策边界(超平面)，让紫色点的标签为1，红色点的标签为-1</p>
<p>二维平面上，任意一条线可以表示为：<br>$$x_1 = ax_2+b$$</p>
<p>将表达式变换：<br>$$0=ax_2-x_1+b$$<br>$$0=(a,-1)*\binom{x_2}{x_1}+b$$<br>$$0=w^Tx+b$$</p>
<p>其中$(a,-1)$是参数向量$w$，$x$是特征向量，$b$是截距，在SVM中，使用这个表达式表示决策边界。</p>
<p><strong>我们的目标是求解能够让边际最大化的决策边界</strong></p>
<p>如果在决策边界上任意取两个点$x_a,x_b$，并代入决策边界的表达式，则有：<br>$$w^Tx_a+b=0$$<br>$$w^Tx_b+b=0$$<br>两式相减，可以得到：<br>$$w^T*(x_a-x_b)=0$$</p>
<p>两个向量的点积为0，证明参数向量$w$的方向垂直与我们的决策边界</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/F29C776BA1854D9C87B9BA1EEFF2F3B9?method=download&shareKey=f6addba1a1e3223deb09059443ef2cf1" alt></p>
<p>由于紫色的点代表标签$y=1，y=-1$，因此以下式判定：<br>$$<br>y=<br>\begin{cases}<br>1, &amp;if\ w*x_t+b&gt;0\\<br>-1, &amp;if\ w * x_t+b&lt;0<br>\end{cases}<br>$$</p>
<p><strong>注意：决策边界以上的点都为正，以下的点都为负，是人为规定的，不会影响对参数$w$和截距$b$的求解</strong></p>
<p>决策边界的两边要有两个超平面，两个超平面在二维空间中是两条平行线（虚线超平面），而他们之间的距离就是我们的边际$d$。<br>我们将这两条平行线表示为：</p>
<p>$$w * x+b=k, w * x+b=-k$$</p>
<p>两边同时除以$k$,则可以得到：</p>
<p>$$w * x+b=1, w * x+b=-1$$</p>
<p>表达式两边1和-1分别表示了两条平行决策边界的虚线到决策边界的相对距离。让这两条线分别过两类数据中距离决策边界最近的点，这些点被称为“支持向量”。</p>
<p>令紫色类的点为$x_p$，红色类的点为$x_r$，则可以得到：</p>
<p>$$w * x_p+b=1,w * x_r+b=-1$$</p>
<p>两个式相减，则有：<br>$$w * (w_p-w_r)=2$$<br>如下图所示，$(x_p-x_r)$可以表示为两点之间的连线，而边际$d$是平行于$w$的。</p>
<p>又因为以下数学性质：</p>
<table>
<thead>
<tr>
<th align="left">线性代数中模长的运用</th>
</tr>
</thead>
<tbody><tr>
<td align="left">向量$b$除以自身的模长可以得到$b$方向上的单位向量</td>
</tr>
<tr>
<td align="left">向量a乘以向量b方向上的单位向量，可以得到向量a在向量b方向上的投影的长度。</td>
</tr>
</tbody></table>
<p>所以，将上述式子两边同时除以$||w||$, 则可以得到：<br>$$\frac{w*(x_p-x_r)}{||w||}=\frac{2}{||w||}$$<br>$$d = \frac{2}{||w||}$$</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/B26C385AAC024C1791BBFDDF643F3D01?method=download&shareKey=aa3e06d8df0f9b472581aa1282400ae3" alt></p>
<p>所以，求最大边界，对应的就是最大化d,求解以下函数的最小值：<br>$$f(w)=\frac{||w||^2}{2}$$</p>
<p>对于任意样本，可以把决策函数写作：<br>$$w * x_i+b&gt;=1\   if\ y_i=1$$<br>$$w * x_i+b&lt;=-1 \  if\ y_i=-1$$<br>将两个式子整合：<br>$$y_i(w*x_i+b)&gt;=1, i=1,2,\dots N$$</p>
<h2 id="1-2-线性SVM的拉格朗日对偶函数和决策函数"><a href="#1-2-线性SVM的拉格朗日对偶函数和决策函数" class="headerlink" title="1.2 线性SVM的拉格朗日对偶函数和决策函数"></a>1.2 线性SVM的拉格朗日对偶函数和决策函数</h2><p>损失函数分为两个部分：最要最小化的函数，以及参数求解后必须满足的约束条件</p>
<h3 id="1-2-1-将损失函数从最初形态转换为拉格朗日乘数形态"><a href="#1-2-1-将损失函数从最初形态转换为拉格朗日乘数形态" class="headerlink" title="1.2.1 将损失函数从最初形态转换为拉格朗日乘数形态"></a>1.2.1 将损失函数从最初形态转换为拉格朗日乘数形态</h3><p><strong>1.为什么要进行转换？</strong></p>
<p>我们的目标是求解让损失函数最小化的$w$，希望能够找出一种方式，能在满足条件$y_i(w*x_i+b)&gt;=1$，因此，一种业界认可的方式是使用拉格朗日乘数法(standard Lagrange multiplier method)</p>
<p><strong>2.为什么可以进行转换</strong></p>
<p>拉格朗日乘数法正好可以用来解决凸优化问题，使用拉格朗日乘数法将损失函数改写为考虑了约束条件的形式：<br>$$L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w*x_i+b)-1) (\alpha_i&gt;=0)$$</p>
<p>其中，$\alpha_i$叫作拉格朗日乘数。</p>
<p><strong>3.怎样进行转换</strong></p>
<p>拉格朗日函数分为两个部分，第一部分是原始损失函数，第二部分是约束条件。我们希望，$L(w,b,\alpha)$在满足约束条件下，最小化损失函数。所以要先以$\alpha$为参数，求解$L(w,b,\alpha)$的最大值，在以$w,b$为参数，求解$L(w,b,\alpha)$的最小值。</p>
<p>因此，我们的目标可写作：</p>
<p>$$\min_{w,b}\max_{\alpha_i&gt;=0}L(w,b,\alpha) (\alpha_i&gt;=0)$$</p>
<p>首先，第一步执行max，即最大化$L(w,b,\alpha)$，有两种情况：</p>
<ul>
<li>当$y_i(w * x_i+b)&gt;1$，函数的第二部分$\sum_{i=1}^N\alpha_i(y_i(w*x_i+b)-1)$一定为正，式子$\frac{1}{2}||w||^2$就要减去一个正数，此时若需要最大化$L(w,b,\alpha)$,则$\alpha$必须取到0</li>
<li>当$y_i(w * x_i+b)&lt;1$，函数的第二部分$\sum_{i=1}^N\alpha_i(y_i(w*x_i+b)-1)$一定为负，式子$\frac{1}{2}||w||^2$就要减去一个负数，此时若需要最大化$L(w,b,\alpha)$,则$\alpha$必须取到正无穷</li>
</ul>
<p>因此，可以把函数的第二部分当做一个惩罚项，只有当$y_i(w*x_i+b)&gt;1$时函数没有受到惩罚，从而在求解最小值的时候让约束条件满足</p>
<p>现在，$L(w,b,\alpha)$是我们新的损失函数，目标要通过先最大化，再最小化求解$w，b$</p>
<h3 id="1-2-2-将拉格朗日函数转换为拉格朗日对偶函数"><a href="#1-2-2-将拉格朗日函数转换为拉格朗日对偶函数" class="headerlink" title="1.2.2 将拉格朗日函数转换为拉格朗日对偶函数"></a>1.2.2 将拉格朗日函数转换为拉格朗日对偶函数</h3><p><strong>1.为什么要进行转换？</strong><br>求极值，最简单的方法是对参数求导后让一阶导数为0，这里对参数$w,b$分别求偏导并使其为0</p>
<p>$$<br>\begin{equation}\begin{split}<br>L(w,b,\alpha)&amp;=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w * x_i+b)-1)\\<br>&amp;=\frac{1}{2}||w||^2-\sum_{i=1}^N(\alpha_iy_iw * x_i+\alpha_iy_ib-\alpha_i) \\<br>&amp; =\frac{1}{2}||w||^2-\sum_{i=1}^N(\alpha_iy_iw * x_i)-\sum_{i=1}^N\alpha_iy_ib+\sum_{i=1}^N\alpha_i\\<br>&amp;=\frac{1}{2}(w^Tw)^{\frac{1}{2}*2}-\sum_{i=1}^N(\alpha_iy_iw * x_i)-\sum_{i=1}^N\alpha_iy_ib+\sum_{i=1}^N\alpha_i<br>\end{split}\end{equation}<br>$$</p>
<p>$$<br>\begin{equation}\begin{split}<br>\frac{\partial L(w,b,\alpha)}{\partial w}&amp;=\frac{1}{2}*2w-\sum_{i=1}^N\alpha_iy_ix_i \\<br>&amp;=w-\sum_{i=1}^N\alpha_iy_ix_i=0 \to w = \sum_{i=1}^N\alpha_iy_ix_i<br>\end{split}\end{equation}<br>$$</p>
<p>$$<br>\begin{equation}\begin{split}<br>\frac{\partial L(w,b,\alpha)}{\partial b}&amp;=\sum_{i=1}^N\alpha_iy_i=0 \to \sum_{i=1}^N\alpha_iy_i=0 \\<br>\end{split}\end{equation}<br>$$</p>
<p>由于两个求偏导结果中都带有未知的拉格朗日乘数$\alpha_i$，因此需要有一种方法来求解拉格朗日乘数，而拉格朗日对偶函数，是只带有$alpha_i$而不带有$w,b$的形式，可以求解$\alpha_i$然后再代入（2）、（3）式中来求解参数$w,b$</p>
<p><strong>2.为什么能够进行转换</strong></p>
<p>对于任意一个拉格朗日函数$L(x,\alpha)=f(x)+\sum_{i=1}^q\alpha_ih_i(x)$，都存在一个与它对应的对偶函数$g(\alpha)$，只带有拉格朗日乘数$\alpha$作为唯一参数。如果$L(x,\alpha)$存在最优解并可以表示为$\min_xL(x,\alpha)$，并且对偶函数的最优解也存在并可以表示为$\max_\alpha g(\alpha)$，则可以定义对偶差异(dual gap)<br>$$\Delta = \min_{x}L(x,\alpha)-\max_\alpha g(\alpha)$$</p>
<p>如果$\Delta=0$,则称$L(x,\alpha)$与其对偶函数之间存在强对偶关系，此时可以通过求其对偶函数的最优解来代替原始函数。</p>
<p>强对偶关系存在需要满足KKT(Karush-Kuhn-Tucker)条件：<br>$$<br>\begin{equation}\begin{split}<br>\frac{\partial L}{\partial x_i}=0,\forall_i = 1,2,\dots,d \\<br>h_i(x)&lt;=0,\forall_i = 1,2,\dots,q \\<br>\alpha_i&gt;=0,\forall_i = 1,2,\dots,q \\<br>\alpha_ih_i(x)=0,\forall_i = 1,2,\dots,q \\<br>\end{split}\end{equation}<br>$$</p>
<p>如果我们可以让KKT条件全部成立，则可以求解出$L(w,b,\alpha)$的对偶函数来求解$\alpha$</p>
<p>之前求偏导时，得到<br>$$w = \sum_{i=1}^N\alpha_iy_ix_i$$<br>$$\sum_{i=1}^N\alpha_iy_i=0$$</p>
<p>并且满足：<br>$$-(y(w*x_i+b)-1)&lt;=0, \alpha_i&gt;=0$$</p>
<p>接下来，只需要满足：<br>$$\alpha_i(y_i(w*x_i+b)-1)=0$$</p>
<p>当这个条件满足时，能够让$y_i(w*x_i+b)-1=0$是在虚线的超平面上的样本点，即我们的支持向量，其余样本点必须满足$\alpha_i=0$。</p>
<p>现在KKT的五个条件都得到了满足，可以使用对偶函数求解。</p>
<p><strong>3.怎样进行转换</strong></p>
<p>首先将求导为0代入原始：<br>$$<br>\begin{equation}\begin{split}<br>L(w,b,\alpha)&amp;=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w * x_i+b)-1)\\<br>&amp;=\frac{1}{2}||w||^2-\sum_{i=1}^N(\alpha_iy_iw * x_i+\alpha_iy_ib-\alpha_i) \\<br>&amp; =\frac{1}{2}||w||^2-\sum_{i=1}^N(\alpha_iy_iw * x_i)-\sum_{i=1}^N\alpha_iy_ib+\sum_{i=1}^N\alpha_i\\<br>&amp;=\frac{1}{2}(w^Tw)^{\frac{1}{2}*2}-w\sum_{i=1}^N(\alpha_iy_i * x_i)-b\sum_{i=1}^N\alpha_iy_i+\sum_{i=1}^N\alpha_i \\<br>\end{split}\end{equation}</p>
<p>将（2）、（3）式代入则有<br>$$=\frac{1}{2}w^Tw-w^Tw+\sum_{i=1}^N\alpha_i $$</p>
<p>$$=-\frac{1}{2}w^Tw+\sum_{i=1}^N\alpha_i$$</p>
<p>将（2）式代入，则有：<br>$$=-\frac{1}{2}\sum_{i=1}^N\alpha_iy_ix_i^T * \sum_{i=1}^N\alpha_iy_ix_i+\sum_{i=1}^N\alpha_i$$</p>
<p>令这两个$w$源于不同的特征和标签：<br>$$=-\frac{1}{2}\sum_{i=1,j=1}^N\alpha_iy_ix_i^T\alpha_iy_ix_i+\sum_{i=1}^N\alpha_i$$</p>
<p>$$=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j$$</p>
<p>将矩阵相乘转换为内积形式：<br>$$L_d = \sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^N\alpha_i\alpha_jy_iy_jx_i * x_j$$</p>
<p>函数$L_d$就是对偶函数，其对偶差异$L(w,b,\alpha)$和$L_d$，则有：<br>$$\Delta = \min_{w,b}\max_{\alpha_i&gt;=0}L(w,b,\alpha)-\max_{\alpha_i&gt;=0}L_d$$</p>
<p>又因为在推导$L_d$时，对$L(w,b,\alpha)$偏导为0，所以可以把公式写成：<br>$$\Delta = \min_{w,b}\max_{\alpha_i&gt;=0}L(w,b,\alpha)-\max_{\alpha_i&gt;=0}\min_{w,b}L(w,b,\alpha)$$</p>
<p>又因为所有KKT条件满足，所以：<br>$$ \min_{w,b}\max_{\alpha_i&gt;=0}L(w,b,\alpha)=\max_{\alpha_i&gt;=0}\min_{w,b}L(w,b,\alpha)$$</p>
<p>所以，我们只需要求解对偶函数的最大值就可以了，最终，目标函数变化为：<br>$$\max_{\alpha_i&gt;=0}(\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^N\alpha_i\alpha_jy_iy_jx_i * x_j)$$</p>
<h3 id="1-2-3-求解拉格朗日对偶函数极值及其后续过程"><a href="#1-2-3-求解拉格朗日对偶函数极值及其后续过程" class="headerlink" title="1.2.3 求解拉格朗日对偶函数极值及其后续过程"></a>1.2.3 求解拉格朗日对偶函数极值及其后续过程</h3><p>需要使用梯度下降、SMO或者二次规划(QP, quadratic programming)来求解，但是这一过程对数学要求的程度已经远远超出了需要的程度，因此不再深入了解。</p>
<p>只需要知道，一旦求到了$\alpha$值，就可以求得决策边界参数$w,b$</p>
<p>当求得参数后，就可以得到决策边界的表达式：<br>$$f(x_{test})=sign(w*x_{test}+b)$$</p>
<p>其中$x_{test}$是任意测试样本，$sign(h)$是符号函数。</p>
<p>至此，我们对SVM的原理已经有了一个比较深刻的理解。</p>
<h2 id="1-3-非线性SVM与核函数"><a href="#1-3-非线性SVM与核函数" class="headerlink" title="1.3 非线性SVM与核函数"></a>1.3 非线性SVM与核函数</h2><h3 id="1-3-1-SVC在非线性数据上的推广"><a href="#1-3-1-SVC在非线性数据上的推广" class="headerlink" title="1.3.1 SVC在非线性数据上的推广"></a>1.3.1 SVC在非线性数据上的推广</h3><p>为了找出非线性数据的线性决策边界，我们需要将数据从原始空间$x$，投射到新空间$\Phi(x)$中，$\Phi(x)$是一个映射函数，代表某种非线性变换。线性SVM的原理可以很容易推广到非线性情况下，推导过程和逻辑都与线性SVM一样，只是在定义决策边界之前，需要对数据升维，将原始的$x$，转换成$\Phi(x)$</p>
<p>因此，非线性SVM的损失函数初始形态为：<br>$$\min_{w,b}\frac{||w||^2}{2}$$<br>$$subject\  to\  y_i(w * \Phi(x)+b&gt;=1), i=1,2,\dots,N$$<br>拉格朗日和拉格朗日对偶函数也可得：<br>$$L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w * \Phi(x)+b)-1)$$<br>$$L_d = \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j\Phi(x_i)\Phi(x_j)$$</p>
<p>同理可以使用相同的推导方式让拉格朗日函数满足KKT条件，并且对每个参数求导，再使用梯度下降或SMO等方式求解，最终得到决策边界函数：<br>$$f(x_{test})=sign(w*\Phi(x_{test})+b)$$</p>
<h3 id="1-3-2-重要参数kernel"><a href="#1-3-2-重要参数kernel" class="headerlink" title="1.3.2 重要参数kernel"></a>1.3.2 重要参数kernel</h3><p>但是，我们不清楚什么样的数据应该使用什么类型的映射函数确保在变换空间中可以找出决策边界。</p>
<table>
<thead>
<tr>
<th align="left">关键概念：核函数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">解决这个问题的数学方式，叫作“核技巧”(Kernel Trick)，是一种讷讷够使用数据原始空间中的向量计算表示生维后的空间中的点积结果的数学方式$K(u,v)=\Phi(u)*\Phi(v)$,而这个点积函数$K(u,v)$，就被叫作“核函数”(Kernel Funciton)</td>
</tr>
</tbody></table>
<p>选用不同的核函数，就可以解决不同数据分布下寻找超平面的问题。以下是“kernel”在Sklearn中的几种选项：</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/0D76DD0804FD486EA95C591CDF1335C5?method=download&shareKey=c5e8aae3bd6137b2f4c315008302565b" alt></p>
<h3 id="1-3-3-核函数的优势和缺陷"><a href="#1-3-3-核函数的优势和缺陷" class="headerlink" title="1.3.3 核函数的优势和缺陷"></a>1.3.3 核函数的优势和缺陷</h3><ol>
<li>线性核，尤其是多项式核：在高次项计算非常缓慢</li>
<li>rbf和多项式核不擅长处理量纲不统一的数据集</li>
</ol>
<p><strong>因此，SVM执行之前，推荐先进行数据的无量纲化处理</strong></p>
<h3 id="1-3-4-核函数相关的参数：degree-amp-gamma-amp-coef0"><a href="#1-3-4-核函数相关的参数：degree-amp-gamma-amp-coef0" class="headerlink" title="1.3.4 核函数相关的参数：degree &amp; gamma &amp; coef0"></a>1.3.4 核函数相关的参数：degree &amp; gamma &amp; coef0</h3><ul>
<li>gamma是表达式中的$\gamma$</li>
<li>degree是多项式核函数的次数$d$</li>
<li>ceof0是常数项</li>
</ul>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">degree</td>
<td align="left">整数，可不填，默认3</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">多项式核函数的次数（’poly’），如果核函数没有选择”poly”，这个参数会被忽略</td>
</tr>
<tr>
<td align="left">gamma</td>
<td align="left">浮点数，可不填，默认“auto”</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">核函数的系数，仅在参数Kernel的选项为”rbf”,”poly”和”sigmoid”的时候有效</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">输入“auto”，自动使用1/(n_features)作为gamma的取值</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">输入”scale”，则使用1/(n_features * X.std())作为gamma的取值</td>
</tr>
<tr>
<td align="left">ceof0</td>
<td align="left">浮点数，可不填，默认=0.0</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">核函数中的常数项，它只在参数kernel为’poly’和’sigmoid’的时候有效。</td>
</tr>
</tbody></table>
<h2 id="1-4-硬间隔与软间隔：重要参数C"><a href="#1-4-硬间隔与软间隔：重要参数C" class="headerlink" title="1.4 硬间隔与软间隔：重要参数C"></a>1.4 硬间隔与软间隔：重要参数C</h2><h3 id="1-4-1-SVM在软间隔数据上的推广"><a href="#1-4-1-SVM在软间隔数据上的推广" class="headerlink" title="1.4.1 SVM在软间隔数据上的推广"></a>1.4.1 SVM在软间隔数据上的推广</h3><p>我们可以通过调整我们对决策边界的定义，将硬间隔时得出的数学结论推广到软间隔的情况上，让决策边界能够忍  受一小部分训练误差。这个时候，我们的决策边界就不是单纯地寻求最大边际了，因为对于软间隔地数据来说，边  际越大被分错的样本也就会越多，因此我们需要找出一个”最大边际“与”被分错的样本数量“之间的平衡<br>|关键概念：硬间隔与软间隔|<br>|:–|<br>|当两组数据是完全线性可分，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为  是存在”硬间隔“的。当两组数据几乎是完全线性可分的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”软间隔“。|</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/C2FFA1AA25854902ACF46B5A0C5D3410?method=download&shareKey=95be7710c6d12317d6fbda16a8bd436e" alt></p>
<p>由上图可知，超平面无法让数据上的训练误差为0，因此需要放松判别函数的条件，引入松弛系数$\zeta$来帮助优化判别函数：<br>$$w<em>x_i +b&gt;=1-\zeta_i if\ y_i = 1$$<br>$$w</em>x_i +b&lt;=-1+\zeta_i if\ y_i = -1$$</p>
<p>其中$\zeta_i&gt;0$，在求解最大边际的损失函数中加上一个惩罚项，我们的拉格朗日函数，拉格朗日对偶函数也因此被松弛系数改变：<br>$$\min_{w,b,\zeta}\frac{||w||^2}{2}+C\sum_{i=1}^n\zeta_i$$<br>$$subject to y_i(w* \Phi(x_i)+b&gt;=1-\zeta_i)$$<br>$$\zeta_i&gt;=0,i=1,2,\dots,N$$</p>
<p>其中，$C$是用来控制惩罚力度的系数</p>
<p>拉格朗日函数为(其中$\mu$是第二个拉格朗日乘数)：<br>$$L(w,b,\alpha,\zeta)= \frac{1}{2}||w||^2+C\sum_{i=1}^N\zeta_i-\sum_{i=1}^N\alpha_i(y_i(w*\zeta(x_i)+b)-1+\zeta_i)-\sum_{i=1}^N\mu_i\zeta_i$$</p>
<p>需要满足的KKT条件为：<br>$$\frac{\partial L(w,b,\alpha,\zeta)}{\partial w}=\frac{\partial L(w,b,\alpha,\zeta)}{\partial b}=\frac{\partial L(w,b,\alpha,\zeta)}{\partial \zeta}=0$$</p>
<p>$$\zeta_i&gt;=0,\alpha_i&gt;=0,\mu_i&gt;=0$$<br>$$\alpha_i(y_i(w*\Phi(x_i)+b)-1+\zeta_i)=0$$<br>$$\mu_i\zeta_i=0$$</p>
<p>拉格朗日对偶函数为：<br>$$L_D = \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\Phi(x_i)\Phi(x_j)$$<br>$$subject\  to\  C &gt;=\alpha_i&gt;=0$$<br>公式中唯一出现的新变量，松弛系数惩罚力度$C$。由参数$C$来进行控制</p>
<h3 id="1-4-2-重要参数C"><a href="#1-4-2-重要参数C" class="headerlink" title="1.4.2 重要参数C"></a>1.4.2 重要参数C</h3><p>参数C用于权衡”训练样本的正确分类“与”决策函数的边际最大化“两个不可同时完成的目标，希望找出一个平衡点来让模型的效果最佳。</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">C</td>
<td align="left">浮点数，默认1，必须大于等于0，可不填</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">松弛系数的惩罚项系数。如果C值设定比较大，那SVC可能会选择边际较小的，能够更好地分类所有训练点的决策边界，不过模型的训练时间也会更长。如果C的设定值较小，那SVC会尽量最大化边界，决策功能会更简单，但代价是训练的准确度。换句话说，C在SVM中的影响就像正则化参数对逻辑回归的影响。</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
